# Query `73`

*How best to count bees entering and leaving a hive to measure hive activity?
This is my first question here, so I apologize for all mistakes I could have possibly made.
I'm a high school student in East-Central Europe and I need to complete some research for a biology contest (asking for advice is accepted, so I'm not cheating). My task is to analyze the influence of certain environmental factors (temperature etc., it's not that important for my question) on the activity of bees. The method is pretty simple: once a week I record the entrance to the bee hive (I do it on four hives), play it in slow-mo (It's impossible to count them properly without doing so) and simply count the number of bees entering or leaving.
Problem is, I don't know how long the observation should take. I play it in like X1/8, you need to play it twice (entering/leaving), so it takes a lot of time to gather one piece of information for a certain day. Till now I've been doing it for one minute - and there seems to be some kind of pattern to their activity analyzed that way. Yet, I'm not sure if it's actually eligible. I can't do the observation for hours - I still need to learn and have other duties.
So, what should I do? Could anyone give me some advice? Is one minute (several, like 6 times a day per hive) legitimate enough?
Thank you in advance.*

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) count_bees/atthehiveentranceloo_88_1.txt
> and over, is always some version of: Less is more; just about everything that you need to know about a hive is right there in plain view at the entrance; everything we know about bees, everything without exception, is rooted in someone sitting in front of a hive spending hours and hours just watching the bees go in and out; shed your preconceptions and learn to observe–the bees will teach you the only lessons you really need to learn.

### DOC[2] (IRRELEVANT) count_bees/atthehiveentranceloo_12_0.txt
> I don’t consider this idle time, although it may look like it to the casual observer. I have read that if you are not getting into your hives at least every two weeks to do inspections, you are a poor beekeeper, or worse: a bee “haver.” However, my hours of observation time are my primary method of hive inspection, and I find the most of the information I need without the need for suiting up, lighting smokers, or disrupting the hard work of the hive.

### DOC[3] (IRRELEVANT) count_bees/atthehiveentranceloo_13_0.txt
> My bee mentor, Jacqueline Freeman (www.spiritbee.com) calls this “Putting in your thousand hours—” not a thousand hours inside the hive, but observing the hive from the outside as you sit beside your bees patiently, over many months. In the beginning of my beekeeping seasons, I was a patient observer mostly because I was keeping three Warre’ hives. There were no viewing windows on the hives, and once a Warre’ begins building up, removing single combs is major surgical event for the bees, so I had to restrict my

### DOC[4] (IRRELEVANT) count_bees/atthehiveentranceloo_88_0.txt
> Reply<br>Rob says:<br>August 17, 2020 at 7:44 pm<br>I like your article on observation (“At the Hive Entrance”)–a lot. I live in a small town in Alaska that is currently experiencing a boom in beginning beekeepers. I’ve been keeping bees for going on fifty years–seems hard to believe, but it’s true–which makes me by far the most experienced beekeeper in the area, and as such I answer a lot of questions, have a lot of mentoring sessions with beginners. My advice, over

### DOC[5] (IRRELEVANT) count_bees/atthehiveentranceloo_60_0.txt
> Reply<br>Rob Schmidt - St Louis MO says:<br>June 15, 2017 at 11:19 pm<br>I love this article. I’ve read it multiple times and shared it with my bee friends.<br>Would you write more on what you can learn from observing your hive from the outside?


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, count_bees/Poisson_distribution_6.txt
> Occurrence and applications[edit]<br>This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources: "Poisson distribution" – news · newspapers · books · scholar · JSTOR (December 2019) (Learn how and when to remove this template message)<br>Some applications of the Poisson distribution to count data (number of events):<br>telecommunication: telephone calls arriving in a system,<br>astronomy: photons arriving at a telescope,<br>chemistry: the molar mass distribution of a living polymerization,<br>biology: the number of mutations on a strand of DNA per unit length,<br>management: customers arriving at a counter or call centre,<br>finance and insurance: number of losses or claims occurring in a given period of time,<br>seismology: asymptotic Poisson model of risk for large earthquakes,<br>radioactivity: decays in a given time interval in a radioactive sample,<br>optics: number of photons emitted in a single laser pulse (a major vulnerability of quantum key distribution protocols, known as Photon Number Splitting).<br>More examples of counting events that may be modelled as Poisson processes include:<br>soldiers killed by horse-kicks each year in each corps in the Prussian cavalry. This example was used in a book by Ladislaus Bortkiewicz (1868–1931),<br>yeast cells used when brewing Guinness beer. This example was used by William Sealy Gosset (1876–1937),<br>phone calls arriving at a call centre within a minute. This example was described by A.K. Erlang (1878–1929),<br>goals in sports involving two competing teams,<br>deaths per year in a given age group,<br>jumps in a stock price in a given time interval,<br>times a web server is accessed per minute (under an assumption of homogeneity),<br>mutations in a given stretch of DNA after a certain amount of radiation,<br>cells infected at a given multiplicity of infection,<br>bacteria in a certain amount of liquid,<br>photons arriving on a pixel circuit at a given illumination over a given time period,<br>landing of V-1 flying bombs on London during World War II, investigated by R. D. Clarke in 1946.<br>In probabilistic number theory, Gallagher showed in 1976 that, if a certain version of the unproved prime r-tuple conjecture holds,  then the counts of prime numbers in short intervals would obey a Poisson distribution.<br><br>Law of rare events[edit]<br><br>Main article: Poisson limit theorem<br>Comparison of the Poisson distribution (black lines) and the binomial distribution with  n = 10  (red circles),  n = 20  (blue circles),  n = 1000  (green circles). All distributions have a mean of 5. The horizontal axis shows the number of events k. As n gets larger, the Poisson distribution becomes an increasingly better approximation for the binomial distribution with the same mean.<br>The rate of an event is related to the probability of an event occurring in some small subinterval (of time, space or otherwise). In the case of the Poisson distribution, one assumes that there exists a small enough subinterval for which the probability of an event occurring twice is "negligible". With this assumption one can derive the Poisson distribution from the Binomial one, given only the information of expected number of total events in the whole interval.<br>Let the total number of events in the whole interval be denoted by <br><br><br><br>λ<br>.<br><br><br>{\displaystyle \lambda .}<br><br> Divide the whole interval into <br><br><br><br>n<br><br><br>{\displaystyle n}<br><br> subintervals <br><br><br><br><br>I<br><br>1<br><br><br>,<br>…<br>,<br><br>I<br><br>n<br><br><br><br><br>{\displaystyle I_{1},\dots ,I_{n}}<br><br> of equal size, such that <br><br><br><br>n<br><br>λ<br><br><br>{\displaystyle n\lambda }<br><br> (since we are interested in only very small portions of the interval this assumption is meaningful). This means that the expected number of events in each of the n subintervals is equal to <br><br><br><br>λ<br><br>/<br><br>n<br>.<br><br><br>{\displaystyle \lambda /n.}<br>Now we assume that the occurrence of an event in the whole interval can be seen as a sequence of n Bernoulli trials, where the <br><br><br><br>i<br><br><br>{\displaystyle i}<br><br>-th Bernoulli trial corresponds to looking whether an event happens at the subinterval <br><br><br><br><br>I<br><br>i<br><br><br><br><br>{\displaystyle I_{i}}<br><br> with probability <br><br><br><br>λ<br><br>/<br><br>n<br>.<br><br><br>{\displaystyle \lambda /n.}<br><br> The expected number of total events in <br><br><br><br>n<br><br><br>{\displaystyle n}<br><br> such trials would be <br><br><br><br>λ<br>,<br><br><br>{\displaystyle \lambda ,}<br><br> the expected number of total events in the whole interval. Hence for each subdivision of the interval we have approximated the occurrence of the event as a Bernoulli process of the form <br><br><br><br><br><br>B<br><br><br>(<br>n<br>,<br>λ<br><br>/<br><br>n<br>)<br>.<br><br><br>{\displaystyle {\textrm {B}}(n,\lambda /n).}<br><br> As we have noted before we want to consider only very small subintervals. Therefore, we take the limit as <br><br><br><br>n<br><br><br>{\displaystyle n}<br><br> goes to infinity.<br>In this case the binomial distribution converges to what is known as the Poisson distribution by the Poisson limit theorem.<br>In several of the above examples — such as, the number of mutations in a given sequence of DNA—the events being counted are actually the outcomes of discrete trials, and would more precisely be modelled using the binomial distribution, that is<br><br><br><br><br>X<br>∼<br><br><br>B<br><br><br>(<br>n<br>,<br>p<br>)<br>.<br><br><br>{\displaystyle X\sim {\textrm {B}}(n,p).}<br>In such cases n is very large and p is very small (and so the expectation n p is of intermediate magnitude). Then the distribution may be approximated by the less cumbersome Poisson distribution <br><br><br><br>X<br>∼<br><br><br>Pois<br><br><br>(<br>n<br>p<br>)<br>.<br><br><br>{\displaystyle X\sim {\textrm {Pois}}(np).}<br>This approximation is sometimes known as the law of rare events, since each of the n individual Bernoulli events rarely occurs.<br>The name "law of rare events" may be misleading because the total count of success events in a Poisson process need not be rare if the parameter n p is not small. For example, the number of telephone calls to a busy switchboard in one hour follows a Poisson distribution with the events appearing frequent to the operator, but they are rare from the point of view of the average member of the population who is very unlikely to make a call to that switchboard in that hour.<br>The variance of the binomial distribution is 1 − p times that of the Poisson distribution, so almost equal when p is very small.<br>The word law is sometimes used as a synonym of probability distribution, and convergence in law means convergence in distribution. Accordingly, the Poisson distribution is sometimes called the "law of small numbers" because it is the probability distribution of the number of occurrences of an event that happens rarely but has very many opportunities to happen. The Law of Small Numbers is a book by Ladislaus Bortkiewicz about the Poisson distribution, published in 1898.<br>Poisson point process[edit]<br><br>Main article: Poisson point process<br>The Poisson distribution arises as the number of points of a Poisson point process located in some finite region. More specifically, if D is some region space, for example Euclidean space R, for which |D|, the area, volume or, more generally, the Lebesgue measure of the region is finite, and if  N(D)  denotes the number of points in D, then<br>P<br>(<br>N<br>(<br>D<br>)<br>=<br>k<br>)<br>=<br><br><br><br>(<br>λ<br><br>|<br><br>D<br><br>|<br><br><br>)<br><br>k<br><br><br><br>e<br><br>−<br>λ<br><br>|<br><br>D<br><br>|<br><br><br><br><br><br>k<br>!<br><br><br><br>.<br><br><br>{\displaystyle P(N(D)=k)={\frac {(\lambda |D|)^{k}e^{-\lambda |D|}}{k!}}.}<br>Poisson regression and negative binomial regression[edit]<br>Poisson regression and negative binomial regression are useful for analyses where the dependent (response) variable is the count  (0, 1, 2, ... )  of the number of events or occurrences in an interval.<br>Other applications in science[edit]<br>In a Poisson process, the number of observed occurrences fluctuates about its mean λ with a standard deviation <br><br><br><br><br>σ<br><br>k<br><br><br>=<br><br><br>λ<br><br><br>.<br><br><br>{\displaystyle \sigma _{k}={\sqrt {\lambda }}.}<br><br> These fluctuations are denoted as Poisson noise or (particularly in electronics) as shot noise.<br>The correlation of the mean and standard deviation in counting independent discrete occurrences is useful scientifically. By monitoring how the fluctuations vary with the mean signal, one can estimate the contribution of a single occurrence, even if that contribution is too small to be detected directly. For example, the charge e on an electron can be estimated by correlating the magnitude of an electric current with its shot noise. If N electrons pass a point in a given time t on the average, the mean current is <br><br><br><br>I<br>=<br>e<br>N<br><br>/<br><br>t<br><br><br>{\displaystyle I=eN/t}<br><br>; since the current fluctuations should be of the order <br><br><br><br><br>σ<br><br>I<br><br><br>=<br>e<br><br><br>N<br><br><br><br>/<br><br>t<br><br><br>{\displaystyle \sigma _{I}=e{\sqrt {N}}/t}<br><br> (i.e., the standard deviation of the Poisson process), the charge <br><br><br><br>e<br><br><br>{\displaystyle e}<br><br> can be estimated from the ratio <br><br><br><br>t<br><br>σ<br><br>I<br><br><br>2<br><br><br><br>/<br><br>I<br>.<br><br><br>{\displaystyle t\sigma _{I}^{2}/I.}<br>An everyday example is the graininess that appears as photographs are enlarged; the graininess is due to Poisson fluctuations in the number of reduced silver grains, not to the individual grains themselves. By correlating the graininess with the degree of enlargement, one can estimate the contribution of an individual grain (which is otherwise too small to be seen unaided). Many other molecular applications of Poisson noise have been developed, e.g., estimating the number density of receptor molecules in a cell membrane.<br>Pr<br>(<br><br>N<br><br>t<br><br><br>=<br>k<br>)<br>=<br>f<br>(<br>k<br>;<br>λ<br>t<br>)<br>=<br><br><br><br>(<br>λ<br>t<br><br>)<br><br>k<br><br><br><br>e<br><br>−<br>λ<br>t<br><br><br><br><br>k<br>!<br><br><br><br>.<br><br><br>{\displaystyle \Pr(N_{t}=k)=f(k;\lambda t)={\frac {(\lambda t)^{k}e^{-\lambda t}}{k!}}.}<br>In causal set theory the discrete elements of spacetime follow a Poisson distribution in the volume.<br>The Poisson distribution also appears in quantum mechanics, especially quantum optics. Namely, for a quantum harmonic oscillator system in a coherent state, the probability of measuring a particular energy level has a Poisson distribution.

### GROUND TRUTH 1, ranked not in top 100, count_bees/Poisson_distribution_0.txt
> In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time if these events occur with a known constant mean rate and independently of the time since the last event. It can also be used for the number of events in other types of intervals than time, and in dimension greater than 1 (e.g., number of events in a given area or volume).<br>The Poisson distribution is named after French mathematician Siméon Denis Poisson (/ˈpwɑːsɒn/; French pronunciation: [pwasɔ̃]). It plays an important role for discrete-stable distributions.<br>Under a Poisson distribution with the expectation of λ events in a given interval, the probability of k events in the same interval is:<br>For instance, consider a call center which receives, randomly, an average of λ = 3 calls per minute at all times of day. If the calls are independent, receiving one does not change the probability of when the next one will arrive. Under these assumptions, the number k of calls received during any minute has a Poisson probability distribution. Receiving  k = 1 to 4 calls then has a probability of about 0.77, while receiving 0 or at least 5 calls has a probability of about 0.23.<br>Another example for which the Poisson distribution is a useful model is the number of radioactive decay events during a fixed observation period.

### GROUND TRUTH 2, ranked not in top 100, count_bees/Poisson_distribution_7.txt
> Computational methods[edit]<br>The Poisson distribution poses two different tasks for dedicated software libraries: evaluating the distribution <br><br><br><br>P<br>(<br>k<br>;<br>λ<br>)<br><br><br>{\displaystyle P(k;\lambda )}<br><br>, and drawing random numbers according to that distribution.<br>Evaluating the Poisson distribution[edit]<br>Computing <br><br><br><br>P<br>(<br>k<br>;<br>λ<br>)<br><br><br>{\displaystyle P(k;\lambda )}<br><br> for given <br><br><br><br>k<br><br><br>{\displaystyle k}<br><br> and <br><br><br><br>λ<br><br><br>{\displaystyle \lambda }<br><br> is a trivial task that can be accomplished by using the standard definition of <br><br><br><br>P<br>(<br>k<br>;<br>λ<br>)<br><br><br>{\displaystyle P(k;\lambda )}<br><br> in terms of exponential, power, and factorial functions. However, the conventional definition of the Poisson distribution contains two terms that can easily overflow on computers: λ and k!. The fraction of λ to k! can also produce a rounding error that is very large compared to e, and therefore give an erroneous result. For numerical stability the Poisson probability mass function should therefore be evaluated as<br>f<br>(<br>k<br>;<br>λ<br>)<br>=<br>exp<br>⁡<br><br>[<br><br>k<br>ln<br>⁡<br>λ<br>−<br>λ<br>−<br>ln<br>⁡<br>Γ<br>(<br>k<br>+<br>1<br>)<br><br>]<br><br>,<br><br><br>{\displaystyle \!f(k;\lambda )=\exp \left[k\ln \lambda -\lambda -\ln \Gamma (k+1)\right],}<br>which is mathematically equivalent but numerically stable. The natural logarithm of the Gamma function can be obtained using the lgamma function in the C standard library (C99 version) or R, the gammaln function in MATLAB or SciPy, or the log_gamma function in Fortran 2008 and later.<br>Some computing languages provide built-in functions to evaluate the Poisson distribution, namely<br>R: function dpois(x, lambda);<br>Excel: function POISSON( x, mean, cumulative), with a flag to specify the cumulative distribution;<br>Mathematica: univariate Poisson distribution as PoissonDistribution[<br><br><br><br>λ<br><br><br>{\displaystyle \lambda }<br><br>], bivariate Poisson distribution as MultivariatePoissonDistribution[<br><br><br><br><br>θ<br><br>12<br><br><br>,<br><br><br>{\displaystyle \theta _{12},}<br><br>{ <br><br><br><br><br>θ<br><br>1<br><br><br>−<br><br>θ<br><br>12<br><br><br>,<br><br><br>{\displaystyle \theta _{1}-\theta _{12},}<br><br> <br><br><br><br><br>θ<br><br>2<br><br><br>−<br><br>θ<br><br>12<br><br><br><br><br>{\displaystyle \theta _{2}-\theta _{12}}<br><br>}],.<br>Random variate generation[edit]<br><br>Further information: Non-uniform random variate generation<br>The less trivial task is to draw integer random variate from the Poisson distribution with given <br><br><br><br>λ<br>.<br><br><br>{\displaystyle \lambda .}<br>Solutions are provided by:<br>R: function rpois(n, lambda);<br>GNU Scientific Library (GSL): function gsl_ran_poisson<br>A simple algorithm to generate random Poisson-distributed numbers (pseudo-random number sampling) has been given by Knuth:<br>algorithm poisson random number (Knuth):<br>    init:<br>        Let L ← e, k ← 0 and p ← 1.<br>    do:<br>        k ← k + 1.<br>        Generate uniform random number u in [0,1] and let p ← p × u.<br>    while p  L.<br>    return k − 1.<br>The complexity is linear in the returned value k, which is λ on average.  There are many other algorithms to improve this. Some are given in Ahrens & Dieter, see § References below.<br>For large values of λ, the value of L = e may be so small that it is hard to represent.  This can be solved by a change to the algorithm which uses an additional parameter STEP such that e does not underflow:<br>algorithm poisson random number (Junhao, based on Knuth):<br>    init:<br>        Let λLeft ← λ, k ← 0 and p ← 1.<br>    do:<br>        k ← k + 1.<br>        Generate uniform random number u in (0,1) and let p ← p × u.<br>        while p < 1 and λLeft  0:<br>            if λLeft  STEP:<br>                p ← p × e<br>                λLeft ← λLeft − STEP<br>            else:<br>                p ← p × e<br>                λLeft ← 0<br>    while p  1.<br>    return k − 1.<br>The choice of STEP depends on the threshold of overflow. For double precision floating point format the threshold is near e, so 500 should be a safe STEP.<br>Other solutions for large values of λ include rejection sampling and using Gaussian approximation.<br>Inverse transform sampling is simple and efficient for small values of λ, and requires only one uniform random number u per sample. Cumulative probabilities are examined in turn until one exceeds u.<br>algorithm Poisson generator based upon the inversion by sequential search:<br>    init:<br>        Let x ← 0, p ← e, s ← p.<br>        Generate uniform random number u in [0,1].<br>    while u  s do:<br>        x ← x + 1.<br>        p ← p × λ / x.<br>        s ← s + p.<br>    return x.
