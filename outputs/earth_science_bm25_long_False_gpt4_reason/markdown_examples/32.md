# Query `32`

## Original Question

*I was checking the weather forecast just now, and it is showing that it "feels like -999 °C". I never heard or saw -999 °C before. I searched other weather channels, and they were showing that it feels like 2 °C. What is the meaning of -999 °C, irrespective of the weather?*

## gpt4 Reasoning

```markdown
The essential problem here seems to be an unusually reported "feels like" temperature of -999 °C from a weather forecasting service, which is far from a plausible or physically possible value for Earth's weather conditions. This anomaly prompts an investigation into what -999 °C could signify in this context, especially given that other sources report a "feels like" temperature of 2 °C, which is within a realistic range. Let's break down the situation step by step to understand the possible reasons behind this anomaly and what information could be relevant to address your questions.

1. **Understanding "Feels Like" Temperature**: The "feels like" temperature is a value that combines air temperature with relative humidity and wind speed to estimate how the human body perceives the outside temperature. In winter, this often refers to the wind chill factor, which can make the air feel colder than the actual air temperature due to the wind increasing the rate of heat loss from the body.

2. **Identifying the Anomaly**: A "feels like" temperature of -999 °C is not just unrealistic; it's physically impossible on Earth, where temperatures range from about -89.2 °C (the lowest natural temperature recorded at Earth's surface) to 56.7 °C (the highest natural temperature recorded). Thus, the reported -999 °C is clearly an error or placeholder value rather than a genuine weather observation.

3. **Possible Reasons for the Anomaly**:
   - **Data Entry Error**: The value could be a result of a manual or automated data entry error, where -999 was entered or transmitted erroneously.
   - **Placeholder Value**: In programming and data processing, certain placeholder values are used to indicate missing, undefined, or erroneous data. The value of -999 °C might be used by the weather service's database or software to indicate that the actual "feels like" temperature couldn't be calculated due to missing input data (like humidity or wind speed) or an error in the calculation process.
   - **Software Bug**: The anomaly could also be due to a bug in the software that processes weather data, leading to incorrect values being displayed under certain conditions.

4. **Addressing the Issue**:
   - **Verification**: Checking other reliable weather sources, as you did, is a good first step to verify the anomaly.
   - **Reporting the Error**: If the weather service provides a feedback mechanism, reporting the -999 °C anomaly could help their technical team identify and correct the error or bug causing this issue.
   - **Seeking Explanation**: Some weather services may offer customer support or FAQ sections where such anomalies are explained, providing insight into common issues or errors users might encounter.

In conclusion, the -999 °C "feels like" temperature you encountered is almost certainly an error or placeholder indicating a problem with the data or its processing. It's not a reflection of actual weather conditions but rather a signal that something went wrong in the data handling or display process for that particular weather forecasting service.
```

### Metrics

```
recip_rank: 0.2500
P_1: 0.0000
P_5: 0.2000
P_10: 0.1000
P_25: 0.0400
P_50: 0.0400
P_100: 0.0300
recall_1: 0.0000
recall_5: 0.1000
recall_10: 0.1000
recall_25: 0.1000
recall_50: 0.2000
recall_100: 0.3000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.1461
ndcg_cut_10: 0.0948
ndcg_cut_25: 0.0948
ndcg_cut_50: 0.1367
ndcg_cut_100: 0.1748
map_cut_1: 0.0000
map_cut_5: 0.0250
map_cut_10: 0.0250
map_cut_25: 0.0250
map_cut_50: 0.0304
map_cut_100: 0.0360
```

## Retrieval Results

### DOC[1] (IRRELEVANT) weather_forcast/forecast_process4.txt
> About the NWS National Program   Forecast Process  Weather.gov  About the NWS  Forecast Process         <br>     About      We are the National Weather Service   Forecasts and Services   Science and Technology   NWS Partners      <br><br>If it falls from the sky, flows across the surface of the Earth, or is released from the Sun, the National Weather Service most likely produces a forecast for it. NWS meteorologists across the country create forecasts for a wide variety of weather elements such as rainfall, snow storms, severe weather and hurricanes. Our hydrologists specialize in how water flows across the landscape and makes it into lakes, streams and rivers. While additional meteorologists and oceanographers produce wave forecasts for our Nation’s oceans and the Great Lakes. Climate forecasters in the NWS focus on large-scale temperature and precipitation patterns and time scales ranging from a couple weeks to several months.<br>The forecast process is roughly the same regardless of the type of weather. Our scientists thoroughly review current observations using technology such as radar, satellite and data from an assortment of ground-based and airborne instruments to get a complete picture of current conditions. Forecasters often rely on computer programs to create what’s called an “analysis,” which is simply a graphical representation of current conditions. Once this assessment is complete and the analysis is created, forecasters use a wide variety of numerical models, statistical and conceptual models, and years of local experience to determine how the current conditions will change with time. Numerical modeling is fully ingrained in the forecast process, and our forecasters review the output of these models daily. Often, the models yield different results, and in these circumstances, forecasters will determine which models perform best for the given situation or seek a blended solution.<br>One of the key considerations associated with any forecast is the element of uncertainty. The chaotic nature of the earth-atmosphere system and incomplete sampling of its complicated physical processes mean that forecasts become more uncertain at longer time ranges. This uncertainty is why the human component remains a vital piece in the forecast process; as once the forecast is complete, effectively communicating the forecast message becomes as important as the details of the forecast itself. This includes communicating which parts of the forecast are “uncertain” or what might be the “worst-case scenario” if the forecast changes. In addition to using the latest science and technology, NWS staff work with their partners to understand the impacts weather and water events have on our local communities. Now more than ever, NWS staff across the Nation are working hand-in-hand with public safety officials providing forecast advice and interpretation to help them make decisions when weather, water and climate impacts the lives and livelihoods of the American people.<br><br><br><br><br><br><br><br><br>Forecasting desk in a WFO.<br>WFO computers use the AWIPS forecasting system. <br>The Storm Prediction Center is located in The National Weather Center in Norman, OK.

### DOC[2] (IRRELEVANT) ears/topo_scale4.txt
> ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●●<br>●<br>●<br>●<br>●<br>● ●<br>● ●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●●<br>● ● ●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●●●<br>●●<br>●<br>●<br>●●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>● ●<br>●●●●●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●●●<br>●<br>●<br>● ●<br>●<br>● ● ●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●●<br>● ●<br>●<br>● ●<br>●<br>●<br>●<br>●●<br>●<br>●<br>●<br>●<br>●<br>●●<br>●<br>● ●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>● ●<br>●<br>●●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ● ● ●<br>● ●<br>●●<br>● ●<br>● ●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>● ●●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>● ●<br>●<br>●<br>● ●<br>●<br>●<br>● ●<br>●<br>● ●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>● ● ●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ● ●<br>●<br>●<br>●<br>●<br>●<br>● ●●<br>●<br>●<br>● ● ● ●<br>●<br>●<br>● ●<br>● ●<br>●<br>● ●<br>●<br>● ●<br>●<br>●<br>●<br>● ●<br>● ●<br>●<br>●<br>● ● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>● ●<br>●●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●●●●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ● ● ●<br>●<br>●<br>●<br>● ● ● ●<br>●<br>●<br>● ●<br>● ● ● ●<br>●● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●●<br>●<br>●<br>● ● ●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>●<br>● ●<br>● ● ●<br>●<br>0 5 10 15 20<br>0<br>5 10 15 20<br>(b)<br>Wind Speed OBS (ms−1)<br>Wind Speed MOD (ms−1)<br>R=0.82<br>RMSE=2.06<br>BIAS =−1.03<br>Fig. 6. Comparison of (a) TopoSCALE and (b) TopoSCALE + wind<br>sub-model for ANETZ station at Natschen above Andermatt. The 2-<br>pronged error signature is corrected by the sub-model. This station<br>was chosen for its position on a large slope as opposed peak or ridge<br>where slope angles are difficult to extract accurately from a DEM.<br>Additionally, Fig. 6 gives a comparison of TopoSCALE<br>and TopoSCALE + wind sub-model for the ANETZ station<br>at Natschen above Andermatt. The 2-pronged error signature<br>(i.e. both topographic wind speed reduction and enhancement) is corrected by the sub-model. The wind sub-model is<br>difficult to test widely (all other wind results do not include<br>the sub-model) as topographic location data is often not precise enough for point validation, especially where locations<br>are peaks or ridges (i.e. flat ridge can be extracted as a 45◦<br>north face even on a 25 m DEM, e.g. Fisher et al., 2004).<br>This station was chosen for its position on a large slope well<br>represented at the DEM resolution.<br>For LW↓ (9 stations) there are clear improvements over<br>GRID with REF and TopoSCALE due to high dependence<br>on T . To isolate this affect, we performed an additional test<br>where the Pirazzini model used in REF was driven with<br>TopoSCALE T and Rh to assess how well the emissivity<br>based part of the TopoSCALE approach performed over the<br>parameterisations employed in REF. This gave results of<br>R = 0.88 and RMSE = 27.9, suggesting that the larger part<br>of the improvement given by the TopoSCALE approach was<br>due to the improved description of emissivity at grid-level.<br>Overall, TopoSCALE gives an improved result over REF<br>both when REF is driven by lapse derived T and Rh and<br>TopoSCALE derived T and Rh. However, TopoSCALE gives<br>a slight increase in BIAS over REF of 1.43 Wm−2<br>.<br>5.2 Surface-based results<br>Figure 5 gives the density scatter plot for SW↓grid OBS (9<br>stations) against GRID and TopoSCALE results (MOD) for<br>both all-sky conditions and clear-sky conditions (defined as<br>kt  0.6). Best RMSE performance is seen in clear-sky conditions due to removal of a large proportion of cloud-based<br>uncertainty. However, BIAS is higher in clear-sky conditions due to residual elevation effects affecting the larger<br>direct beam component. TopoSCALE reduces this BIAS by<br>around 3.5 Wm−2<br>as well as improving the RMSE score by<br>www.geosci-model-dev.net/7/387/2014/ Geosci. Model Dev., 7, 387–405, 2014<br>396 J. Fiddes and S. Gruber: TopoSCALE: downscaling gridded climate data in complex terrain<br>Percentage Dry Days<br>Frequency<br>0.30 0.35 0.40 0.45 0.50 0.55 0.60<br>0 2 4 6<br>8<br>GRID<br>VAL<br>1996 1998 2000 2002 2004 2006 2008<br>0 2 4 6 8 10 12<br>Date<br>Wind Speed (ms−1) MOD−OBS<br>IMIS<br>ANETZ<br>(A) (B)<br>OBS<br>Fig. 7. Two problems encountered in the analysis. (a) Several discontinuities were observed in the wind time series i.e period 1999–<br>2001 and 2005–2009. Therefore, the wind analysis was restricted<br>to a three year period (1996–1998) that was stable. (b) A common<br>problem with climate models is a low number of dry days which is<br>compensated for by too high frequency and low intensity precipitation. The percentage dry days of OBS is much higher and distribution not even overlapping that of GRID data (IMIS-Data© 2013,<br>SLF).<br>5 Wm−2<br>. BIAS is also reduced under all-sky conditions by<br>TopoSCALE but more modestly and RMSE score is roughly<br>equal. This indicates that TopoSCALE improves the direct<br>beam component most as corrections focus on this part of<br>the radiation budget. In a separate analysis the Erbs partitioning scheme for direct and diffuse SW↓ was tested.<br>As expected the partitioning scheme adds some uncertainty<br>(i.e. R reduced from 0.88 to 0.81/0.75under all-sky conditions). Results for direct/diffuse are negatively/positively<br>biased (−20.4/17.2 %). Full details are not given here as<br>this topic is well covered in the literature (e.g. Erbs et al.,<br>1982; Ruiz-Arias et al., 2010a). Despite high uncertainty introduced in decomposition models, the reaggregation of solar<br>components after elevation/terrain correction minimizes the<br>potential effects in the final terrain corrected estimates (RuizArias et al., 2010a).<br>Two quantities are important in modelling precipitation,<br>quantity and frequency, the former representing total inputs<br>and the second controlling distribution of those inputs over<br>a given period of time. Figure 7 shows a common problem<br>with climate model precipitation fields – that is “constant<br>drizzle” (i.e. a low number of dry days which is compensated for by high frequency and low intensity precipitation)<br>(Piani et al., 2009; Manders et al., 2012). The percentage dry<br>days of OBS is much higher (and not even overlapping) that<br>of ERA-I GRID data. This cannot be changed by the current scheme as precipitation can not be created (to be conservative), but only distributed to the subgrid according to the<br>scheme. A conservative approach would require a temporal<br>redistribution of precipitation as opposed to the spatial corrections we apply in this study. Figure 8 shows that both REF<br>and TopoSCALE improve the distribution of monthly precipitation totals, especially high intensity events accounting for<br>approximately 50 % of mass inputs (central grey line in the<br>0 200 400 600 800<br>0.000 0.002 0.004 0.006 0.008<br>Monthly Precipitation [mm/month]<br>Density<br>VAL<br>GRID<br>REF<br>TOPOSCALE<br>OBS<br>Fig. 8. PDF of GRID, REF and TopoSCALE precipitation schemes<br>with respect to OBS. Data is monthly precipitation sums over all<br>valid stations. Vertical lines correspond to 25–75 % quantiles of total precipitation mass (OBS). Simulation of high intensity events is<br>improved by REF and TopoSCALE over GRID values.<br>figure). The dominant effect is from the lapse rate as both<br>REF and TopoSCALE distributions are reasonably similar.<br>Figure 9b gives monthly and annual totals for all eligible stations in the OBS data set. The improvement of TopoSCALE<br>with the inclusion of the spatial component over REF (purely<br>lapse-rate based), is evident with improved R, RMSE and<br>BIAS scores. Figure 9 also highlights the improved simulation of both extremes.<br>5.3 Diurnal cycles<br>The diurnal cycle in surface and boundary-layer variables is<br>important for the global climate system (Dai and Trenberth,<br>2004), and particularly in simulating daily variation in the<br>surface energy balance. Figure 10 shows the diurnal cycle<br>of SW↓ and T , two fields characterised by distinctive diurnal cycles, in order to investigate the performance of the<br>scheme at sub-daily timescales. Additionally these fields represent surface (SW↓) and pressure-level (T ) fields. We calculated the average of all 03:00–00:00 UTC 3 h time steps<br>over the entire study period for months of December and<br>June. A subset of OBS stations is presented, representing<br>an elevation range of 370–3580 m a.s.l. In general, the diurnal cycle of SW↓ appears to be well reproduced by ERA-I.<br>However, seasonal differences are apparent with more accurate simulation in June than December as indicated by the<br>full range of values at 12:00 UTC being more comprehensively represented. Lower amplitudes of diurnal cycles in T<br>make the analysis less clear. In December, diurnal cycles are<br>quite strongly smoothed at low elevation whereas cycles are<br>Geosci. Model Dev., 7, 387–405, 2014 www.geosci-model-dev.net/7/387/2014/<br>J. Fiddes and S. Gruber: TopoSCALE: downscaling gridded climate data in complex terrain 397<br>virtually non-existent in the OBS data at high elevation. In<br>June there is a degree of smoothing at low elevations but<br>cycles are generally reproduced. However, at high elevation<br>there is a very strong smoothing. Where TopoSCALE performs less well for T (i.e. winter and high elevation) is likely<br>related to poor representation of surface boundary layer in<br>ERA-I data (cf. Sect. 6.3).<br>5.4 Seasonal error<br>Figure 11 gives boxplots of deviation of daily values of MOD<br>from OBS, as defined above and grouped according to month<br>of the year in order to investigate seasonal patterns in the error signature. No averaging is performed, all daily mean values are considered. Results for T suggests that TopoSCALE<br>is too warm in winter and too cold in summer. The median<br>and the majority of the 25–75 % quantile lie within a 1◦<br>error<br>margin. The boxplot for Rh shows that TopoSCALE greatly<br>reduces the seasonal error signal over both REF and GRID.<br>There is an almost constant small negative bias throughout<br>the year. If a bias correction were applied the 25–75 % quantiles would lie within a 10 % (referring to the unit of Rh) error<br>margin. Results for Ws shows the strong bias correction by<br>TopoSCALE throughout the year with slightly poorer performance in spring. Results for LW↓ show a negligible seasonal pattern in GRID, REF and TopoSCALE. TopoSCALE<br>has a lower magnitude of error. Both REF and TopoSCALE<br>show slightly larger errors in April and May (but with opposing sign). Results for SW↓ show negative bias for both<br>all-sky and clear-sky conditions. This effect is strongest in<br>spring/summer, possibly due to higher magnitudes of values.<br>The TopoSCALE correction is most evident under clear-sky<br>conditions in autumn/winter.<br>5.5 Elevation effects<br>Figure 12 shows the daily mean error of GRID, REF and<br>TopoSCALE results (MOD) with respect to OBS as a function of relative elevation of station (e.g. station elevation –<br>grid elevation). Each box may contain multiple stations as<br>long as they share the same elevation difference from their<br>respective ERA-I grid cell. The plot is binned into 400 m intervals (300 m for radiation). This analysis was performed to<br>investigate any elevation dependency of the error signal as<br>well as to look at the effect of the different methods implemented in the ERA-I model to compute variables on pressure<br>levels above and below the model surface (cf. Sect. 2.2). The<br>red box represents surface data (grid-level ±200 m) in order<br>to investigate the relative performance gain/loss close to the<br>grid-surface. This may point to suitability of TopoSCALE<br>outside of mountain areas.<br>The results for T show larger error for stations below grid<br>(RMSE = 2.46) than above (RMSE = 1.60). This result is<br>also slightly negatively biased. This shows that the extrapolation of data below grid level produces a poorer result<br>and only slightly better than REF. The fact that observations tend to be colder indicates that non-represented subgrid effects, such as inversions, could be significant in driving this bias. Above grid level there are large improvements<br>over REF and GRID. REF shows the expected result that<br>error related to lapse-rate-based approaches increases with<br>the distance over which they are applied. The Rh plot shows<br>that TopoSCALE is increasing positively/negatively biased<br>with distance above/below grid level as compared to REF.<br>However the absolute magnitude of error is much lower. Ws<br>bias in GRID error signature is corrected by TopoSCALE (albeit slightly overcompensated above grid). TopoSCALE performs better below grid level, possibly due to lower absolute<br>Ws magnitudes leading to lower error values.<br>Looking only at surface data (red-box), pressure-levelbased results (i.e. TopoSCALE) outperform surface databased results (i.e. GRID and REF) in all cases but most significantly in T and Ws. This is quite surprising as it would<br>seem likely that the surface data should contain more of the<br>boundary layer effect (cf. Sect. 6.3). However, this indicates<br>that TopoSCALE could also be usefully applied in locations<br>close to grid level without reduced quality over surface-based<br>data.<br>6 Discussion<br>Reanalyses are complex products in that they combine a climate model with observations. This section provides a discussion of key issues relevant to the use of reanalysis and<br>other climate data sets, in order to place this study and results<br>in context, as well as to highlight some important limitations<br>to this approach.<br>6.1 Assimilation issues<br>Reanalyses assimilate a large number of observations in spatially and temporally varying quantities and densities. It is<br>therefore important to know which observational data sets<br>are assimilated into the ERA-I product, as this not only affects how independent observations are in terms of validation, but will also suggest how the performance of ERA-I<br>(and therefore TopoSCALE) can vary with observation density. Assimilated data that is also used for evaluation in this<br>study originates from the SYNOP registered MeteoSchweiz<br>stations (a subset of the ANETZ network), and only affects<br>observations of air temperature and relative humidity (Table 1). Furthermore, for screen-level analysis (2 m temperature, 2 m relative humidity) surface observations that differ<br>by more that 300 m from the model orography are rejected in<br>the ERA-I assimilation.<br>6.2 Bias and spatial-temporal variability of errors<br>LSM results are sensitive to bias in climate data (e.g. Berg,<br>2003), which may result in unrealistic estimates of mass,<br>www.geosci-model-dev.net/7/387/2014/ Geosci. Model Dev., 7, 387–405, 2014<br>398 J. Fiddes and S. Gruber: TopoSCALE: downscaling gridded climate data in complex terrain<br>Precipitation OBS mm Month-1<br>Precipitation MOD mm Month-1<br>100<br>200<br>300<br>400<br>500<br>100 200 300 400 500<br>R=0.6<br>RMSE=75<br>BIAS =-16.65<br>GRID<br>100 200 300 400 500<br>R=0.59<br>RMSE=78.4<br>BIAS =2.9<br>REF<br>100 200 300 400 500<br>R=0.65<br>RMSE=69.7<br>BIAS =1.8<br>TOPOSCALE<br>Precipitation OBS mm Year-1

### DOC[3] (IRRELEVANT) number_of_hadley_cell/Hadley_cell3.txt
> part3 -------------------<br>Formulation and discovery[edit]<br>The broad structure and mechanism of the Hadley circulation – comprising convective cells moving air due to temperature differences in a manner influenced by the Earth's rotation – was first proposed by Edmund Halley in 1685 and George Hadley in 1735. Hadley had sought to explain the physical mechanism for the trade winds and the westerlies; the Hadley circulation and the Hadley cells are named in honor of his pioneering work. Although Hadley's ideas invoked physical concepts that would not be formalized until well after his death, his model was largely qualitative and without mathematical rigor. Hadley's formulation was later recognized by most meteorologists by the 1920s to be a simplification of more complicated atmospheric processes. The Hadley circulation may have been the first attempt to explain the global distribution of winds in Earth's atmosphere using physical processes. However, Hadley's hypothesis could not be verified without observations of winds in the upper-atmosphere. Data collected by routine radiosondes beginning in the mid-20th century confirmed the existence of the Hadley circulation.<br>Early explanations of the trade winds[edit]<br>Attempts to explain the trade winds were motivated by their steadiness and importance to maritime trade.<br>In the 15th and 16th centuries, observations of maritime weather conditions were of considerable importance to maritime transport. Compilations of these observations showed consistent weather conditions from year to year and significant seasonal variability. The prevalence of dry conditions and weak winds at around 30° latitude and the equatorward trade winds closer to the equator, mirrored in the northern and southern hemispheres, was apparent by 1600. Early efforts by scientists to explain aspects of global wind patterns often focused on the trade winds as the steadiness of the winds was assumed to portend a simple physical mechanism. Galileo Galilei proposed that the trade winds resulted from the atmosphere lagging behind the Earth's faster tangential rotation speed in the low latitudes, resulting in the westward trades directed opposite of Earth's rotation.<br>In 1685, English polymath Edmund Halley proposed at a debate organized by the Royal Society that the trade winds resulted from east to west temperature differences produced over the course of a day within the tropics. In Halley's model, as the Earth rotated, the location of maximum heating from the Sun moved west across the Earth's surface. This would cause air to rise, and by conservation of mass, Halley argued that air would be moved to the region of evacuated air, generating the trade winds. Halley's hypothesis was criticized by his friends, who noted that his model would lead to changing wind directions throughout the course of a day rather than the steady trade winds. Halley conceded in personal correspondence with John Wallis that "Your questioning my hypothesis for solving the Trade Winds makes me less confident of the truth thereof". Nonetheless, Halley's formulation was incorporated into Chambers's Encyclopaedia and La Grande Encyclopédie, becoming the most widely-known explanation for the trade winds until the early 19th century. Though his explanation of the trade winds was incorrect, Halley correctly predicted that the surface trade winds should be accompanied by an opposing flow aloft following mass conservation.<br>George Hadley's explanation[edit]<br>Hadley's conception of the atmospheric circulation involved large, hemisphere-spanning circulations<br>Unsatisfied with preceding explanations for the trade winds, George Hadley proposed an alternate mechanism in 1735. Hadley's hypothesis was published in the paper "On the Cause of the General Trade Winds" in Philosophical Transactions of the Royal Society. Like Halley, Hadley's explanation viewed the trade winds as a manifestation of air moving to take the place of rising warm air. However, the region of rising air prompting this flow lay along the lower latitudes. Understanding that the tangential rotation speed of the Earth was fastest at the equator and slowed farther poleward, Hadley conjectured that as air with lower momentum from higher latitudes moved equatorward to replace the rising air, it would conserve its momentum and thus curve west. By the same token, the rising air with higher momentum would spread poleward, curving east and then sinking as it cooled to produce westerlies in the mid-latitudes. Hadley's explanation implied the existence of hemisphere-spanning circulation cells in the northern and southern hemispheres extending from the equator to the poles, though he relied on an idealization of Earth's atmosphere that lacked seasonality or the asymmetries of the oceans and continents. His model also predicted rapid easterly trade winds of around 37 m/s (130 km/h; 83 mph), though he argued that the action of surface friction over the course of a few days slowed the air to the observed wind speeds. Colin Maclaurin extended Hadley's model to the ocean in 1740, asserting that meridional ocean currents were subject to similar westward or eastward deflections.<br>Hadley was not widely associated with his theory due to conflation with his older brother, John Hadley, and Halley; his theory failed to gain much traction in the scientific community for over a century due to its unintuitive explanation and the lack of validating observations. Several other natural philosophers independently forwarded explanations for the global distribution of winds soon after Hadley's 1735 proposal. In 1746, Jean le Rond d'Alembert provided a mathematical formulation for global winds, but disregarded solar heating and attributed the winds to the gravitational effects of the Sun and Moon. Immanuel Kant, also unsatisfied with Halley's explanation for the trade winds, published an explanation for the trade winds and westerlies in 1756 with similar reasoning as Hadley. In the latter part of the 18th century, Pierre-Simon Laplace developed a set of equations establishing a direct influence of Earth's rotation on wind direction. Swiss scientist Jean-André Deluc published an explanation of the trade winds in 1787 similar to Hadley's hypothesis, connecting differential heating and the Earth's rotation with the direction of the winds.<br>English chemist John Dalton was the first to clearly credit Hadley's explanation of the trade winds to George Hadley, mentioning Hadley's work in his 1793 book Meteorological Observations and Essays. In 1837, Philosophical Magazine published a new theory of wind currents developed by Heinrich Wilhelm Dove without reference to Hadley but similarly explaining the direction of the trade winds as being influenced by the Earth's rotation. In response, Dalton later wrote a letter to the editor to the journal promoting Hadley's work. Dove subsequently credited Hadley so frequently that the overarching theory became known as the "Hadley–Dove principle", popularizing Hadley's explanation for the trade winds in Germany and Great Britain.<br>Critique of Hadley's explanation[edit]<br>Routine radiosonde samples of the upper troposphere beginning in the 20th century provided the first direct observational evidence of the Hadley circulation.<br>The work of Gustave Coriolis, William Ferrel, Jean Bernard Foucault, and Henrik Mohn in the 19th century helped establish the Coriolis force as the mechanism for the deflection of winds due to Earth's rotation, emphasizing the conservation of angular momentum in directing flows rather than the conservation of linear momentum as Hadley suggested; Hadley's assumption led to an underestimation of the deflection by a factor of two. The acceptance of the Coriolis force in shaping global winds led to debate among German atmospheric scientists beginning in the 1870s over the completeness and validity of Hadley's explanation, which narrowly explained the behavior of initially meridional motions. Hadley's use of surface friction to explain why the trade winds were much slower than his theory would predict was seen as a key weakness in his ideas. The southwesterly motions observed in cirrus clouds at around 30°N further discounted Hadley's theory as their movement was far slower than the theory would predict when accounting for the conservation of angular momentum. In 1899, William Morris Davis, a professor of physical geography at Harvard University, gave a speech at the Royal Meteorological Society criticizing Hadley's theory for its failure to account for the transition of an initially unbalanced flow to geostrophic balance. Davis and other meteorologists in the 20th century recognized that the movement of air parcels along Hadley's envisaged circulation was sustained by a constant interplay between the pressure gradient and Coriolis forces rather than the conservation of angular momentum alone. Ultimately, while the atmospheric science community considered the general ideas of Hadley's principle valid, his explanation was viewed as a simplification of more complex physical processes.<br>Hadley's model of the global atmospheric circulation being characterized by hemisphere-wide circulation cells was also challenged by weather observations showing a zone of high pressure in the subtropics and a belt of low pressure at around 60° latitude. This pressure distribution would imply a poleward flow near the surface in the mid-latitudes rather than an equatorward flow implied by Hadley's envisioned cells. Ferrel and James Thomson later reconciled the pressure pattern with Hadley's model by proposing a circulation cell limited to lower altitudes in the mid-latitudes and nestled within the broader, hemisphere-wide Hadley cells. Carl-Gustaf Rossby proposed in 1947 that the Hadley circulation was limited to the tropics, forming one part of a dynamically-driven and multi-celled meridional flow. Rossby's model resembled that of a similar three-celled model developed by Ferrel in 1860.<br>Direct observation[edit]<br>The three-celled model of the global atmospheric circulation – with Hadley's conceived circulation forming its tropical component – had been widely accepted by the meteorological community by the early 20th century. However, the Hadley cell's existence was only validated by weather observations near the surface, and its predictions of winds in the upper troposphere remained untested. The routine sampling of the upper troposphere by radiosondes that emerged in the mid-20th century confirmed the existence of meridional overturning cells in the atmosphere.

### DOC[4] (GROUND TRUTH) -999_celsius_degree/Absolute_zero6.txt
> part6 -------------------<br>History[edit]<br>Robert Boyle pioneered the idea of an absolute zero.<br>One of the first to discuss the possibility of an absolute minimal temperature was Robert Boyle. His 1665 New Experiments and Observations touching Cold, articulated the dispute known as the primum frigidum. The concept was well known among naturalists of the time. Some contended an absolute minimum temperature occurred within earth (as one of the four classical elements), others within water, others air, and some more recently within nitre. But all of them seemed to agree that, "There is some body or other that is of its own nature supremely cold and by participation of which all other bodies obtain that quality."<br>Limit to the "degree of cold"[edit]<br>The question whether there is a limit to the degree of coldness possible, and, if so, where the zero must be placed, was first addressed by the French physicist Guillaume Amontons in 1703, in connection with his improvements in the air thermometer. His instrument indicated temperatures by the height at which a certain mass of air sustained a column of mercury—the pressure, or "spring" of the air varying with temperature. Amontons therefore argued that the zero of his thermometer would be that temperature at which the spring of the air was reduced to nothing. He used a scale that marked the boiling point of water at +73 and the melting point of ice at +51+1⁄2, so that the zero was equivalent to about −240 on the Celsius scale. Amontons held that the absolute zero cannot be reached, so never attempted to compute it explicitly.<br>The value of −240 °C, or "431 divisions [in Fahrenheit's thermometer] below the cold of freezing water" was published by George Martine in 1740.<br>This close approximation to the modern value of −273.15 °C for the zero of the air thermometer was further improved upon in 1779 by Johann Heinrich Lambert, who observed that −270 °C (−454.00 °F; 3.15 K) might be regarded as absolute cold.<br>Values of this order for the absolute zero were not, however, universally accepted about this period. Pierre-Simon Laplace and Antoine Lavoisier, in their 1780 treatise on heat, arrived at values ranging from 1,500 to 3,000 below the freezing point of water, and thought that in any case it must be at least 600 below. John Dalton in his Chemical Philosophy gave ten calculations of this value, and finally adopted −3,000 °C as the natural zero of temperature.<br>Charles's law[edit]<br>From 1787 to 1802, it was determined by Jacques Charles (unpublished), John Dalton, and Joseph Louis Gay-Lussac that, at constant pressure, ideal gases expanded or contracted their volume linearly (Charles's law) by about 1/273 parts per degree Celsius of temperature's change up or down, between 0° and 100° C. This suggested that the volume of a gas cooled at about −273 °C would reach zero.<br>Lord Kelvin's work[edit]<br>After James Prescott Joule had determined the mechanical equivalent of heat, Lord Kelvin approached the question from an entirely different point of view, and in 1848 devised a scale of absolute temperature that was independent of the properties of any particular substance and was based on Carnot's theory of the Motive Power of Heat and data published by Henri Victor Regnault. It followed from the principles on which this scale was constructed that its zero was placed at −273 °C, at almost precisely the same point as the zero of the air thermometer, where the air volume would reach "nothing". This value was not immediately accepted; values ranging from −271.1 °C (−455.98 °F) to −274.5 °C (−462.10 °F), derived from laboratory measurements and observations of astronomical refraction, remained in use in the early 20th century.<br>The race to absolute zero[edit]<br><br>See also: Timeline of low-temperature technology<br>Commemorative plaque in Leiden<br>With a better theoretical understanding of absolute zero, scientists were eager to reach this temperature in the lab. By 1845, Michael Faraday had managed to liquefy most gases then known to exist, and reached a new record for lowest temperatures by reaching −130 °C (−202 °F; 143 K). Faraday believed that certain gases, such as oxygen, nitrogen, and hydrogen, were permanent gases and could not be liquefied. Decades later, in 1873 Dutch theoretical scientist Johannes Diderik van der Waals demonstrated that these gases could be liquefied, but only under conditions of very high pressure and very low temperatures. In 1877, Louis Paul Cailletet in France and Raoul Pictet in Switzerland succeeded in producing the first droplets of liquid air at −195 °C (−319.0 °F; 78.1 K). This was followed in 1883 by the production of liquid oxygen −218 °C (−360.4 °F; 55.1 K) by the Polish professors Zygmunt Wróblewski and Karol Olszewski.<br>Scottish chemist and physicist James Dewar and Dutch physicist Heike Kamerlingh Onnes took on the challenge to liquefy the remaining gases, hydrogen and helium. In 1898, after 20 years of effort, Dewar was the first to liquefy hydrogen, reaching a new low-temperature record of −252 °C (−421.6 °F; 21.1 K). However, Kamerlingh Onnes, his rival, was the first to liquefy helium, in 1908, using several precooling stages and the Hampson–Linde cycle. He lowered the temperature to the boiling point of helium −269 °C (−452.20 °F; 4.15 K). By reducing the pressure of the liquid helium, he achieved an even lower temperature, near 1.5 K. These were the coldest temperatures achieved on Earth at the time and his achievement earned him the Nobel Prize in 1913. Kamerlingh Onnes would continue to study the properties of materials at temperatures near absolute zero, describing superconductivity and superfluids for the first time.

### DOC[5] (IRRELEVANT) humidity_and_rain/humidity_when_rains3_42.txt
> p<strongNote in this second example</strong<brNote in this second example from Allison, the actual air temperature is 80 F, it is raining, and the relative humidity is 75%.  This means that the raindrops are at or below the dewpoint temperature of 71 F. This all makes sense.  But the “Feels Like” temperature is 84 F which is higher than the air temperature.  Why?  Because it is calculated based on humidity which is still relatively high.  Do you think that if you were standing in that rain you would feel


## Ground Truth

### GROUND TRUTH 0, ranked 53, -999_celsius_degree/Lowest_temperature_recorded_on_Earth1.txt
> part1 -------------------<br><br>The lowest natural temperature ever directly recorded at ground level on Earth is −89.2 °C (−128.6 °F; 184.0 K) at the then-Soviet Vostok Station in Antarctica on 21 July 1983 by ground measurements.<br>On 10 August 2010, satellite observations showed a surface temperature of −92 °C (−134 °F; 181 K) at 81°48′S 59°18′E﻿ / ﻿81.8°S 59.3°E﻿ / -81.8; 59.3, along a ridge between Dome Argus and Dome Fuji, at 3,900 m (12,800 ft) elevation. The result was reported at the 46th annual meeting of the American Geophysical Union in San Francisco in December 2013; it is a provisional figure, and may be subject to revision. The value is not listed as the record lowest temperature as it was measured by remote sensing from satellite and not by ground-based thermometers, unlike the 1983 record. The temperature announced reflects that of the ice surface, while the Vostok readings measured the air above the ice, and so the two are not directly comparable. More recent work shows many locations in the high Antarctic where surface temperatures drop to approximately −98 °C (−144 °F; 175 K). Due to the very strong temperature gradient near the surface, these imply near-surface air temperature minima of approximately −94 °C (−137 °F; 179 K).

### GROUND TRUTH 1, ranked not in top 100, -999_celsius_degree/Absolute_zero4.txt
> part4 -------------------<br>Absolute temperature scales[edit]<br>Absolute, or thermodynamic, temperature is conventionally measured in kelvin (Celsius-scaled increments) and in the Rankine scale (Fahrenheit-scaled increments) with increasing rarity. Absolute temperature measurement is uniquely determined by a multiplicative constant which specifies the size of the degree, so the ratios of two absolute temperatures, T2/T1, are the same in all scales. The most transparent definition of this standard comes from the Maxwell–Boltzmann distribution. It can also be found in Fermi–Dirac statistics (for particles of half-integer spin) and Bose–Einstein statistics (for particles of integer spin). All of these define the relative numbers of particles in a system as decreasing exponential functions of energy (at the particle level) over kT, with k representing the Boltzmann constant and T representing the temperature observed at the macroscopic level.

### GROUND TRUTH 2, ranked not in top 100, -999_celsius_degree/Absolute_zero1.txt
> part1 -------------------<br><br>Absolute zero is the lowest limit of the thermodynamic temperature scale; a state at which the enthalpy and entropy of a cooled ideal gas reach their minimum value, taken as zero kelvin. The fundamental particles of nature have minimum vibrational motion, retaining only quantum mechanical, zero-point energy-induced particle motion. The theoretical temperature is determined by extrapolating the ideal gas law; by international agreement, absolute zero is taken as −273.15 degrees on the Celsius scale (International System of Units), which equals −459.67 degrees on the Fahrenheit scale (United States customary units or imperial units). The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition.<br>It is commonly thought of as the lowest temperature possible, but it is not the lowest enthalpy state possible, because all real substances begin to depart from the ideal gas when cooled as they approach the change of state to liquid, and then to solid; and the sum of the enthalpy of vaporization (gas to liquid) and enthalpy of fusion (liquid to solid) exceeds the ideal gas's change in enthalpy to absolute zero. In the quantum-mechanical description, matter at absolute zero is in its ground state, the point of lowest internal energy.<br>The laws of thermodynamics indicate that absolute zero cannot be reached using only thermodynamic means, because the temperature of the substance being cooled approaches the temperature of the cooling agent asymptotically. Even a system at absolute zero, if it could somehow be achieved, would still possess quantum mechanical zero-point energy, the energy of its ground state at absolute zero; the kinetic energy of the ground state cannot be removed.<br>Scientists and technologists routinely achieve temperatures close to absolute zero, where matter exhibits quantum effects such as Bose–Einstein condensate, superconductivity and superfluidity.

### GROUND TRUTH 3, ranked not in top 100, -999_celsius_degree/Lowest_temperature_recorded_on_Earth3.txt
> part3 -------------------<br>Laboratory cooling[edit]<br>Further information: Absolute zero § History<br>Early experiments[edit]<br>In 1904 Dutch scientist Heike Kamerlingh Onnes created a special lab in Leiden in the Netherlands with the aim of producing liquid helium. In 1908 he managed to lower the temperature to less than −269 °C (−452.2 F, 4 K), which is four degrees above absolute zero. Only in this exceptionally cold state will helium liquefy, the boiling point of helium being at −268.94 °C (−452.092 F). Kamerlingh Onnes received a Nobel Prize for his achievement.<br>Onnes' method relied upon depressurising the subject gases, causing them to cool by adiabatic cooling. This follows from the first law of thermodynamics;<br>Δ<br>U<br>=<br>Δ<br>Q<br>−<br>Δ<br>W<br><br><br>{\displaystyle \Delta U=\Delta Q-\Delta W}<br>where U = internal energy, Q = heat added to the system, W = work done by the system.<br>Consider a gas in a box of set volume. If the pressure in the box is higher than atmospheric pressure, then upon opening the gas will do work on the surrounding atmosphere to expand. As this expansion is adiabatic and the gas has done work<br>Δ<br>Q<br>=<br>0<br><br><br>{\displaystyle \Delta Q=0}<br>Δ<br>W<br><br>0<br><br><br>{\displaystyle \Delta W0}<br>⇒<br>Δ<br>U<br><<br>0<br><br><br>{\displaystyle \Rightarrow \Delta U<0}<br>Now as the internal energy has decreased, so has the temperature.<br>Modern experiments[edit]<br>As of November 2000, nuclear spin temperatures below 100 pK were reported for an experiment at the Helsinki University of Technology Low Temperature Lab. However, this was the temperature of one particular type of motion—a quantum property called nuclear spin—not the overall average thermodynamic temperature for all possible degrees of freedom. At such low temperatures, the concept of "temperature" becomes multifaceted since molecular motion cannot be assumed to average out across degrees of freedom. The corresponding peak emission will be in radio waves, rather than in the familiar infrared, so it is very inefficiently absorbed by neighboring atoms, making it difficult to reach thermal equilibrium.<br>The Low Temperature Laboratory recorded a record low temperature of 100 pK, or 1.0 × 10 K in 1999.<br>The current apparatus for achieving low temperatures has two stages. The first utilizes a helium dilution refrigerator to get to temperatures of millikelvins, then the next stage uses adiabatic nuclear demagnetisation to reach picokelvins.<br>Extremely low temperatures are useful for observation of quantum mechanical phases of matter such as superfluids and Bose–Einstein condensates, which would be disrupted by thermal motion.

### GROUND TRUTH 4, ranked 3, -999_celsius_degree/Absolute_zero6.txt
> part6 -------------------<br>History[edit]<br>Robert Boyle pioneered the idea of an absolute zero.<br>One of the first to discuss the possibility of an absolute minimal temperature was Robert Boyle. His 1665 New Experiments and Observations touching Cold, articulated the dispute known as the primum frigidum. The concept was well known among naturalists of the time. Some contended an absolute minimum temperature occurred within earth (as one of the four classical elements), others within water, others air, and some more recently within nitre. But all of them seemed to agree that, "There is some body or other that is of its own nature supremely cold and by participation of which all other bodies obtain that quality."<br>Limit to the "degree of cold"[edit]<br>The question whether there is a limit to the degree of coldness possible, and, if so, where the zero must be placed, was first addressed by the French physicist Guillaume Amontons in 1703, in connection with his improvements in the air thermometer. His instrument indicated temperatures by the height at which a certain mass of air sustained a column of mercury—the pressure, or "spring" of the air varying with temperature. Amontons therefore argued that the zero of his thermometer would be that temperature at which the spring of the air was reduced to nothing. He used a scale that marked the boiling point of water at +73 and the melting point of ice at +51+1⁄2, so that the zero was equivalent to about −240 on the Celsius scale. Amontons held that the absolute zero cannot be reached, so never attempted to compute it explicitly.<br>The value of −240 °C, or "431 divisions [in Fahrenheit's thermometer] below the cold of freezing water" was published by George Martine in 1740.<br>This close approximation to the modern value of −273.15 °C for the zero of the air thermometer was further improved upon in 1779 by Johann Heinrich Lambert, who observed that −270 °C (−454.00 °F; 3.15 K) might be regarded as absolute cold.<br>Values of this order for the absolute zero were not, however, universally accepted about this period. Pierre-Simon Laplace and Antoine Lavoisier, in their 1780 treatise on heat, arrived at values ranging from 1,500 to 3,000 below the freezing point of water, and thought that in any case it must be at least 600 below. John Dalton in his Chemical Philosophy gave ten calculations of this value, and finally adopted −3,000 °C as the natural zero of temperature.<br>Charles's law[edit]<br>From 1787 to 1802, it was determined by Jacques Charles (unpublished), John Dalton, and Joseph Louis Gay-Lussac that, at constant pressure, ideal gases expanded or contracted their volume linearly (Charles's law) by about 1/273 parts per degree Celsius of temperature's change up or down, between 0° and 100° C. This suggested that the volume of a gas cooled at about −273 °C would reach zero.<br>Lord Kelvin's work[edit]<br>After James Prescott Joule had determined the mechanical equivalent of heat, Lord Kelvin approached the question from an entirely different point of view, and in 1848 devised a scale of absolute temperature that was independent of the properties of any particular substance and was based on Carnot's theory of the Motive Power of Heat and data published by Henri Victor Regnault. It followed from the principles on which this scale was constructed that its zero was placed at −273 °C, at almost precisely the same point as the zero of the air thermometer, where the air volume would reach "nothing". This value was not immediately accepted; values ranging from −271.1 °C (−455.98 °F) to −274.5 °C (−462.10 °F), derived from laboratory measurements and observations of astronomical refraction, remained in use in the early 20th century.<br>The race to absolute zero[edit]<br><br>See also: Timeline of low-temperature technology<br>Commemorative plaque in Leiden<br>With a better theoretical understanding of absolute zero, scientists were eager to reach this temperature in the lab. By 1845, Michael Faraday had managed to liquefy most gases then known to exist, and reached a new record for lowest temperatures by reaching −130 °C (−202 °F; 143 K). Faraday believed that certain gases, such as oxygen, nitrogen, and hydrogen, were permanent gases and could not be liquefied. Decades later, in 1873 Dutch theoretical scientist Johannes Diderik van der Waals demonstrated that these gases could be liquefied, but only under conditions of very high pressure and very low temperatures. In 1877, Louis Paul Cailletet in France and Raoul Pictet in Switzerland succeeded in producing the first droplets of liquid air at −195 °C (−319.0 °F; 78.1 K). This was followed in 1883 by the production of liquid oxygen −218 °C (−360.4 °F; 55.1 K) by the Polish professors Zygmunt Wróblewski and Karol Olszewski.<br>Scottish chemist and physicist James Dewar and Dutch physicist Heike Kamerlingh Onnes took on the challenge to liquefy the remaining gases, hydrogen and helium. In 1898, after 20 years of effort, Dewar was the first to liquefy hydrogen, reaching a new low-temperature record of −252 °C (−421.6 °F; 21.1 K). However, Kamerlingh Onnes, his rival, was the first to liquefy helium, in 1908, using several precooling stages and the Hampson–Linde cycle. He lowered the temperature to the boiling point of helium −269 °C (−452.20 °F; 4.15 K). By reducing the pressure of the liquid helium, he achieved an even lower temperature, near 1.5 K. These were the coldest temperatures achieved on Earth at the time and his achievement earned him the Nobel Prize in 1913. Kamerlingh Onnes would continue to study the properties of materials at temperatures near absolute zero, describing superconductivity and superfluids for the first time.

### GROUND TRUTH 5, ranked not in top 100, -999_celsius_degree/Absolute_zero3.txt
> part3 -------------------<br>Relation with Bose–Einstein condensate[edit]<br><br>Main article: Bose–Einstein condensate<br>Velocity-distribution data of a gas of rubidium atoms at a temperature within a few billionths of a degree above absolute zero. Left: just before the appearance of a Bose–Einstein condensate. Center: just after the appearance of the condensate. Right: after further evaporation, leaving a sample of nearly pure condensate.<br>A Bose–Einstein condensate (BEC) is a state of matter of a dilute gas of weakly interacting bosons confined in an external potential and cooled to temperatures very near absolute zero. Under such conditions, a large fraction of the bosons occupy the lowest quantum state of the external potential, at which point quantum effects become apparent on a macroscopic scale.<br>This state of matter was first predicted by Satyendra Nath Bose and Albert Einstein in 1924–25. Bose first sent a paper to Einstein on the quantum statistics of light quanta (now called photons). Einstein was impressed, translated the paper from English to German and submitted it for Bose to the Zeitschrift für Physik, which published it. Einstein then extended Bose's ideas to material particles (or matter) in two other papers.<br>Seventy years later, in 1995, the first gaseous condensate was produced by Eric Cornell and Carl Wieman at the University of Colorado at Boulder NIST-JILA lab, using a gas of rubidium atoms cooled to 170 nanokelvin (nK) (1.7×10 K).<br>A record cold temperature of 450 ± 80 picokelvin (pK) (4.5×10 K) in a BEC of sodium atoms was achieved in 2003 by researchers at the Massachusetts Institute of Technology (MIT). The associated black-body (peak emittance) wavelength of 6,400 kilometers is roughly the radius of Earth.

### GROUND TRUTH 6, ranked not in top 100, -999_celsius_degree/Lowest_temperature_recorded_on_Earth2.txt
> part2 -------------------<br>Historical progression[edit]<br>On 21 January 1838, a Russian merchant named Neverov recorded a temperature of −60 °C (−76 °F; 213 K) in Yakutsk. On 15 January 1885, H. Wild reported that a temperature of −68 °C (−90 °F; 205 K) was measured in Verkhoyansk. A later measurement at the same place in February 1892 was reported as −69.8 °C (−93.6 °F; 203.3 K). Soviet researchers later announced a recording of −67.7 °C (−89.9 °F; 205.5 K) in February 1933 at Oymyakon, about 650 km (400 mi) to the south-east of Verkhoyansk; this measurement was reported by Soviet texts through the 1940s as a record low, with the previous measurement from Verkhoyansk retroactively adjusted to −67.6 °C (−89.7 °F; 205.6 K).<br>The next reliable measurement was made during the 1957 season at the Amundsen–Scott South Pole Station in Antarctica, yielding −73.6 °C (−100.5 °F; 199.6 K) on 11 May and −74.5 °C (−102.1 °F; 198.7 K) on 17 September. The next world record low temperature was a reading of −88.3 °C (−126.9 °F; 184.8 K), measured at the Soviet Vostok Station in 1968, on the Antarctic Plateau. Vostok again broke its own record with a reading of −89.2 °C (−128.6 °F; 184.0 K) on 21 July 1983. This remains the record for a directly recorded temperature.

### GROUND TRUTH 7, ranked 36, -999_celsius_degree/Absolute_zero7.txt
> part7 -------------------<br>Very low temperatures[edit]<br>The rapid expansion of gases leaving the Boomerang Nebula, a bi-polar, filamentary, likely proto-planetary nebula in Centaurus, has a temperature of 1 K, the lowest observed outside of a laboratory.<br>The average temperature of the universe today is approximately 2.73 kelvins (−454.76 °F), or about −270.42 °C, based on measurements of cosmic microwave background radiation. Standard models of the future expansion of the universe predict that the average temperature of the universe is decreasing over time. This temperature is calculated as the mean density of energy in space; it should not be confused with the mean electron temperature (total energy divided by particle count) which has increased over time.<br>Absolute zero cannot be achieved, although it is possible to reach temperatures close to it through the use of evaporative cooling, cryocoolers, dilution refrigerators, and nuclear adiabatic demagnetization. The use of laser cooling has produced temperatures of less than a billionth of a kelvin. At very low temperatures in the vicinity of absolute zero, matter exhibits many unusual properties, including superconductivity, superfluidity, and Bose–Einstein condensation. To study such phenomena, scientists have worked to obtain even lower temperatures.<br>In November 2000, nuclear spin temperatures below 100 pK were reported for an experiment at the Helsinki University of Technology's Low Temperature Lab in Espoo, Finland. However, this was the temperature of one particular degree of freedom—a quantum property called nuclear spin—not the overall average thermodynamic temperature for all possible degrees in freedom.<br>In February 2003, the Boomerang Nebula was observed to have been releasing gases at a speed of 500,000 km/h (310,000 mph) for the last 1,500 years. This has cooled it down to approximately 1 K, as deduced by astronomical observation, which is the lowest natural temperature ever recorded.<br>In November 2003, 90377 Sedna was discovered and is one of the coldest known objects in the Solar System. With an average surface temperature of -400°F (-240°C), due to its extremely far orbit of 903 astronomical units.<br>In May 2005, the European Space Agency proposed research in space to achieve femtokelvin temperatures.<br>In May 2006, the Institute of Quantum Optics at the University of Hannover gave details of technologies and benefits of femtokelvin research in space.<br>In January 2013, physicist Ulrich Schneider of the University of Munich in Germany reported to have achieved temperatures formally below absolute zero ("negative temperature") in gases. The gas is artificially forced out of equilibrium into a high potential energy state, which is, however, cold. When it then emits radiation it approaches the equilibrium, and can continue emitting despite reaching formal absolute zero; thus, the temperature is formally negative.<br>In September 2014, scientists in the CUORE collaboration at the Laboratori Nazionali del Gran Sasso in Italy cooled a copper vessel with a volume of one cubic meter to 0.006 kelvins (−273.144 °C; −459.659 °F) for 15 days, setting a record for the lowest temperature in the known universe over such a large contiguous volume.<br>In June 2015, experimental physicists at MIT cooled molecules in a gas of sodium potassium to a temperature of 500 nanokelvin, and it is expected to exhibit an exotic state of matter by cooling these molecules somewhat further.<br>In 2017, Cold Atom Laboratory (CAL), an experimental instrument was developed for launch to the International Space Station (ISS) in 2018. The instrument has created extremely cold conditions in the microgravity environment of the ISS leading to the formation of Bose–Einstein condensates. In this space-based laboratory, temperatures as low as 1 picokelvin (10 K) are projected to be achievable, and it could further the exploration of unknown quantum mechanical phenomena and test some of the most fundamental laws of physics.<br>The current world record for effective temperatures was set in 2021 at 38 picokelvin (pK), or 0.000000000038 of a kelvin, through matter-wave lensing of rubidium Bose–Einstein condensates.

### GROUND TRUTH 8, ranked not in top 100, -999_celsius_degree/Absolute_zero5.txt
> part5 -------------------<br>Negative temperatures[edit]<br><br>Main article: Negative temperature<br>Temperatures that are expressed as negative numbers on the familiar Celsius or Fahrenheit scales are simply colder than the zero points of those scales. Certain systems can achieve truly negative temperatures; that is, their thermodynamic temperature (expressed in kelvins) can be of a negative quantity. A system with a truly negative temperature is not colder than absolute zero. Rather, a system with a negative temperature is hotter than any system with a positive temperature, in the sense that if a negative-temperature system and a positive-temperature system come in contact, heat flows from the negative to the positive-temperature system.<br>Most familiar systems cannot achieve negative temperatures because adding energy always increases their entropy. However, some systems have a maximum amount of energy that they can hold, and as they approach that maximum energy their entropy actually begins to decrease. Because temperature is defined by the relationship between energy and entropy, such a system's temperature becomes negative, even though energy is being added. As a result, the Boltzmann factor for states of systems at negative temperature increases rather than decreases with increasing state energy. Therefore, no complete system, i.e. including the electromagnetic modes, can have negative temperatures, since there is no highest energy state, so that the sum of the probabilities of the states would diverge for negative temperatures. However, for quasi-equilibrium systems (e.g. spins out of equilibrium with the electromagnetic field) this argument does not apply, and negative effective temperatures are attainable.<br>On 3 January 2013, physicists announced that for the first time they had created a quantum gas made up of potassium atoms with a negative temperature in motional degrees of freedom.

### GROUND TRUTH 9, ranked not in top 100, -999_celsius_degree/Absolute_zero2.txt
> part2 -------------------<br>Thermodynamics near absolute zero[edit]<br>At temperatures near 0 K (−273.15 °C; −459.67 °F), nearly all molecular motion ceases and ΔS = 0 for any adiabatic process, where S is the entropy. In such a circumstance, pure substances can (ideally) form perfect crystals with no structural imperfections as T → 0. Max Planck's strong form of the third law of thermodynamics states the entropy of a perfect crystal vanishes at absolute zero. The original Nernst heat theorem makes the weaker and less controversial claim that the entropy change for any isothermal process approaches zero as T → 0:<br>lim<br><br>T<br>→<br>0<br><br><br>Δ<br>S<br>=<br>0<br><br><br>{\displaystyle \lim _{T\to 0}\Delta S=0}<br>The implication is that the entropy of a perfect crystal approaches a constant value. An adiabat is a state with constant entropy, typically represented on a graph as a curve in a manner similar to isotherms and isobars.<br>The Nernst postulate identifies the isotherm T = 0 as coincident with the adiabat S = 0, although other isotherms and adiabats are distinct. As no two adiabats intersect, no other adiabat can intersect the T = 0 isotherm. Consequently no adiabatic process initiated at nonzero temperature can lead to zero temperature. (≈ Callen, pp. 189–190)<br>A perfect crystal is one in which the internal lattice structure extends uninterrupted in all directions. The perfect order can be represented by translational symmetry along three (not usually orthogonal) axes. Every lattice element of the structure is in its proper place, whether it is a single atom or a molecular grouping. For substances that exist in two (or more) stable crystalline forms, such as diamond and graphite for carbon, there is a kind of chemical degeneracy. The question remains whether both can have zero entropy at T = 0 even though each is perfectly ordered.<br>Perfect crystals never occur in practice; imperfections, and even entire amorphous material inclusions, can and do get "frozen in" at low temperatures, so transitions to more stable states do not occur.<br>Using the Debye model, the specific heat and entropy of a pure crystal are proportional to T, while the enthalpy and chemical potential are proportional to T. (Guggenheim, p. 111) These quantities drop toward their T = 0 limiting values and approach with zero slopes. For the specific heats at least, the limiting value itself is definitely zero, as borne out by experiments to below 10 K. Even the less detailed Einstein model shows this curious drop in specific heats. In fact, all specific heats vanish at absolute zero, not just those of crystals. Likewise for the coefficient of thermal expansion. Maxwell's relations show that various other quantities also vanish. These phenomena were unanticipated.<br>Since the relation between changes in Gibbs free energy (G), the enthalpy (H) and the entropy is<br>Δ<br>G<br>=<br>Δ<br>H<br>−<br>T<br>Δ<br>S<br><br><br><br>{\displaystyle \Delta G=\Delta H-T\Delta S\,}<br>thus, as T decreases, ΔG and ΔH approach each other (so long as ΔS is bounded). Experimentally, it is found that all spontaneous processes (including chemical reactions) result in a decrease in G as they proceed toward equilibrium. If ΔS and/or T are small, the condition ΔG < 0 may imply that ΔH < 0, which would indicate an exothermic reaction. However, this is not required; endothermic reactions can proceed spontaneously if the TΔS term is large enough.<br>Moreover, the slopes of the derivatives of ΔG and ΔH converge and are equal to zero at T = 0. This ensures that ΔG and ΔH are nearly the same over a considerable range of temperatures and justifies the approximate empirical Principle of Thomsen and Berthelot, which states that the equilibrium state to which a system proceeds is the one that evolves the greatest amount of heat, i.e., an actual process is the most exothermic one. (Callen, pp. 186–187)<br>One model that estimates the properties of an electron gas at absolute zero in metals is the Fermi gas. The electrons, being fermions, must be in different quantum states, which leads the electrons to get very high typical velocities, even at absolute zero. The maximum energy that electrons can have at absolute zero is called the Fermi energy. The Fermi temperature is defined as this maximum energy divided by the Boltzmann constant, and is on the order of 80,000 K for typical electron densities found in metals. For temperatures significantly below the Fermi temperature, the electrons behave in almost the same way as at absolute zero. This explains the failure of the classical equipartition theorem for metals that eluded classical physicists in the late 19th century.
