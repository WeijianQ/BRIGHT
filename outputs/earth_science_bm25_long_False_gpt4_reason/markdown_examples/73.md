# Query `73`

## Original Question

*I have downloaded the sea surface temperature data from the Ocean Color website, the file is in NetCDF format and contains no of geophysical_data variables like sst, qual_sst, flag_sst, bias_sst, etc. I used the following MATLAB code for file read and it is giving me an error:

temp=ncread('A2014213085500.L2_LAC_SST.x.nc','sst') 
Error:

Error using internal.matlab.imagesci.nc/getGroupAndVarid (line 2075)
Could not find variable or group 'sst' in file.

Error in internal.matlab.imagesci.nc/read (line 593)
        [gid, varid] = getGroupAndVarid(this, location);

Error in ncread (line 58)
vardata = ncObj.read(varName, varargin{:});
Can someone tell me what is the cause of the error?*

## gpt4 Reasoning

```markdown
The error you're encountering in MATLAB when trying to read the sea surface temperature (SST) data from a NetCDF file indicates that MATLAB is unable to find a variable or group named 'sst' in the file. This can occur for several reasons, and troubleshooting this issue involves a step-by-step approach to identify and resolve the problem. Here are some steps and considerations that could help you address this issue:

### 1. Check Variable Names
The first step is to ensure that the variable name 'sst' exactly matches the name of the variable in the NetCDF file. Variable names are case-sensitive, so 'SST' is different from 'sst'.

**Action:** You can use the `ncdisp` function in MATLAB to display the contents of the NetCDF file, including variable names, dimensions, and attributes. This will help you verify the correct variable name. For example:
```matlab
ncdisp('A2014213085500.L2_LAC_SST.x.nc');
```

### 2. Check for Grouped Variables
NetCDF-4 introduced the concept of groups, which can be used to organize related variables. If 'sst' is within a group, you need to specify the group path along with the variable name.

**Action:** If you discover that 'sst' is inside a group, you would access it using a path like `'group_name/sst'`. Use `ncdisp` to check for groups and their contents.

### 3. Verify File Integrity and Format
Ensure that the file is not corrupted and is indeed a NetCDF file. A corrupted or partially downloaded file could cause issues when trying to access its contents.

**Action:** Try opening the file with another tool capable of reading NetCDF files, or re-download the file if you suspect it might be corrupted.

### 4. Ensure Compatibility
Make sure that your version of MATLAB and the NetCDF library it uses are compatible with the format of the NetCDF file. While MATLAB generally supports a wide range of NetCDF formats, incompatibilities can sometimes occur, especially with newer versions of the NetCDF format.

**Action:** Check MATLAB's documentation for supported NetCDF versions and consider updating MATLAB or the NetCDF library if necessary.

### 5. Correct Syntax
Double-check that the syntax of your `ncread` command is correct. While your syntax appears correct, it's always good to review and ensure no small mistakes.

### Summary
The error message you're seeing typically points to a discrepancy between the variable name you're using in your code and the actual variable names within the NetCDF file. Use `ncdisp` to explore the file structure and verify the correct variable names and their paths. Also, consider the other steps to ensure that there are no other underlying issues preventing you from reading the 'sst' variable from the file.
```

### Metrics

```
recip_rank: 0.0061
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) cd4_file/netcdf4_python4_28.txt
> a sequence of netCDF files or a string with a<br>wildcard (converted to a sorted list of files using glob)<br>If<br>the master_file kwarg is not specified, the first file<br>in the list will become the "master" file, defining all the<br>variables with an aggregation dimension which may span<br>subsequent files. Attribute access returns attributes only from "master"<br>file. The files are always opened in read-only mode.<br>check: True if you want to do consistency checking to ensure the<br>correct variables structure for all of the

### DOC[2] (IRRELEVANT) VARIANT_IDs_in_CMIP6/modelers_html3.txt
> Data managers can aslo register errata using the ES-DOC Errata Command Line Client<br>if they wish to do so.<br>Further information about the service is available in the Errata Service Documentation.<br>2. Experiment design<br>The CMIP6 protocol and experiments are described in a special issue<br>of Geoscientific Model Development with an overview of the overall design and scientific<br>strategy provided in the lead article of that issue by Eyring et al. (2016)<br><br><br>Each model participating in CMIP6 must contribute results from the four DECK<br> experiments (piControl, AMIP, abrupt4xCO2, and 1pctCO2) and the CMIP6 historical<br> simulation. See Eyring et al. (2016) where the experiment protocol<br> is documented. These experiments are considered to define the ongoing (slowly<br> evolving) “CMIP Activity” and are directly overseen by the CMIP Panel<br><br><br>In addition to the DECK and historical simulations, each modeling group may choose<br> to contribute to any CMIP6 endorsed MIPs of interest, but<br> for each MIP component, results must be provided from the full subset of “tier 1”<br> experiments. See the GMD Special CMIP6 Issue for descriptions<br> of each MIP and its experiment specifications. Each endorsed MIP<br> is managed by an independent committee. The MIPs are identified as separate<br> “CMIP6 Activities”, but their coordination and their endorsement as part of CMIP6<br> is the responsibility of the CMIP Panel. The process by which MIP<br> activities become endorsed is described here and the criteria<br> for endorsement are listed in Table 1 of Eyring et al. (2016).<br> The official names of the currently endorsed CMIP6 MIPs are recorded in a “json” file<br><br><br>When called for by the experiment protocol, standard forcing data sets<br> should be used. Any deviation from the standard forcing must be clearly documented.<br><br><br>Further documentation about CMIP6 experiments will be available from ES-DOC,<br> and the reference controlled vocabularies used to define and identify these experiments<br> are available in a “json” file and can be displayed in table form<br><br><br>3. Forcing data sets<br>In CMIP6 all models should adopt the same forcing datasets (and boundary conditions).<br>Experts contacted by the CMIP Panel have prepared the forcing datasets, and a new<br>“input4MIPs” activity has been initiated by PCMDI to encourage adherence to many<br>of the same data standards imposed on obs4MIPs data and CMIP data. These datasets<br>are being collected into a curated archive at PCMDI. All conforming datasets can<br>be downloaded via the Earth System Grid Federation’s input4MIPs CoG.<br>Any dataset not yet conforming to the input4MIPs specifications can be obtained<br>from the individual preparing the dataset, as indicated in the input4MIPs summary sheet.<br>The input4MIPs summary sheet separately lists the CMIP6<br>datasets needed for the DECK and historical simulations and the datasets needed<br>for the CMIP6-endorsed MIP experiments. The summary provides contact information,<br>documentation of the data, and citation requirements. Included in the collection<br>are, for example, datasets specifying emissions and concentrations of various atmospheric<br>species, sea surface temperatures and sea ice (for AMIP), solar variability, and<br>land cover characteristics. The current version of the official CMIP Panel forcing<br>dataset collection is 6.2. Users of these datasets should consult the input4MIPs summary sheet<br>before configuring and beginning any new simulation to ensure that they are using<br>the latest versions available.<br>Some of the endorsed-MIP forcing datasets are still in preparation, but should<br>be available soon. Any changes made to a released dataset will be documented in<br>the summary.<br>4. Model output fields<br>The CMIP6 Data Request defines the variables that should<br>be archived for each experiment and specifies the time intervals for which they<br>should be reported. It provides much of the variable-specific metadata that should<br>be stored along with the data. It also provides tools for estimating the data storage<br>requirements for CMIP6.<br>Additional information about the data request is available at https://cmip6dr.github.io/Data_Request_Home<br>5. Model output requirements<br>CMIP6 model output requirements are similar to those in CMIP5, but changes have<br>been made to accommodate the more complex structure of CMIP6 and its data request.<br>Some changes will make it easier for users to find the data they need and will<br>enable new services to be established providing, for example, model and experiment<br>documentation and citation information.<br>As in CMIP5, all CMIP6 output will be stored in netCDF files with one variable<br>stored per file. The requested output fields can be determined as described above,<br>and as in CMIP5, the data must be “cmorized” (i.e., written in conformance with<br>all the CMIP standards). The CMIP standards build on the CF-conventions,<br>which define metadata that provide a description of the variables and their spatial<br>and temporal properties. This facilitates analysis of the data by users who can<br>read and interpret data from all models in the same way.<br>As described in section 6, it is recommended,<br>but not required, that the CMOR software library<br>be used to rewrite model output in conformance with the standards. In any case<br>to ensure that a critical subset of the requirements have been met, a CMIP data<br>checker (“PrePARE”) will be applied<br>before data are placed in the CMIP6 data archive.<br>The CMIP6 data requirements are defined and discussed in the following documents:<br><br>Definition of CMIP6 netCDF global attributes<br>Reference “controlled vocabularies” (CV’s) for CMIP6<br>Specifications for file names, directory structures,<br> and CMIP6 Data Reference Syntax (DRS)<br>Specifications for output file content, structure, and metadata are available in draft google doc.  Use of CMOR3 will ensure compliance.<br>Guidance on grid requirements<br>Information on pressure levels requested<br>Guidance on time-averaging (with masking)<br><br>Additional metadata requirements are imposed on a variable by variable basis as<br>specified in the CMIP6 Data Request. Many of these are recognized<br>by CMOR (through input via the CMIP6 CMOR Tables), which will<br>ensure compliance.<br>Note that in the above, controlled vocabularies (CV’s) play a key role in ensuring<br>uniformity in the description of data sets across all models. For all but variable-specific<br>information, reference CV’s are being maintained by PCMDI against which<br>all quality assurance checks will be performed. These CV’s will be relied on in<br>constructing file names and directory structures, and they will enable faceted<br>searches of the CMIP6 archive as called for in the search requirements document.<br>Additional, variable-specific CVs are part of the CMIP6 Data Request.<br>These CV’s are structured in a way that makes clear relationships between certain<br>items appearing in separate CV’s. For example, the CV for model names (“source_id”)<br>indicates which institutions are authorized to run each model, and the complete

### DOC[3] (IRRELEVANT) cd4_file/NetCDF4_8.txt
> [8] packages), Perl, Python, Ruby, Haskell,[9] Mathematica, MATLAB, IDL, Julia and Octave. The specification of the API calls is very similar across the different languages, apart from inevitable differences of syntax. The API calls for version 2 were rather different from those in version 3, but are also supported by versions 3 and 4 for backward compatibility. Application programmers using supported languages need not normally be concerned with the file structure itself, even though it is available as open formats.<br><br>Applications[edit]<br>A wide range of application software has been written

### DOC[4] (IRRELEVANT) cd4_file/netcdf4_python1_14.txt
> hosted datasets can be accessed for<br>reading over http if a URL is provided to the Dataset constructor instead of a<br>filename.<br>However, this requires that the netCDF library be built with<br>OPenDAP support, via the --enable-dap configure option (added in<br>version 4.0.1).<br>Groups in a netCDF file<br>netCDF version 4 added support for organizing data in hierarchical<br>groups, which are analogous to directories in a filesystem. Groups serve<br>as containers for variables, dimensions and attributes, as well as other

### DOC[5] (IRRELEVANT) VARIANT_IDs_in_CMIP6/modelers_html4.txt
> list of institutions is recorded in a CV for “institution_id”.<br>As indicated in the guidance specifications for output grids,<br>weights should be provided to regrid all output to a few standard grids (e.g.,<br>1x1 degree). All regridding information (weights, lats, lons, etc.) should be stored<br>consistent with a standard format approved by the WIP. Specifications for the required<br>standard format will be forthcoming.<br>CMIP6 output requirements that are critical for successful ingestion and access<br>via ESGF will be enforced when publication of the data is initiated. The success<br>of CMIP6 depends on making sure that even the requirements that can not be checked<br>by ESGF are met. This is the responsibility of anyone preparing model output for<br>CMIP6. A minimum set of requirements for publication of CMIP6 data will be met<br>if a dataset passes the checks performed by the PrePARE software package described<br>in the next section.<br>6. Software for preparing/checking output<br>To facilitate the production of model output files that meet the CMIP6 technical<br>standards, a software library called “CMOR” (Climate Model Output Rewriter) has<br>been developed and version 3 (CMOR3) is now available at this site,<br>but read the installation instructions available here. This package<br>was first used in CMIP3 and has been generalized and improved for each new CMIP<br>phase. Use of CMOR is not mandatory, but past experience suggests that many common<br>errors in model output files can be avoided by its use.<br>For those not using CMOR, some checks for compliance with CMIP specifications can<br>be performed using a new code developed in support of CMIP6: the Pre-Publication<br>Attribute Reviewer for ESGF (PrePARE). For information about tests<br>performed by PrePARE, view the design requirements. PrePARE is included as part<br>of the CMOR software suite and all files produced by CMOR are effectively checked<br>by PrePARE, but PrePARE can be invoked without using CMOR to write the output.<br>In addition to PrePARE, tests for file compliance with the CF-conventions<br>can be made using a tool called the CF-checker. Both PrePARE<br>and the CF-checker will be run as part of the ESGF publication job stream, and<br>only files passing all tests will be published and made available for download.<br>It should be noted if data are written using CMOR, additional checks will be performed<br>that will, for example:<br><br>Guarantee that the metadata associated with each variable is recorded in the<br> file (PrePARE only checks some of the variable attributes)<br>Check for monotonicity of a coordinate values<br>Check for “gaps” in the time coordinates<br>Check that coordinates are stored in the right direction (and for the longitude<br>coordinate check that the range is correct)<br>Check that data values are within limits specified in the cmor tables (but for<br> most variables, this won’t happen since limits are yet to be defined)<br><br>Additional codes useful in preparing model output for CMIP6 include:<br><br>Code to create regridding weights: not yet available<br><br>Code to calculate nominal_resolution: For the common case of a regular spherical coordinate<br>(latitude x longitude) global grid, the nominal_resolution can be calculated using a formula<br>given in Appendix 2 of the CMIP6 netCDF global attributes document.<br>For other grids, the nominal_resolution can be calculated with the following code:<br><br>Code documentation: https://pcmdi.github.io/nominal_resolution/html/index.html.<br>The code can be obtained via a conda package: conda install -c pcmdi nominal_resolution<br>The package repository is hosted on Github at: https://github.com/pcmdi/nominal_resolution<br><br>The library source (api.py) is in the lib directory.<br>The test codes reside in the tests directory.<br><br><br><br><br><br>7. Archiving/publishing output<br>The Earth System Grid Federation (ESGF) will facilitate the global<br>distribution of CMIP6 output.<br>For CMIP6, the original copies of data will be availble through the data nodes, many of which will be installed and maintained by the modeling centers themselves. Certain ESGF data nodes (known as “Tier<br>1 nodes”) will serve as the primary access points to the data. A searchable record of model output: the access method and metadata,<br>will be “published” to these nodes, and additionally, replicas of the data will be hosted on these nodes.<br>As part of “publication”, certain conformance checks are performed,<br>metadata are recorded in a catalog where it can be accessed by the other data nodes,<br>and versioning is managed. The data provider (modeling center) will need to closely<br>coordinate and cooperate with the ESGF data manager(s) of a specific ESGF data<br>node site. Here is a summary of the main steps and requirements in the procedure:<br><br>CMIP6 data compliance checking: Before data are passed to the data node for<br> publication, modeling centers should check that it is in conformance with all<br> the output requirements outlined in the sections above<br>Selection of an ESGF data node: Modeling centers can either set up and host<br> their own ESGF data node or engage with an existing ESGF node. In either case<br> certain rules must be followed as outlined in the “ESGF Data Node Managers and Operators”<br> guide. If the node hosting the data has not been designated “Tier 1”, then one<br> of the Tier 1 nodes will have to be selected to serve as the publication site.<br> Improperly configured data nodes will not be accessible through the federated<br> ESGF system<br>Data transfer and ESGF data management: In addition to putting in place a<br> procedure for smoothly transferring and publishing CMIP6 data, a clearly defined<br> process for handling corrections to flawed data should be established. This would<br> include a formal procedure for recording “errata” information in the case of correction<br> and replacement of erroneous data<br>Data publication: The ESGF data node managers are responsible for ESGF data<br> publication and storage as described more completely in the “ESGF Data Node Managers and Operators”<br> guide. Publication of data not meeting the minimal CMIP6 data quality requirements<br> will be blocked<br>Data replication: Some of the Tier 1 nodes plan to replicate some of the<br> data published by other nodes. This will provide some redundancy across the federation<br> protecting against loss of at least some of the data in the event of a catastrophic<br> storage failure at one node. It will also provide a backup source of data when<br> one node is temporarily offline. Not all data will be replicated, so it is recommended<br> that modeling groups retain a backup copy of their model output<br>Data access: After data publication the CMIP6 data (as well as associated<br> errata information, documentation and citation information) will be visible and<br> accessible via the following designated CMIP6 data portals: PCMDI,<br> DKRZ, IPSL, CEDA, and others<br>Data long-term archival: A “snapshot” of CMIP6 data as it exists at the time<br> of a deadline imposed by the IPCC’s 6th Assessment Report (IPCC-AR6) will be archived<br> at the IPCC Data Distribution Centre (IPCC DDC, http://ipcc-data.org)<br><br>8. Documentation process<br>Given the wide variety of users and the need for traceability, the CMIP6 results<br>will be fully documented and made accessible via the ES-DOC viewer and comparator


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, cd4_file/ncgeodataset_wiki1.txt
> <body<br>    <div class="maia-header" id="maia-header" role="banner"<br>      <div class="maia-aux"<br>        <a href="/archive/"<br>          </a<h1<a href="/archive/"<br>            <img alt="Google" src="//www.google.com/images/branding/googlelogo/1x/googlelogo_color_116x41dp.png" Code</a<br>          </h1<br>        <br>        <a href="/archive/"<br>          <h2Archive</h2<br>        </a<br>        <a class="maia-teleport" href="#content"Skip to content</a<br>      </div<br>    </div<br><br>    <!-- ngView: --<div ng-view="" class="ng-scope"<div ng-controller="ProjectWikiCtrl as wikiCtrl" class="ng-scope"<br><nav-bar-widget nav-level-1="wikiCtrl.navLevel1" class="ng-isolate-scope"<div class="maia-nav maia-complex" id="maia-nav-x" role="navigation"<br>  <div class="maia-aux"<br>    <ul<br>      <li ng-class="{active: navLevel1 == 'projects'}" class="active"<br>        <a href="/archive/"Projects</a<br>        <!-- ngIf: navLevel2 --<br>      </li<br>      <li ng-class="{active: navLevel1 == 'search'}"<br>        <a href="https://www.google.com/search?q=site:codesite-archive.appspot.com"Search</a<br>      </li<br>      <li ng-class="{active: navLevel1 == 'about'}"<br>        <a href="/archive/about"About</a<br>      </li<br>    </ul<br>  </div<br></div<br></nav-bar-widget<br><div id="maia-main" role="main"<br><br>  <!-- ngIf: wikiCtrl.notFound --<br><br>  <div ng-hide="wikiCtrl.notFound"<br>    <!-- Left-side resource nav --<br>    <project-resources-widget domain="wikiCtrl.domain" project="wikiCtrl.projectName" class="ng-isolate-scope"<div class="maia-nav" id="maia-nav-y" role="navigation"<br>  <!-- ngIf: domain == 'code.google.com' --<ul ng-if="domain == 'code.google.com'" class="ng-scope"<br>    <li class="active"<br>      <a href="/archive/p/nctoolbox/"Project</a<br>    </li<br>    <li<br>      <a href="/archive/p/nctoolbox/source"Source</a<br>    </li<br>    <li<br>      <a href="/archive/p/nctoolbox/issues"Issues</a<br>    </li<br>    <li<br>      <a href="/archive/p/nctoolbox/wikis"Wikis</a<br>    </li<br>    <li<br>      <a href="/archive/p/nctoolbox/downloads"Downloads</a<br>    </li<br>  </ul<!-- end ngIf: domain == 'code.google.com' --<br><br>    <!-- ngIf: domain != 'code.google.com' --<br></div</project-resources-widget<br><br>    <div class="maia-article" role="article"<br>      <div class="maia-teleport" id="content"</div<br>      <div class="maia-cols"<br>        <div id="gca-project-header" class="maia-col-10"<br>          <a id="gca-export-to-gh" class="maia-button" href="https://code.google.com/export-to-github/export?project=nctoolbox"Export to GitHub</a<br>          <img class="gca-project-logo" src="https://storage.googleapis.com/google-code-archive/v2/code.google.com/nctoolbox/logo.png"<br>          <h1 class="ng-binding"nctoolbox - ncgeodataset.wiki</h1<br>        </div<br><br>        <!-- ngIf: wikiCtrl.error --<br><br>        <!-- ngIf: !wikiCtrl.error --<div ng-if="!wikiCtrl.error" class="maia-col-10 ng-scope"<br>          <hr<br><br>          <markdown-widget text="wikiCtrl.wikiContents" class="markdown"<h1NCGEODATASET CLASS</h1<br><br><pThe ncgeodataset class is a high level class for dealing with local or remote CF compliant geographic netcdf datasets. The constructor is called with the filesystem path or OPeNDAP link as its argument. This class is the most sophisticated class dealing with data and aims to be the most interoperable by exposing methods to aid data access and comparison in a consistent way.</p<br><br><pNcgeodataset exposes the <codevariable</code, <codeattributes</code, <codevariables</code, <codedata</code, <codegrid</code etc. methods that the cfdataset class implements, but also adds its own methods like <codemetadata</code and <codegeovariable</code.  These methods provide a higher level and more interoperable version of the variable class.</p<br><br><h1Details</h1<br><br><h3ncgeodataset</h3<br><br><psubclass of cfdataset<br>```</p<br><br><blockquote

### GROUND TRUTH 1, ranked not in top 100, cd4_file/ncgeodataset_wiki4.txt
> <pans = </p<br><br><pre<code'lat_rho'<br>'lon_rho'<br>'ocean_time'<br>'s_rho'<br></code</pre<br><br><p<code<br>Grab variable attribute value from specific key:<br></code</p<br><br><blockquote<br>  <blockquote<br>    <pgvar.attributes</p<br>  </blockquote<br></blockquote<br><br><pans = <br>    'long_name'        'salinity' <br<br>    'units'            'PSU' <br<br>    'time'             'ocean_time' <br<br>    'coordinates'      'lat_rho lon_rho' <br<br>    'field'            'salinity, scalar, series'<br>    'missing_value'    [                       0]<br>    'wms-layer'        'chesroms/salt' <br<br>    'standard_name'    'sea_water_salinity'    </p<br><br><blockquote<br>  <blockquote<br>    <plongname = gvar.attribute('long_name')</p<br>  </blockquote<br></blockquote<br><br><plongname =</p<br><br><ppotential temperature</p<br><br><blockquote<br>  <blockquote<br>    <pvar.attribute('long_name')</p<br>  </blockquote<br></blockquote<br><br><pans =</p<br><br><psalinity</p<br><br><p<code<br>% Returns grid, but with standardized field names where appropriate (also `grid_interop` does the vertical coordinate conversions and time to matlab datenum convention conversion:<br></code</p<br><br><blockquote<br>  <blockquote<br>    <pgrid = gvar.grid_interop(1:end, :, 1:50, 1:50)<br>    grid = <br>         lat: [50x50 double]<br>         lon: [50x50 double]<br>        time: [5479x1 double]<br>           z: [20x50x50 double]</p<br>    <br>    <poldgrid = gvar.grid(1:end, :, 1:50, 1:50)<br>    oldgrid = <br>           lat_rho: [50x50 double]<br>           lon_rho: [50x50 double]<br>        ocean_time: [5479x1 double]<br>             s_rho: [20x1 double]</p<br>  </blockquote<br></blockquote<br><br><p```</p<br><br><p% Subset on lat/lon values and time values or indices based on the fields in input structure:<br>```</p<br><br><blockquote<br>  <blockquote<br>    <ps.time={'01-Jan-2002', '01-Feb-2002'}; % Can be omitted or s.t_index=[i1 i2]; or s.time=[datenum1, datenum2];<br>    s.t_stride=2; % Can be omitted<br>    s.lat=[36.1 37.0];<br>    s.lon=[-76.0 -75.0];<br>    s.h_stride=[2 2]; % Can be omitted<br>    s.z_index=[1 3]; % Can be omitted<br>    s.v_stride=2; % Can be omitted</p<br>    <br>    <psub = gvar.geosubset(s); % Subset method<br>    ```</p

### GROUND TRUTH 2, ranked not in top 100, cd4_file/ncgeodataset_wiki3.txt
> attributes: {8x2 cell}<br>  Methods, Events, Superclasses<br><code<br>Grab attribute value from specific variable/key (in that order) or global (with just 1 argument):<br></code</p<br><br><blockquote<br>  <blockquote<br>    <psaltunits=nc.attribute('salt','units')<br>    saltunits =<br>    PSU</p<br>    <br>    <pmodeltitle = nc.attribute('title')</p<br>  </blockquote<br></blockquote<br><br><pmodeltitle =<br>ChesROMS (UMCES) - ROMS-2.2<br>```</p<br><br><pFind the bounding box of a particular variable:<br>```</p<br><br><blockquote<br>  <blockquote<br>    <pnc.extent('salt')<br>    ans = <br>        lon: [-77.1732 -75.0920]<br>        lat: [36.0202 39.9742]<br>    ```</p<br>  </blockquote<br></blockquote<br><br><pIt may be useful to return to identify the time axis for a given geovariable.</p<br><br><pTo find the name of the netcdf variable that provides the time axis for geovariable use <codegettimename</code.<br>```</p<br><br><blockquote<br>  <blockquote<br>    <pname = nc.gettimename('u_water')<br>    ```</p<br>  </blockquote<br></blockquote<br><br><pTo return the geovariable object representing the time axis for a given geovariable use <codegettimevar</code.<br>```</p<br><br><blockquote<br>  <blockquote<br>    <pt_gvar = nc.gettimevar('u_water')<br>    ```</p<br>  </blockquote<br></blockquote<br><br><h3ncgeovariable</h3<br><br><psubclass of ncvariable</p<br><br><p```</p<br><br><blockquote<br>  <blockquote<br>    <pnc = ncgeodataset('<a href="http://testbedapps.sura.org/thredds/dodsC/estuarine_hypoxia/chesroms/agg.nc"http://testbedapps.sura.org/thredds/dodsC/estuarine_hypoxia/chesroms/agg.nc</a');</p<br>    <br>    <pgvar = nc.geovariable('salt')</p<br>  </blockquote<br></blockquote<br><br><pgvar = <br>  ncgeovariable handle</p<br><br><pProperties:<br>       dataset: [1x1 ncgeodataset]<br>          name: 'salt'<br>          axes: {4x1 cell}<br>    attributes: {8x2 cell}<br>  Methods, Events, Superclasses<br>```</p<br><br><pGet coordinate axes (NB this returns all the variable names, without the blanks as opposed to the ncvariable/ncdataset/cfdataset methods and properties):<br>```</p<br><br><blockquote<br>  <blockquote<br>    <pgvar.axes</p<br>  </blockquote<br></blockquote

### GROUND TRUTH 3, ranked not in top 100, cd4_file/ncgeodataset_wiki5.txt
> </blockquote<br></blockquote<br><br><pGet time (returned as datenum) or indices for time within window:<br>```</p<br><br><blockquote<br>  <blockquote<br>    <blockquote<br>      <blockquote<br>        <ptime = gvar.timewindow(731944, 732272) %args in datevec or datenum form.</p<br>      </blockquote<br>    </blockquote<br>  </blockquote<br></blockquote<br><br><ptime =</p<br><br><pre<code  731944<br>  731945<br>  731946<br>  731947<br>  731948<br>  731949<br>  731950<br>  731951 ...<br></code</pre<br><br><p...</p<br><br><blockquote<br>  <blockquote<br>    <ptimewindow = gvar.timewindowij(731944, 732272);<br>    timewindow = </p<br>  </blockquote<br></blockquote<br><br><pre<codeindex: [350x1 double]<br> time: [350x1 double]<br></code</pre<br><br><p<code<br>Or can also be used like:<br></code<br>time = timewindow(gvar, start, stop);<br>time2 = timewindowij(gvar, start, stop);<br>```</p<br><br><pGet lat/lon indices within subset bounds. (If lat/lon are vectors, output is: <code[minlat_ind maxlat_ind minlon_ind maxlon_ind]</code.):<br><code<br>[rowmin rowmax colmin colmax] = geoij(gvar, subsetstruct);<br></code</p</markdown-widget<br>        </div<!-- end ngIf: !wikiCtrl.error --<br>      </div<br>    </div<br>  </div<br></div<br></div</div<br>    <noscript<br>      <pThe Google Code Archive requires JavaScript to be enabled in your browser.</p<br>    </noscript<br><br>    <div id="maia-signature"</div<br>    <div class="maia-footer" id="maia-footer"<br>      <div id="maia-footer-local"<br>      </div<br>      <div id="maia-footer-global"<br>        <div class="maia-aux"<br>          <ul<br>            <li<br>              <a href="//www.google.com/"Google</a<br>            </li<br>            <li<br>              <a href="//www.google.com/intl/en/about/"About Google</a<br>            </li<br>            <li<br>              <a href="//www.google.com/intl/en/policies/privacy/"Privacy</a<br>            </li<br>            <li<br>              <a href="//www.google.com/intl/en/policies/terms/"Terms</a<br>            </li<br>          </ul<br>        </div<br>      </div<br>    </div<br>    <script src="//www.google.com/js/maia.js"</script

### GROUND TRUTH 4, ranked not in top 100, cd4_file/ncgeodataset_wiki2.txt
> <blockquote<br>  <blockquote<br>    <pnc = ncgeodataset('<a href="http://testbedapps.sura.org/thredds/dodsC/estuarine_hypoxia/chesroms/agg.nc"http://testbedapps.sura.org/thredds/dodsC/estuarine_hypoxia/chesroms/agg.nc</a')</p<br>  </blockquote<br></blockquote<br><br><pnc = <br>  ncgeodataset handle</p<br><br><pProperties:<br>     location: '<a href="http://testbedapps.sura.org/thredds/dodsC/estuarine_hypoxia/chesroms/agg.nc"http://testbedapps.sura.org/thredds/dodsC/estuarine_hypoxia/chesroms/agg.nc</a'<br>       netcdf: [1x1 ucar.nc2.dataset.NetcdfDataset]<br>    variables: {92x1 cell}<br>  Methods, Events, Superclasses<br><code<br>Grab the metadata for the dataset:<br></code</p<br><br><blockquote<br>  <blockquote<br>    <pnc.metadata</p<br>  </blockquote<br></blockquote<br><br><pans = </p<br><br><blockquote<br>  <blockquote<br>    <pnc.metadata<br>    ans = <br>        global_attributes: {32x2 cell}<br>                   ntimes: {'long_name'  'number of long time-steps'}<br>                  ndtfast: {'long_name'  'number of short time-steps'}<br>                       dt: {2x2 cell}<br>            ...<br>                        u: {7x2 cell}<br>                        v: {7x2 cell}<br>                        w: {5x2 cell}<br>                     temp: {8x2 cell}<br>                     salt: {8x2 cell}<br>                      rho: {5x2 cell}<br>    ...</p<br>  </blockquote<br></blockquote<br><br><p<code<br>Grab just the global attributes of the dataset:<br></code</p<br><br><blockquote<br>  <blockquote<br>    <pnc.attributes<br>    ans = <br>        'type'                'ROMS/TOMS history file' <br<br>        'Conventions'         'CF-1.0' <br<br>        'title'               'ChesROMS (UMCES) - ROMS-2.2' <br<br>        'var_info'                                       [1x54  char]<br>        'rst_file'            'chesroms_rst_Nz20.nc' <br<br>        'his_base'            'chesroms_his_Nz20' <br<br>        'avg_base'            'chesroms_avg_Nz20' <br<br>        ...<br>        'creator_name'        'Wen Long' <br<br>        'creator_email'       'wenlong@umces.edu' <br<br>        'creator_url'         '<a href="http://www.umces.edu/"http://www.umces.edu/</a'                </p<br>  </blockquote<br></blockquote<br><br><p<code<br>Create variable object:<br></code<br>var = nc.variable('salt');<br><code<br>Create instance of ncgeovariable object:<br></code</p<br><br><blockquote<br>  <blockquote<br>    <pgvar = nc.geovariable('salt')</p<br>  </blockquote<br></blockquote<br><br><pgvar = <br>  ncgeovariable handle</p<br><br><pProperties:<br>       dataset: [1x1 ncgeodataset]<br>          name: 'salt'<br>          axes: {4x1 cell}<br>    attributes: {8x2 cell}
