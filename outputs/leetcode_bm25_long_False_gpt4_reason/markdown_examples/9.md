# Query `9`

## Original Question

*Given an array of integers `citations` where `citations[i]` is the number of citations a researcher received for their `ith` paper, return _the researcher's h-index_.

According to the [definition of h-index on Wikipedia](https://en.wikipedia.org/wiki/H-index): The h-index is defined as the maximum value of `h` such that the given researcher has published at least `h` papers that have each been cited at least `h` times.

Example 1:

Input: citations = \[3,0,6,1,5\]
Output: 3
Explanation: \[3,0,6,1,5\] means the researcher has 5 papers in total and each of them had received 3, 0, 6, 1, 5 citations respectively.
Since the researcher has 3 papers with at least 3 citations each and the remaining two with no more than 3 citations each, their h-index is 3.

Example 2:

Input: citations = \[1,3,1\]
Output: 1

Constraints:

   `n == citations.length`
   `1 <= n <= 5000`
   `0 <= citations[i] <= 1000`*

## gpt4 Reasoning

```markdown
The essential problem here is to calculate the h-index of a researcher based on their paper citations. The h-index is a metric that aims to measure both the productivity and citation impact of the publications of a scientist or scholar. It is defined as the maximum value of `h` such that the given researcher has published at least `h` papers that have each been cited at least `h` times.

### Step-by-Step Approach

1. **Understand the Definition**: The first step is to fully grasp the definition of the h-index. An h-index of `x` means the researcher has `x` papers that have been cited at least `x` times. Note that if a researcher has more than `x` papers with at least `x` citations, the h-index is still `x` (not higher), because the h-index cannot be higher than the number of papers.

2. **Sort the Citations**: A logical approach to solve this problem programmatically is to first sort the array of citations. This way, we can easily find out how many papers have at least a certain number of citations. Sorting in descending order can be particularly helpful, as we can then iterate through the citations from the most to the least cited paper.

3. **Iterate and Find the H-Index**: After sorting, we iterate through the sorted citations. During the iteration, for each position `i` in the sorted array, we check if the citation count of the paper at position `i` is greater than or equal to `i + 1` (since array indexing is zero-based and we are looking for the count of papers, we use `i + 1`). The iteration stops when we find a paper whose citation count is less than `i + 1`, or we reach the end of the array. The highest `i + 1` value that satisfies the condition is the h-index.

4. **Edge Cases**: It's important to handle edge cases, such as when all papers have citations greater than the total number of papers (in which case the h-index is the total number of papers) or when no papers meet the criteria for h-index greater than 0.

### Example Walkthrough

Let's consider the first example provided: citations = [3,0,6,1,5].

1. **Sort the Citations**: After sorting in descending order, we have [6,5,3,1,0].
   
2. **Iterate and Find the H-Index**:
   - For the 1st paper, citation count is 6, which is greater than 1. Continue.
   - For the 2nd paper, citation count is 5, which is greater than 2. Continue.
   - For the 3rd paper, citation count is 3, which is greater than 3. Continue.
   - For the 4th paper, citation count is 1, which is not greater than 4. Stop.

   We found that the researcher has 3 papers with at least 3 citations each. Thus, the h-index is 3.

### Algorithm in Pseudocode

```plaintext
1. Sort the citations array in descending order.
2. Initialize an index i to 0.
3. While i < length of the sorted citations array:
    a. If the citation count at index i is greater than or equal to i+1, then increment i.
    b. Else, break the loop.
4. The h-index is i.
```

This approach efficiently computes the h-index by leveraging sorting and a single pass through the citations array, addressing both the productivity and impact of a researcher's publications.
```

### Metrics

```
recip_rank: 1.0000
P_1: 1.0000
P_5: 0.2000
P_10: 0.1000
P_25: 0.0400
P_50: 0.0200
P_100: 0.0100
recall_1: 1.0000
recall_5: 1.0000
recall_10: 1.0000
recall_25: 1.0000
recall_50: 1.0000
recall_100: 1.0000
ndcg_cut_1: 1.0000
ndcg_cut_5: 1.0000
ndcg_cut_10: 1.0000
ndcg_cut_25: 1.0000
ndcg_cut_50: 1.0000
ndcg_cut_100: 1.0000
map_cut_1: 1.0000
map_cut_5: 1.0000
map_cut_10: 1.0000
map_cut_25: 1.0000
map_cut_50: 1.0000
map_cut_100: 1.0000
```

## Retrieval Results

### DOC[1] (GROUND TRUTH) leetcode/leetcode_275.txt
> def hIndex(citations: List[int]) - int:<br>    """Given an array of integers `citations` where `citations[i]` is the number of citations a researcher received for their `ith` paper and `citations` is sorted in ascending order, return _the researcher's h-index_.<br><br>According to the [definition of h-index on Wikipedia](https://en.wikipedia.org/wiki/H-index): The h-index is defined as the maximum value of `h` such that the given researcher has published at least `h` papers that have each been cited at least `h` times.<br><br>You must write an algorithm that runs in logarithmic time.<br><br>Example 1:<br><br>Input: citations = \[0,1,3,5,6\]<br>Output: 3<br>Explanation: \[0,1,3,5,6\] means the researcher has 5 papers in total and each of them had received 0, 1, 3, 5, 6 citations respectively.<br>Since the researcher has 3 papers with at least 3 citations each and the remaining two with no more than 3 citations each, their h-index is 3.<br><br>Example 2:<br><br>Input: citations = \[1,2,100\]<br>Output: 2<br><br>Constraints:<br><br>   `n == citations.length`<br>   `1 <= n <= 105`<br>   `0 <= citations[i] <= 1000`<br>   `citations` is sorted in ascending order."""<br><br>    n = len(citations)<br>    left, right = 0, n - 1<br><br>    while left <= right:<br>        mid = left + (right - left) // 2<br>        if citations[mid] == n - mid:<br>            return n - mid<br>        elif citations[mid] < n - mid:<br>            left = mid + 1<br>        else:<br>            right = mid - 1<br><br>    return n - left

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_32273.txt
> def rpys(self, minYear = None, maxYear = None, dropYears = None, rankEmptyYears = False):<br>        """This implements _Referenced Publication Years Spectroscopy_ a techinique for finding import years in citation data. The authors of the original papers have a website with more information, found [here](http://www.leydesdorff.net/software/rpys/).<br><br>        This function computes the spectra of the `RecordCollection` and returns a dictionary mapping strings to lists of `ints`. Each list is ordered and the values of each with the same index form a row and each list a column. The strings are the names of the columns. This is intended to be read directly by pandas `DataFrames`.<br><br>        The columns returned are:<br><br>        1. `'year'`, the years of the counted citations, missing years are inserted with a count of 0, unless they are outside the bounds of the highest year or the lowest year and the default value is used. e.g. if the highest year is 2016, 2017 will not be inserted unless _maxYear_ has been set to 2017 or higher<br>        2. `'count'`, the number of times the year was cited<br>        3. `'abs-deviation'`, deviation from the 5-year median. Calculated by taking the absolute deviation of the count from the median of it and the next 2 years and the preceding 2 years<br>        4. `'rank'`, the rank of the year, the highest ranked year being the one with the highest deviation, the second highest being the second highest deviation and so on. All years with 0 count are given the rank 0 by default<br><br>        # Parameters<br><br>        _minYear_ : `optional int`<br><br>         Default `1000`, The lowest year to be returned, note years outside this bound will be used to calculate the deviation from the 5-year median<br><br>        _maxYear_ : `optional int`<br><br>         Default `2100`, The highest year to be returned, note years outside this bound will be used to calculate the deviation from the 5-year median<br><br>        _dropYears_ : `optional int or list[int]`<br><br>         Default `None`, year or collection of years that will be removed from the returned value, note the dropped years will still be used to calculate the deviation from the 5-year<br><br>        _rankEmptyYears_ : `optional [bool]`<br><br>         Default `False`, if `True` years with 0 count will be ranked according to their deviance, if many 0 count years exist their ordering is not guaranteed to be stable<br><br>        # Returns<br><br>        `dict[str:list]`<br><br>         The table of values from the _Referenced Publication Years Spectroscopy_<br>        """<br><br>        def deviation(targetYear, targetValue, targetDict):<br>            yearCounts = [targetValue]<br>            for deltaY in [-2, -1, 1, 2]:<br>                try:<br>                    yearCounts.append(targetDict[targetYear + deltaY])<br>                except KeyError:<br>                    yearCounts.append(0)<br>            medianCount = list(sorted(yearCounts))[2]<br>            absDiff = targetValue - medianCount<br>            return absDiff<br><br>        if dropYears is None:<br>            dropYears = set()<br>        yearCounts = {}<br>        retDict = {'year' : [], 'count' : [], 'abs-deviation' : [], 'rank' : []}<br><br>        for R in self:<br>            try:<br>                cites = R['citations']<br>            except KeyError:<br>                continue<br>            recYear = R.get('year', float('inf'))<br>            for cite in cites:<br>                try:<br>                    #year can be None<br>                    cYear = int(cite.year)<br>                except (AttributeError, TypeError):<br>                    continue<br>                else:<br>                    #need the extra years for the normlization<br>                    if (maxYear is not None and cYear  (maxYear + 2)) or (minYear is not None and cYear < (minYear - 2)):<br>                        continue<br>                    #years from before the paper are an error<br>                    elif recYear < (cYear + 2):<br>                        continue<br>                if cYear in yearCounts:<br>                    yearCounts[cYear] += 1<br>                else:<br>                    yearCounts[cYear] = 1<br><br>        if minYear is None:<br>            smallest = min(yearCounts.keys())<br>            if smallest  1000:<br>                minYear = smallest<br>        if maxYear is None:<br>            biggest = max(yearCounts.keys())<br>            if biggest < 2100:<br>                maxYear = biggest<br><br>        targetYears = set(( i for i in range(minYear, maxYear + 1) if i not in dropYears))<br><br>        ranks = {}<br>        yearDeviances = {}<br><br>        for y in targetYears:<br>            try:<br>                c = yearCounts[y]<br>            except KeyError:<br>                c = 0<br>            yearDeviances[y] = deviation(y, c, yearCounts)<br><br>        for rank, year in enumerate(sorted(yearDeviances.items(), key = lambda x: x[1], reverse = False), start = 1):<br>            ranks[year[0]] = rank<br><br>        for y in targetYears:<br>            try:<br>                c = yearCounts[y]<br>            except KeyError:<br>                c = 0<br>            if c == 0 and not rankEmptyYears:<br>                retDict['rank'].append(0)<br>            else:<br>                retDict['rank'].append(ranks[y])<br>            retDict['abs-deviation'].append(yearDeviances[y])<br>            retDict['year'].append(y)<br>            retDict['count'].append(c)<br><br>        return retDict

### DOC[3] (IRRELEVANT) leetcode/leetcode_975.txt
> def rangeSumBST(self, root: TreeNode, low: int, high: int) - int:<br>    """You are given an integer array `arr`. From some starting index, you can make a series of jumps. The (1st, 3rd, 5th, ...) jumps in the series are called odd-numbered jumps, and the (2nd, 4th, 6th, ...) jumps in the series are called even-numbered jumps. Note that the jumps are numbered, not the indices.<br><br>You may jump forward from index `i` to index `j` (with `i < j`) in the following way:<br><br>   During odd-numbered jumps (i.e., jumps 1, 3, 5, ...), you jump to the index `j` such that `arr[i] <= arr[j]` and `arr[j]` is the smallest possible value. If there are multiple such indices `j`, you can only jump to the smallest such index `j`.<br>   During even-numbered jumps (i.e., jumps 2, 4, 6, ...), you jump to the index `j` such that `arr[i] = arr[j]` and `arr[j]` is the largest possible value. If there are multiple such indices `j`, you can only jump to the smallest such index `j`.<br>   It may be the case that for some index `i`, there are no legal jumps.<br><br>A starting index is good if, starting from that index, you can reach the end of the array (index `arr.length - 1`) by jumping some number of times (possibly 0 or more than once).<br><br>Return _the number of good starting indices_.<br><br>Example 1:<br><br>Input: arr = \[10,13,12,14,15\]<br>Output: 2<br>Explanation: <br>From starting index i = 0, we can make our 1st jump to i = 2 (since arr\[2\] is the smallest among arr\[1\], arr\[2\], arr\[3\], arr\[4\] that is greater or equal to arr\[0\]), then we cannot jump any more.<br>From starting index i = 1 and i = 2, we can make our 1st jump to i = 3, then we cannot jump any more.<br>From starting index i = 3, we can make our 1st jump to i = 4, so we have reached the end.<br>From starting index i = 4, we have reached the end already.<br>In total, there are 2 different starting indices i = 3 and i = 4, where we can reach the end with some number of<br>jumps.<br><br>Example 2:<br><br>Input: arr = \[2,3,1,1,4\]<br>Output: 3<br>Explanation: <br>From starting index i = 0, we make jumps to i = 1, i = 2, i = 3:<br>During our 1st jump (odd-numbered), we first jump to i = 1 because arr\[1\] is the smallest value in \[arr\[1\], arr\[2\], arr\[3\], arr\[4\]\] that is greater than or equal to arr\[0\].<br>During our 2nd jump (even-numbered), we jump from i = 1 to i = 2 because arr\[2\] is the largest value in \[arr\[2\], arr\[3\], arr\[4\]\] that is less than or equal to arr\[1\]. arr\[3\] is also the largest value, but 2 is a smaller index, so we can only jump to i = 2 and not i = 3<br>During our 3rd jump (odd-numbered), we jump from i = 2 to i = 3 because arr\[3\] is the smallest value in \[arr\[3\], arr\[4\]\] that is greater than or equal to arr\[2\].<br>We can't jump from i = 3 to i = 4, so the starting index i = 0 is not good.<br>In a similar manner, we can deduce that:<br>From starting index i = 1, we jump to i = 4, so we reach the end.<br>From starting index i = 2, we jump to i = 3, and then we can't jump anymore.<br>From starting index i = 3, we jump to i = 4, so we reach the end.<br>From starting index i = 4, we are already at the end.<br>In total, there are 3 different starting indices i = 1, i = 3, and i = 4, where we can reach the end with some<br>number of jumps.<br><br>Example 3:<br><br>Input: arr = \[5,1,3,4,2\]<br>Output: 3<br>Explanation: We can reach the end from starting indices 1, 2, and 4.<br><br>Constraints:<br><br>   `1 <= arr.length <= 2  104`<br>   `0 <= arr[i] < 105`"""<br><br>    if not root:<br>        return 0<br>    if root.val < low:<br>        return self.rangeSumBST(root.right, low, high)<br>    if root.val  high:<br>        return self.rangeSumBST(root.left, low, high)<br>    return root.val + self.rangeSumBST(root.left, low, high) + self.rangeSumBST(root.right, low, high)

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_144450.txt
> def ner_net(source, destinations, width, height, wrap_around=False, radius=10):<br>    """Produce a shortest path tree for a given net using NER.<br><br>    This is the kernel of the NER algorithm.<br><br>    Parameters<br>    ----------<br>    source : (x, y)<br>        The coordinate of the source vertex.<br>    destinations : iterable([(x, y), ...])<br>        The coordinates of destination vertices.<br>    width : int<br>        Width of the system (nodes)<br>    height : int<br>        Height of the system (nodes)<br>    wrap_around : bool<br>        True if wrap-around links should be used, false if they should be<br>        avoided.<br>    radius : int<br>        Radius of area to search from each node. 20 is arbitrarily selected in<br>        the paper and shown to be acceptable in practice.<br><br>    Returns<br>    -------<br>    (:py:class:`~.rig.place_and_route.routing_tree.RoutingTree`,<br>     {(x,y): :py:class:`~.rig.place_and_route.routing_tree.RoutingTree`, ...})<br>        A RoutingTree is produced rooted at the source and visiting all<br>        destinations but which does not contain any vertices etc. For<br>        convenience, a dictionarry mapping from destination (x, y) coordinates<br>        to the associated RoutingTree is provided to allow the caller to insert<br>        these items.<br>    """<br>    # Map from (x, y) to RoutingTree objects<br>    route = {source: RoutingTree(source)}<br><br>    # Handle each destination, sorted by distance from the source, closest<br>    # first.<br>    for destination in sorted(destinations,<br>                              key=(lambda destination:<br>                                   shortest_mesh_path_length(<br>                                       to_xyz(source), to_xyz(destination))<br>                                   if not wrap_around else<br>                                   shortest_torus_path_length(<br>                                       to_xyz(source), to_xyz(destination),<br>                                       width, height))):<br>        # We shall attempt to find our nearest neighbouring placed node.<br>        neighbour = None<br><br>        # Try to find a nearby (within radius hops) node in the routing tree<br>        # that we can route to (falling back on just routing to the source).<br>        #<br>        # In an implementation according to the algorithm's original<br>        # specification looks for nodes at each point in a growing set of rings<br>        # of concentric hexagons. If it doesn't find any destinations this<br>        # means an awful lot of checks: 1261 for the default radius of 20.<br>        #<br>        # An alternative (but behaviourally identical) implementation scans the<br>        # list of all route nodes created so far and finds the closest node<br>        # which is < radius hops (falling back on the origin if no node is<br>        # closer than radius hops).  This implementation requires one check per<br>        # existing route node. In most routes this is probably a lot less than<br>        # 1261 since most routes will probably have at most a few hundred route<br>        # nodes by the time the last destination is being routed.<br>        #<br>        # Which implementation is best is a difficult question to answer:<br>        #  In principle nets with quite localised connections (e.g.<br>        #   nearest-neighbour or centroids traffic) may route slightly more<br>        #   quickly with the original algorithm since it may very quickly find<br>        #   a neighbour.<br>        #  In nets which connect very spaced-out destinations the second<br>        #   implementation may be quicker since in such a scenario it is<br>        #   unlikely that a neighbour will be found.<br>        #  In extremely high-fan-out nets (e.g. broadcasts), the original<br>        #   method is very likely to perform far better than the alternative<br>        #   method since most iterations will complete immediately while the<br>        #   alternative method must scan all the route vertices.<br>        # As such, it should be clear that neither method alone is 'best' and<br>        # both have degenerate performance in certain completely reasonable<br>        # styles of net. As a result, a simple heuristic is used to decide<br>        # which technique to use.<br>        #<br>        # The following micro-benchmarks are crude estimate of the<br>        # runtime-per-iteration of each approach (at least in the case of a<br>        # torus topology)::<br>        #<br>        #     $ # Original approach<br>        #     $ python -m timeit --setup 'x, y, w, h, r = 1, 2, 5, 10, \<br>        #                                     {x:None for x in range(10)}' \<br>        #                        'x += 1; y += 1; x %= w; y %= h; (x, y) in r'<br>        #     1000000 loops, best of 3: 0.207 usec per loop<br>        #     $ # Alternative approach<br>        #     $ python -m timeit --setup 'from rig.geometry import \<br>        #                                 shortest_torus_path_length' \<br>        #                        'shortest_torus_path_length( \<br>        #                             (0, 1, 2), (3, 2, 1), 10, 10)'<br>        #     1000000 loops, best of 3: 0.666 usec per loop<br>        #<br>        # From this we can approximately suggest that the alternative approach<br>        # is 3x more expensive per iteration. A very crude heuristic is to use<br>        # the original approach when the number of route nodes is more than<br>        # 1/3rd of the number of routes checked by the original method.<br>        concentric_hexagons = memoized_concentric_hexagons(radius)<br>        if len(concentric_hexagons) < len(route) / 3:<br>            # Original approach: Start looking for route nodes in a concentric<br>            # spiral pattern out from the destination node.<br>            for x, y in concentric_hexagons:<br>                x += destination[0]<br>                y += destination[1]<br>                if wrap_around:<br>                    x %= width<br>                    y %= height<br>                if (x, y) in route:<br>                    neighbour = (x, y)<br>                    break<br>        else:<br>            # Alternative approach: Scan over every route node and check to see<br>            # if any are < radius, picking the closest one if so.<br>            neighbour = None<br>            neighbour_distance = None<br>            for candidate_neighbour in route:<br>                if wrap_around:<br>                    distance = shortest_torus_path_length(<br>                        to_xyz(candidate_neighbour), to_xyz(destination),<br>                        width, height)<br>                else:<br>                    distance = shortest_mesh_path_length(<br>                        to_xyz(candidate_neighbour), to_xyz(destination))<br><br>                if distance <= radius and (neighbour is None or<br>                                           distance < neighbour_distance):<br>                    neighbour = candidate_neighbour<br>                    neighbour_distance = distance<br><br>        # Fall back on routing directly to the source if no nodes within radius<br>        # hops of the destination was found.<br>        if neighbour is None:<br>            neighbour = source<br><br>        # Find the shortest vector from the neighbour to this destination<br>        if wrap_around:<br>            vector = shortest_torus_path(to_xyz(neighbour),<br>                                         to_xyz(destination),<br>                                         width, height)<br>        else:<br>            vector = shortest_mesh_path(to_xyz(neighbour), to_xyz(destination))<br><br>        # The longest-dimension-first route may inadvertently pass through an<br>        # already connected node. If the route is allowed to pass through that<br>        # node it would create a cycle in the route which would be VeryBad(TM).<br>        # As a result, we work backward through the route and truncate it at<br>        # the first point where the route intersects with a connected node.<br>        ldf = longest_dimension_first(vector, neighbour, width, height)<br>        i = len(ldf)<br>        for direction, (x, y) in reversed(ldf):<br>            i -= 1<br>            if (x, y) in route:<br>                # We've just bumped into a node which is already part of the<br>                # route, this becomes our new neighbour and we truncate the LDF<br>                # route. (Note ldf list is truncated just after the current<br>                # position since it gives (direction, destination) pairs).<br>                neighbour = (x, y)<br>                ldf = ldf[i + 1:]<br>                break<br><br>        # Take the longest dimension first route.<br>        last_node = route[neighbour]<br>        for direction, (x, y) in ldf:<br>            this_node = RoutingTree((x, y))<br>            route[(x, y)] = this_node<br><br>            last_node.children.append((Routes(direction), this_node))<br>            last_node = this_node<br><br>    return (route[source], route)

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_273440.txt
> def get_cited_dois(file):<br>    """<br>    Get the DOIs of the papers cited in a plaintext file. The file should \<br>            have one citation per line.<br><br>    .. note::<br><br>        This function is also used as a backend tool by most of the others \<br>        citations processors, to factorize the code.<br><br>    :param file: Either the path to the plaintext file or the content of a \<br>            plaintext file. It can also be a parsed list of plaintext \<br>            citations and, in this case, no preprocessing is done.<br>    :returns: A dict of cleaned plaintext citations and their associated DOI.<br>    """<br>    # If file is not a pre-processed list of plaintext citations<br>    if not isinstance(file, list):<br>        # It is either a path to a plaintext file or the content of a plaintext<br>        # file, we need some pre-processing to get a list of citations.<br>        plaintext_citations = get_plaintext_citations(file)<br>    else:<br>        # Else, we passed a list of plaintext citations.<br>        plaintext_citations = file<br>    dois = {}<br>    crossref_queue = []<br><br>    # Try to get the DOI directly from the citation<br>    for citation in plaintext_citations[:]:<br>        # Some citations already contain a DOI so try to match it directly<br>        matched_dois = doi.extract_from_text(citation)<br>        if len(matched_dois)  0:<br>            # Add the DOI and go on<br>            dois[citation] = next(iter(matched_dois))<br>            continue<br>        # Same thing for arXiv id<br>        matched_arxiv = arxiv.extract_from_text(citation)<br>        if len(matched_arxiv)  0:<br>            # Add the associated DOI and go on<br>            dois[citation] = arxiv.to_doi(next(iter(matched_arxiv)))<br>            continue<br>        # If no match found, stack it for next step<br>        # Note to remove URLs in the citation as the plaintext citations can<br>        # contain URLs and they are bad for the CrossRef API.<br>        crossref_queue.append(tools.remove_urls(citation))<br><br>    # Do batch with remaining papers, to prevent from the timeout of CrossRef<br>    for batch in tools.batch(crossref_queue, CROSSREF_MAX_BATCH_SIZE):<br>        batch = [i for i in batch]<br>        try:<br>            # Fetch results from CrossRef<br>            request = requests.post(CROSSREF_LINKS_API_URL, json=batch)<br>            for result in request.json()["results"]:<br>                # Try to get a DOI<br>                try:<br>                    dois[result["text"]] = result["doi"]<br>                except KeyError:<br>                    # Or set it to None<br>                    dois[result["text"]] = None<br>        except (RequestException, ValueError, KeyError):<br>            # If an exception occurred, set all the DOIs to None for the<br>            # current batch<br>            for i in batch:<br>                dois[i] = None<br>    return dois


## Ground Truth

### GROUND TRUTH 0, ranked 0, leetcode/leetcode_275.txt
> def hIndex(citations: List[int]) - int:<br>    """Given an array of integers `citations` where `citations[i]` is the number of citations a researcher received for their `ith` paper and `citations` is sorted in ascending order, return _the researcher's h-index_.<br><br>According to the [definition of h-index on Wikipedia](https://en.wikipedia.org/wiki/H-index): The h-index is defined as the maximum value of `h` such that the given researcher has published at least `h` papers that have each been cited at least `h` times.<br><br>You must write an algorithm that runs in logarithmic time.<br><br>Example 1:<br><br>Input: citations = \[0,1,3,5,6\]<br>Output: 3<br>Explanation: \[0,1,3,5,6\] means the researcher has 5 papers in total and each of them had received 0, 1, 3, 5, 6 citations respectively.<br>Since the researcher has 3 papers with at least 3 citations each and the remaining two with no more than 3 citations each, their h-index is 3.<br><br>Example 2:<br><br>Input: citations = \[1,2,100\]<br>Output: 2<br><br>Constraints:<br><br>   `n == citations.length`<br>   `1 <= n <= 105`<br>   `0 <= citations[i] <= 1000`<br>   `citations` is sorted in ascending order."""<br><br>    n = len(citations)<br>    left, right = 0, n - 1<br><br>    while left <= right:<br>        mid = left + (right - left) // 2<br>        if citations[mid] == n - mid:<br>            return n - mid<br>        elif citations[mid] < n - mid:<br>            left = mid + 1<br>        else:<br>            right = mid - 1<br><br>    return n - left
