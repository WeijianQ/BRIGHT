# Query `105`

## Original Question

*You are given an integer array `prices` representing the daily price history of a stock, where `prices[i]` is the stock price on the `ith` day.

A smooth descent period of a stock consists of one or more contiguous days such that the price on each day is lower than the price on the preceding day by exactly `1`. The first day of the period is exempted from this rule.

Return _the number of smooth descent periods_.

Example 1:

Input: prices = \[3,2,1,4\]
Output: 7
Explanation: There are 7 smooth descent periods:
\[3\], \[2\], \[1\], \[4\], \[3,2\], \[2,1\], and \[3,2,1\]
Note that a period with one day is a smooth descent period by the definition.

Example 2:

Input: prices = \[8,6,7,7\]
Output: 4
Explanation: There are 4 smooth descent periods: \[8\], \[6\], \[7\], and \[7\]
Note that \[8,6\] is not a smooth descent period as 8 - 6 ≠ 1.

Example 3:

Input: prices = \[1\]
Output: 1
Explanation: There is 1 smooth descent period: \[1\]

Constraints:

   `1 <= prices.length <= 105`
   `1 <= prices[i] <= 105`*

## gpt4 Reasoning

```markdown
The essential problem here is to calculate the total number of smooth descent periods in a given array of stock prices. A smooth descent period is defined as a sequence of days where each day's stock price is exactly 1 less than the previous day's price, except for the first day in the sequence which does not have this restriction.

To solve this problem, we can approach it step by step:

1. **Initialize a Counter**: We need a counter to keep track of the total number of smooth descent periods. This counter will be incremented as we find valid periods.

2. **Iterate Through Prices**: We will loop through the `prices` array to examine each price and compare it with the next price in the array. This step is crucial because we need to identify when a smooth descent starts and ends.

3. **Identify Smooth Descent Periods**: As we iterate, we need to check if the current price is exactly 1 more than the next price. If so, we have found the start of a smooth descent period. We then continue to check subsequent prices to determine the length of this period.

4. **Count Single-Day Periods**: According to the problem statement, a single day counts as a smooth descent period by definition. This means we start with a count equal to the length of the `prices` array, as each day is a valid period by itself.

5. **Calculate Additional Periods**: For each smooth descent period longer than one day, we need to calculate how many additional combinations it contributes to the total count. For example, a 3-day smooth descent contributes not just one, but three additional periods ([first, second], [second, third], and [first, second, third]) beyond the individual days already counted.

6. **Use a Running Length Variable**: To efficiently calculate the number of additional periods a smooth descent contributes, we can use a variable to keep track of the length of the current smooth descent as we iterate through the array. When the descent ends, we calculate the additional combinations it contributes based on its length, add this to our total count, and reset the length variable for the next potential descent.

7. **Calculate Additional Combinations**: For a smooth descent of length `n`, the number of additional combinations it contributes is the sum of integers from 1 to `n-1`. This can be calculated using the formula `n*(n-1)/2` (which derives from the sum of the first `n-1` positive integers).

8. **Handle Edge Cases**: We must be careful to handle the end of the array correctly. If the last prices are part of a smooth descent, we need to ensure we count the additional combinations they contribute before we finish iterating.

By following these steps, we can calculate the total number of smooth descent periods in the given `prices` array. This approach is efficient and takes into account both the individual day periods and the additional combinations contributed by longer smooth descent periods.
```

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_313710.txt
> def load_rf(freq="M"):<br>    """Build a risk-free rate return series using 3-month US T-bill yields.<br><br>    The 3-Month Treasury Bill: Secondary Market Rate from the Federal Reserve<br>    (a yield) is convert to a total return.  See 'Methodology' for details.<br><br>    The time series should closely mimic returns of the BofA Merrill Lynch US<br>    Treasury Bill (3M) (Local Total Return) index.<br><br>    Parameters<br>    ----------<br>    freq : str, sequence, or set<br>        If a single-character string, return a single-column DataFrame with<br>        index frequency corresponding to `freq`.  If a sequence or set, return<br>        a dict of DataFrames with the keys corresponding to `freq`(s)<br><br>    Methodology<br>    -----------<br>    The Federal Reserve publishes a daily chart of Selected Interest Rates<br>    (release H.15; www.federalreserve.gov/releases/h15/).  As with a yield<br>    curve, some yields are interpolated from recent issues because Treasury<br>    auctions do not occur daily.<br><br>    While the de-annualized ex-ante yield itself is a fairly good tracker of<br>    the day's total return, it is not perfect and can exhibit non-neglible<br>    error in periods of volatile short rates.  The purpose of this function<br>    is to convert yields to total returns for 3-month T-bills.  It is a<br>    straightforward process given that these are discount (zero-coupon)<br>    securities.  It consists of buying a 3-month bond at the beginning of each<br>    month, then amortizing that bond throughout the month to back into the<br>    price of a <3-month tenor bond.<br><br>    The source data (pulled from fred.stlouisfed.org) is quoted on a discount<br>    basis.  (See footnote 4 from release H.15.)  This is converted to a<br>    bond-equivlanet yield (BEY) and then translated to a hypothetical daily<br>    total return.<br><br>    The process largely follows Morningstar's published Return Calculation of<br>    U.S. Treasury Constant Maturity Indices, and is as follows:<br>    - At the beginning of each month a bill is purchased at the prior month-end<br>      price, and daily returns in the month reflect the change in daily<br>      valuation of this bill<br>    - If t is not a business day, its yield is the yield of the prior<br>      business day.<br>    - At each day during the month, the price of a 3-month bill purchased on<br>      the final calendar day of the previous month is computed.<br>    - Month-end pricing is unique.  At each month-end date, there are<br>      effectively two bonds and two prices.  The first is the bond<br>      hypothetically purchased on the final day of the prior month with 2m<br>      remaining to maturity, and the second is a new-issue bond purchased that<br>      day with 3m to maturity.  The former is used as the numerator to compute<br>      that day's total return, while the latter is used as the denominator<br>      to compute the next day's (1st day of next month) total return.<br><br>    Description of the BofA Merrill Lynch US 3-Month Treasury Bill Index:<br>    The BofA Merrill Lynch US 3-Month Treasury Bill Index is comprised of a<br>    single issue purchased at the beginning of the month and held for a full<br>    month. At the end of the month that issue is sold and rolled into a newly<br>    selected issue. The     issue selected at each month-end rebalancing is the<br>    outstanding Treasury Bill that matures closest to, but not beyond, three<br>    months from the rebalancing date. To qualify for selection, an issue must<br>    have settled on or before the month-end rebalancing date.<br>        (Source: Bank of America Merrill Lynch)<br><br>    See also<br>    --------<br>    FRED: 3-Month Treasury Bill: Secondary Market Rate (DTB3)<br>      https://fred.stlouisfed.org/series/DTB3<br>    McGraw-Hill/Irwin, Interest Rates, 2008.<br>      https://people.ucsc.edu/~lbaum/econ80h/LS-Chap009.pdf<br>    Morningstar, Return Calculation of U.S. Treasury Constant Maturity Indices,<br>      September 2008.<br>    """<br><br>    freqs = "DWMQA"<br>    freq = freq.upper()<br>    if freq not in freqs:<br>        raise ValueError(<br>            "`freq` must be either a single element or subset"<br>            " from %s, case-insensitive" % freqs<br>        )<br><br>    # Load daily 3-Month Treasury Bill: Secondary Market Rate.<br>    # Note that this is on discount basis and will be converted to BEY.<br>    # Periodicity is daily.<br>    rates = (<br>        pdr.DataReader("DTB3", "fred", DSTART)<br>        .mul(0.01)<br>        .asfreq("D", method="ffill")<br>        .fillna(method="ffill")<br>        .squeeze()<br>    )<br><br>    # Algebra doesn't 'work' on DateOffsets, don't simplify here!<br>    minus_one_month = offsets.MonthEnd(-1)<br>    plus_three_months = offsets.MonthEnd(3)<br>    trigger = rates.index.is_month_end<br>    dtm_old = rates.index + minus_one_month + plus_three_months - rates.index<br>    dtm_new = (<br>        rates.index.where(trigger, rates.index + minus_one_month)<br>        + plus_three_months<br>        - rates.index<br>    )<br><br>    # This does 2 things in one step:<br>    # (1) convert discount yield to BEY<br>    # (2) get the price at that BEY and days to maturity<br>    # The two equations are simplified<br>    # See https://people.ucsc.edu/~lbaum/econ80h/LS-Chap009.pdf<br>    p_old = (100 / 360)  (360 - rates  dtm_old.days)<br>    p_new = (100 / 360)  (360 - rates  dtm_new.days)<br><br>    res = p_old.pct_change().where(trigger, p_new.pct_change()).dropna()<br>    # TODO: For purpose of using in TSeries, we should drop upsampled<br>    #       periods where we don't have the full period constituents.<br>    return res.add(1.0).resample(freq).prod().sub(1.0)

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_245562.txt
> def getBestDiscount(sender,kwargs):<br>    '''<br>    When a customer registers for events, discounts may need to be<br>    automatically applied.  A given shopping cart may, in fact,<br>    be eligible for multiple different types of discounts (e.g. hours-based<br>    discounts for increasing numbers of class hours), but typically, only one<br>    discount should be applied.  Therefore, this handler loops through all potential<br>    discounts, finds the ones that are applicable to the passed registration or set<br>    of items, and returns the code and discounted price of the best available discount,<br>    in a tuple of the form (code, discounted_price).<br>    '''<br>    if not getConstant('general__discountsEnabled'):<br>        return<br><br>    logger.debug('Signal fired to request discounts.')<br><br>    reg = kwargs.pop('registration',None)<br>    if not reg:<br>        logger.warning('No registration passed, discounts not applied.')<br>        return<br><br>    payAtDoor = reg.payAtDoor<br><br>    # Check if this is a new customer, who may be eligible for special discounts<br>    newCustomer = True<br>    customer = Customer.objects.filter(email=reg.email,first_name=reg.firstName,last_name=reg.lastName).first()<br>    if (customer and customer.numClassSeries  0) or sender != RegistrationSummaryView:<br>        newCustomer = False<br><br>    eligible_filter = (<br>        Q(event__series__pricingTier__isnull=False) |<br>        Q(event__publicevent__pricingTier__isnull=False)<br>    )<br>    ineligible_filter = (<br>        (Q(event__series__isnull=False) & Q(event__series__pricingTier__isnull=True)) |<br>        (Q(event__publicevent__isnull=False) & Q(event__publicevent__pricingTier__isnull=True)) |<br>        Q(dropIn=True)<br>    )<br>    if apps.is_installed('danceschool.private_lessons'):<br>        eligible_filter = eligible_filter | Q(event__privatelessonevent__pricingTier__isnull=False)<br>        ineligible_filter = ineligible_filter | (<br>            Q(event__privatelessonevent__isnull=False) &<br>            Q(event__privatelessonevent__pricingTier__isnull=True)<br>        )<br><br>    # The items for which the customer registered.<br>    eventregs_list = reg.temporaryeventregistration_set.all()<br>    eligible_list = eventregs_list.filter(dropIn=False).filter(eligible_filter)<br>    ineligible_list = eventregs_list.filter(ineligible_filter)<br><br>    ineligible_total = sum(<br>        [x.event.getBasePrice(payAtDoor=payAtDoor) for x in ineligible_list.exclude(dropIn=True)] +<br>        [x.price for x in ineligible_list.filter(dropIn=True)]<br>    )<br><br>    # Get the applicable discounts and sort them in ascending category order<br>    # so that the best discounts are always listed in the order that they will<br>    # be applied.<br>    discountCodesApplicable = getApplicableDiscountCombos(eligible_list, newCustomer, reg.student, customer=customer, addOn=False, cannotCombine=False, dateTime=reg.dateTime)<br>    discountCodesApplicable.sort(key=lambda x: x.code.category.order)<br><br>    # Once we have a list of codes to try, calculate the discounted price for each possibility,<br>    # and pick the one in each category that has the lowest total price.  We also need to keep track<br>    # of the way in which some discounts are allocated across individual events.<br>    best_discounts = OrderedDict()<br><br>    initial_prices = [x.event.getBasePrice(payAtDoor=payAtDoor) for x in eligible_list]<br>    initial_total = sum(initial_prices)<br><br>    if discountCodesApplicable:<br>        net_allocated_prices = initial_prices<br>        net_precategory_price = initial_total<br>        last_category = discountCodesApplicable[0].code.category<br><br>    for discount in discountCodesApplicable:<br><br>        # If the category has changed, then the new net_allocated_prices and the<br>        # new net_precategory price are whatever was found to be best in the last category.<br>        if (discount.code.category != last_category):<br>            last_category = discount.code.category<br><br>            if best_discounts:<br>                # Since this is an OrderedDict, we can get the last element of the dict from<br>                # the iterator, which is the last category for which there was a valid discount.<br>                last_discount = best_discounts.get(next(reversed(best_discounts)))<br>                net_allocated_prices = last_discount.net_allocated_prices<br>                net_precategory_price = last_discount.net_price<br><br>        # The second item in each tuple is now adjusted, so that each item that is wholly or partially<br>        # applied against the discount will be wholly (value goes to 0) or partially subtracted from the<br>        # remaining value to be calculated at full price.<br>        tieredTuples = [(x,1) for x in eligible_list[:]]<br><br>        for itemTuple in discount.itemTuples:<br>            tieredTuples = [(p,q) if p != itemTuple[0] else (p,q - itemTuple[1]) for (p,q) in tieredTuples]<br><br>        response = discount.code.applyAndAllocate(net_allocated_prices,tieredTuples,payAtDoor)<br><br>        # Once the final price has been calculated, apply it iff it is less than<br>        # the previously best discount found.<br>        current_code = best_discounts.get(discount.code.category.name, None)<br>        if (<br>            response and (<br>                (not current_code and response.net_price < net_precategory_price) or<br>                (current_code and response.net_price < current_code.net_price)<br>            )<br>        ):<br>            best_discounts[discount.code.category.name] = response<br><br>    # Now, repeat the basic process for codes that cannot be combined.  These codes are always<br>    # compared against the base price, and there is no need to allocate across items since<br>    # only one code will potentially be applied.<br>    uncombinedCodesApplicable = getApplicableDiscountCombos(<br>        eligible_list, newCustomer, reg.student, customer=customer, addOn=False, cannotCombine=True, dateTime=reg.dateTime<br>    )<br><br>    for discount in uncombinedCodesApplicable:<br><br>        # The second item in each tuple is now adjusted, so that each item that is wholly or partially<br>        # applied against the discount will be wholly (value goes to 0) or partially subtracted from the<br>        # remaining value to be calculated at full price.<br>        tieredTuples = [(x,1) for x in eligible_list[:]]<br><br>        for itemTuple in discount.itemTuples:<br>            tieredTuples = [(p,q) if p != itemTuple[0] else (p,q - itemTuple[1]) for (p,q) in tieredTuples]<br><br>        response = discount.code.applyAndAllocate(initial_prices,tieredTuples,payAtDoor)<br><br>        # Once the final price has been calculated, apply it iff it is less than<br>        # the previously best discount or combination of discounts found.<br>        if (<br>            response and<br>            response.net_price < min([x.net_price for x in best_discounts.values()] + [initial_total])<br>        ):<br>            best_discounts = OrderedDict({discount.code.category.name: response})<br><br>    if not best_discounts:<br>        logger.debug('No applicable discounts found.')<br><br>    # Return the list of discounts to be applied (in DiscountInfo tuples), along with the additional<br>    # price of ineligible items to be added.<br>    return DiscountCombo.DiscountApplication([x for x in best_discounts.values()], ineligible_total)

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_158671.txt
> def get_clean_factor_and_forward_returns(factor,<br>                                         prices,<br>                                         groupby=None,<br>                                         binning_by_group=False,<br>                                         quantiles=5,<br>                                         bins=None,<br>                                         periods=(1, 5, 10),<br>                                         filter_zscore=20,<br>                                         groupby_labels=None,<br>                                         max_loss=0.35,<br>                                         zero_aware=False,<br>                                         cumulative_returns=True):<br>    """<br>    Formats the factor data, pricing data, and group mappings into a DataFrame<br>    that contains aligned MultiIndex indices of timestamp and asset. The<br>    returned data will be formatted to be suitable for Alphalens functions.<br><br>    It is safe to skip a call to this function and still make use of Alphalens<br>    functionalities as long as the factor data conforms to the format returned<br>    from get_clean_factor_and_forward_returns and documented here<br><br>    Parameters<br>    ----------<br>    factor : pd.Series - MultiIndex<br>        A MultiIndex Series indexed by timestamp (level 0) and asset<br>        (level 1), containing the values for a single alpha factor.<br>        ::<br>            -----------------------------------<br>                date    |    asset   |<br>            -----------------------------------<br>                        |   AAPL     |   0.5<br>                        -----------------------<br>                        |   BA       |  -1.1<br>                        -----------------------<br>            2014-01-01  |   CMG      |   1.7<br>                        -----------------------<br>                        |   DAL      |  -0.1<br>                        -----------------------<br>                        |   LULU     |   2.7<br>                        -----------------------<br><br>    prices : pd.DataFrame<br>        A wide form Pandas DataFrame indexed by timestamp with assets<br>        in the columns.<br>        Pricing data must span the factor analysis time period plus an<br>        additional buffer window that is greater than the maximum number<br>        of expected periods in the forward returns calculations.<br>        It is important to pass the correct pricing data in depending on<br>        what time of period your signal was generated so to avoid lookahead<br>        bias, or  delayed calculations.<br>        'Prices' must contain at least an entry for each timestamp/asset<br>        combination in 'factor'. This entry should reflect the buy price<br>        for the assets and usually it is the next available price after the<br>        factor is computed but it can also be a later price if the factor is<br>        meant to be traded later (e.g. if the factor is computed at market<br>        open but traded 1 hour after market open the price information should<br>        be 1 hour after market open).<br>        'Prices' must also contain entries for timestamps following each<br>        timestamp/asset combination in 'factor', as many more timestamps<br>        as the maximum value in 'periods'. The asset price after 'period'<br>        timestamps will be considered the sell price for that asset when<br>        computing 'period' forward returns.<br>        ::<br>            ----------------------------------------------------<br>                        | AAPL |  BA  |  CMG  |  DAL  |  LULU  |<br>            ----------------------------------------------------<br>               Date     |      |      |       |       |        |<br>            ----------------------------------------------------<br>            2014-01-01  |605.12| 24.58|  11.72| 54.43 |  37.14 |<br>            ----------------------------------------------------<br>            2014-01-02  |604.35| 22.23|  12.21| 52.78 |  33.63 |<br>            ----------------------------------------------------<br>            2014-01-03  |607.94| 21.68|  14.36| 53.94 |  29.37 |<br>            ----------------------------------------------------<br><br>    groupby : pd.Series - MultiIndex or dict<br>        Either A MultiIndex Series indexed by date and asset,<br>        containing the period wise group codes for each asset, or<br>        a dict of asset to group mappings. If a dict is passed,<br>        it is assumed that group mappings are unchanged for the<br>        entire time period of the passed factor data.<br>    binning_by_group : bool<br>        If True, compute quantile buckets separately for each group.<br>        This is useful when the factor values range vary considerably<br>        across gorups so that it is wise to make the binning group relative.<br>        You should probably enable this if the factor is intended<br>        to be analyzed for a group neutral portfolio<br>    quantiles : int or sequence[float]<br>        Number of equal-sized quantile buckets to use in factor bucketing.<br>        Alternately sequence of quantiles, allowing non-equal-sized buckets<br>        e.g. [0, .10, .5, .90, 1.] or [.05, .5, .95]<br>        Only one of 'quantiles' or 'bins' can be not-None<br>    bins : int or sequence[float]<br>        Number of equal-width (valuewise) bins to use in factor bucketing.<br>        Alternately sequence of bin edges allowing for non-uniform bin width<br>        e.g. [-4, -2, -0.5, 0, 10]<br>        Chooses the buckets to be evenly spaced according to the values<br>        themselves. Useful when the factor contains discrete values.<br>        Only one of 'quantiles' or 'bins' can be not-None<br>    periods : sequence[int]<br>        periods to compute forward returns on.<br>    filter_zscore : int or float, optional<br>        Sets forward returns greater than X standard deviations<br>        from the the mean to nan. Set it to 'None' to avoid filtering.<br>        Caution: this outlier filtering incorporates lookahead bias.<br>    groupby_labels : dict<br>        A dictionary keyed by group code with values corresponding<br>        to the display name for each group.<br>    max_loss : float, optional<br>        Maximum percentage (0.00 to 1.00) of factor data dropping allowed,<br>        computed comparing the number of items in the input factor index and<br>        the number of items in the output DataFrame index.<br>        Factor data can be partially dropped due to being flawed itself<br>        (e.g. NaNs), not having provided enough price data to compute<br>        forward returns for all factor values, or because it is not possible<br>        to perform binning.<br>        Set max_loss=0 to avoid Exceptions suppression.<br>    zero_aware : bool, optional<br>        If True, compute quantile buckets separately for positive and negative<br>        signal values. This is useful if your signal is centered and zero is<br>        the separation between long and short signals, respectively.<br>    cumulative_returns : bool, optional<br>        If True, forward returns columns will contain cumulative returns.<br>        Setting this to False is useful if you want to analyze how predictive<br>        a factor is for a single forward day.<br><br>    Returns<br>    -------<br>    merged_data : pd.DataFrame - MultiIndex<br>        A MultiIndex Series indexed by date (level 0) and asset (level 1),<br>        containing the values for a single alpha factor, forward returns for<br>        each period, the factor quantile/bin that factor value belongs to, and<br>        (optionally) the group the asset belongs to.<br>        - forward returns column names follow  the format accepted by<br>          pd.Timedelta (e.g. '1D', '30m', '3h15m', '1D1h', etc)<br>        - 'date' index freq property (merged_data.index.levels[0].freq) will be<br>          set to a trading calendar (pandas DateOffset) inferred from the input<br>          data (see infer_trading_calendar for more details). This is currently<br>          used only in cumulative returns computation<br>        ::<br>           -------------------------------------------------------------------<br>                      |       | 1D  | 5D  | 10D  |factor|group|factor_quantile<br>           -------------------------------------------------------------------<br>               date   | asset |     |     |      |      |     |<br>           -------------------------------------------------------------------<br>                      | AAPL  | 0.09|-0.01|-0.079|  0.5 |  G1 |      3<br>                      --------------------------------------------------------<br>                      | BA    | 0.02| 0.06| 0.020| -1.1 |  G2 |      5<br>                      --------------------------------------------------------<br>           2014-01-01 | CMG   | 0.03| 0.09| 0.036|  1.7 |  G2 |      1<br>                      --------------------------------------------------------<br>                      | DAL   |-0.02|-0.06|-0.029| -0.1 |  G3 |      5<br>                      --------------------------------------------------------<br>                      | LULU  |-0.03| 0.05|-0.009|  2.7 |  G1 |      2<br>                      --------------------------------------------------------<br>    """<br><br>    forward_returns = compute_forward_returns(factor, prices, periods,<br>                                              filter_zscore,<br>                                              cumulative_returns)<br><br>    factor_data = get_clean_factor(factor, forward_returns, groupby=groupby,<br>                                   groupby_labels=groupby_labels,<br>                                   quantiles=quantiles, bins=bins,<br>                                   binning_by_group=binning_by_group,<br>                                   max_loss=max_loss, zero_aware=zero_aware)<br><br>    return factor_data

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_326876.txt
> def hurst_rs(data, nvals=None, fit="RANSAC", debug_plot=False,<br>             debug_data=False, plot_file=None, corrected=True, unbiased=True):<br>  """<br>  Calculates the Hurst exponent by a standard rescaled range (R/S) approach.<br><br>  Explanation of Hurst exponent:<br>    The Hurst exponent is a measure for the "long-term memory" of a<br>    time series, meaning the long statistical dependencies in the data that do<br>    not originate from cycles.<br><br>    It originates from H.E. Hursts observations of the problem of long-term<br>    storage in water reservoirs. If x_i is the discharge of a river in year i<br>    and we observe this discharge for N years, we can calculate the storage<br>    capacity that would be required to keep the discharge steady at its mean<br>    value.<br><br>    To do so, we first substract the mean over all x_i from the individual<br>    x_i to obtain the departures x'_i from the mean for each year i. As the<br>    excess or deficit in discharge always carrys over from year i to year i+1,<br>    we need to examine the cumulative sum of x'_i, denoted by y_i. This<br>    cumulative sum represents the filling of our hypothetical storage. If the<br>    sum is above 0, we are storing excess discharge from the river, if it is<br>    below zero we have compensated a deficit in discharge by releasing<br>    water from the storage. The range (maximum - minimum) R of y_i therefore<br>    represents the total capacity required for the storage.<br><br>    Hurst showed that this value follows a steady trend for varying N if it<br>    is normalized by the standard deviation sigma over the x_i. Namely he<br>    obtained the following formula:<br><br>    R/sigma = (N/2)^K<br><br>    In this equation, K is called the Hurst exponent. Its value is 0.5 for<br>    white noise, but becomes greater for time series that exhibit some positive<br>    dependency on previous values. For negative dependencies it becomes less<br>    than 0.5.<br><br>  Explanation of the algorithm:<br>    The rescaled range (R/S) approach is directly derived from Hurst's<br>    definition. The time series of length N is split into non-overlapping<br>    subseries of length n. Then, R and S (S = sigma) are calculated for each<br>    subseries and the mean is taken over all subseries yielding (R/S)_n. This<br>    process is repeated for several lengths n. Finally, the exponent K is<br>    obtained by fitting a straight line to the plot of log((R/S)_n) vs log(n).<br><br>    There seems to be no consensus how to chose the subseries lenghts n.<br>    This function therefore leaves the choice to the user. The module provides<br>    some utility functions for "typical" values:<br><br>       binary_n: N/2, N/4, N/8, ...<br>       logarithmic_n: min_n, min_n  f, min_n  f^2, ...<br><br>  References:<br>    .. [h_1] H. E. Hurst, “The problem of long-term storage in reservoirs,”<br>       International Association of Scientific Hydrology. Bulletin, vol. 1,<br>       no. 3, pp. 13–27, 1956.<br>    .. [h_2] H. E. Hurst, “A suggested statistical model of some time series<br>       which occur in nature,” Nature, vol. 180, p. 494, 1957.<br>    .. [h_3] R. Weron, “Estimating long-range dependence: finite sample<br>       properties and confidence intervals,” Physica A: Statistical Mechanics<br>       and its Applications, vol. 312, no. 1, pp. 285–299, 2002.<br><br>  Reference Code:<br>    .. [h_a] "hurst" function in R-package "pracma",<br>             url: https://cran.r-project.org/web/packages/pracma/pracma.pdf<br><br>             Note: Pracma yields several estimates of the Hurst exponent, which<br>             are listed below. Unless otherwise stated they use the divisors<br>             of the length of the sequence as n. The length is reduced by at<br>             most 1% to find the value that has the most divisors.<br><br>              The "Simple R/S" estimate is just log((R/S)_n) / log(n) for <br>               n = N.<br>              The "theoretical Hurst exponent" is the value that would be<br>               expected of an uncorrected rescaled range approach for random<br>               noise of the size of the input data.<br>              The "empirical Hurst exponent" is the uncorrected Hurst exponent<br>               obtained by the rescaled range approach.<br>              The "corrected empirical Hurst exponent" is the Anis-Lloyd-Peters<br>               corrected Hurst exponent, but with sqrt(1/2  pi  n) added to<br>               the (R/S)_n before the log.<br>              The "corrected R over S Hurst exponent" uses the R-function "lm"<br>               instead of pracmas own "polyfit" and uses n = N/2, N/4, N/8, ...<br>               by successively halving the subsequences (which means that some<br>               subsequences may be one element longer than others). In contrast<br>               to its name it does not use the Anis-Lloyd-Peters correction<br>               factor.<br><br>             If you want to compare the output of pracma to the output of<br>             nolds, the "empirical hurst exponent" is the only measure that<br>             exactly corresponds to the Hurst measure implemented in nolds<br>             (by choosing corrected=False, fit="poly" and employing the same<br>             strategy for choosing n as the divisors of the (reduced)<br>             sequence length).<br>    .. [h_b] Rafael Weron, "HURST: MATLAB function to compute the Hurst<br>             exponent using R/S Analysis",<br>             url: https://ideas.repec.org/c/wuu/hscode/m11003.html<br><br>             Note: When the same values for nvals are used and fit is set to<br>             "poly", nolds yields exactly the same results as this<br>             implementation.<br>    .. [h_c] Bill Davidson, "Hurst exponent",<br>             url: http://www.mathworks.com/matlabcentral/fileexchange/9842-hurst-exponent<br>    .. [h_d] Tomaso Aste, "Generalized Hurst exponent",<br>             url: http://de.mathworks.com/matlabcentral/fileexchange/30076-generalized-hurst-exponent<br><br>  Args:<br>    data (array-like of float):<br>      time series<br>  Kwargs:<br>    nvals (iterable of int):<br>      sizes of subseries to use<br>      (default: logmid_n(total_N, ratio=1/4.0, nsteps=15) , that is 15<br>      logarithmically spaced values in the medium 25% of the logarithmic range)<br><br>      Generally, the choice for n is a trade-off between the length and the<br>      number of the subsequences that are used for the calculation of the<br>      (R/S)_n. Very low values of n lead to high variance in the ``r`` and ``s``<br>      while very high values may leave too few subsequences that the mean along<br>      them is still meaningful. Logarithmic spacing makes sense, because it <br>      translates to even spacing in the log-log-plot.<br>    fit (str):<br>      the fitting method to use for the line fit, either 'poly' for normal<br>      least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which<br>      is more robust to outliers<br>    debug_plot (boolean):<br>      if True, a simple plot of the final line-fitting step will be shown<br>    debug_data (boolean):<br>      if True, debugging data will be returned alongside the result<br>    plot_file (str):<br>      if debug_plot is True and plot_file is not None, the plot will be saved<br>      under the given file name instead of directly showing it through<br>      ``plt.show()``<br>    corrected (boolean):<br>      if True, the Anis-Lloyd-Peters correction factor will be applied to the<br>      output according to the expected value for the individual (R/S)_n<br>      (see [h_3]_)<br>    unbiased (boolean):<br>      if True, the standard deviation based on the unbiased variance<br>      (1/(N-1) instead of 1/N) will be used. This should be the default choice,<br>      since the true mean of the sequences is not known. This parameter should<br>      only be changed to recreate results of other implementations.<br><br>  Returns:<br>    float:<br>      estimated Hurst exponent K using a rescaled range approach (if K = 0.5<br>      there are no long-range correlations in the data, if K < 0.5 there are<br>      negative long-range correlations, if K  0.5 there are positive<br>      long-range correlations)<br>    (1d-vector, 1d-vector, list):<br>      only present if debug_data is True: debug data of the form<br>      ``(nvals, rsvals, poly)`` where ``nvals`` are the values used for log(n), <br>      ``rsvals`` are the corresponding log((R/S)_n) and ``poly`` are the line <br>      coefficients (``[slope, intercept]``)<br>  """<br>  data = np.asarray(data)<br>  total_N = len(data)<br>  if nvals is None:<br>    # chooses a default value for nvals that will give 15 logarithmically<br>    # spaced datapoints leaning towards the middle of the logarithmic range<br>    # (since both too small and too large n introduce too much variance)<br>    nvals = logmid_n(total_N, ratio=1/4.0, nsteps=15)<br>  # get individual values for (R/S)_n<br>  rsvals = np.array([rs(data, n, unbiased=unbiased) for n in nvals])<br>  # filter NaNs (zeros should not be possible, because if R is 0 then<br>  # S is also zero)<br>  not_nan = np.logical_not(np.isnan(rsvals))<br>  rsvals = rsvals[not_nan]<br>  nvals = np.asarray(nvals)[not_nan]<br>  # it may happen that no rsvals are left (if all values of data are the same)<br>  if len(rsvals) == 0:<br>    poly = [np.nan, np.nan]<br>    if debug_plot:<br>      warnings.warn("Cannot display debug plot, all (R/S)_n are NaN")<br>  else:<br>    # fit a line to the logarithm of the obtained (R/S)_n<br>    xvals = np.log(nvals)<br>    yvals = np.log(rsvals)<br>    if corrected:<br>      yvals -= np.log([expected_rs(n) for n in nvals])<br>    poly = poly_fit(xvals, yvals, 1, fit=fit)<br>    if debug_plot:<br>      plot_reg(xvals, yvals, poly, "log(n)", "log((R/S)_n)",<br>               fname=plot_file)<br>  # account for correction if necessary<br>  h = poly[0] + 0.5 if corrected else poly[0]<br>  # return line slope (+ correction) as hurst exponent<br>  if debug_data:<br>    return (h, (np.log(nvals), np.log(rsvals), poly))<br>  else:<br>    return h

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_158667.txt
> def compute_forward_returns(factor,<br>                            prices,<br>                            periods=(1, 5, 10),<br>                            filter_zscore=None,<br>                            cumulative_returns=True):<br>    """<br>    Finds the N period forward returns (as percent change) for each asset<br>    provided.<br><br>    Parameters<br>    ----------<br>    factor : pd.Series - MultiIndex<br>        A MultiIndex Series indexed by timestamp (level 0) and asset<br>        (level 1), containing the values for a single alpha factor.<br><br>        - See full explanation in utils.get_clean_factor_and_forward_returns<br><br>    prices : pd.DataFrame<br>        Pricing data to use in forward price calculation.<br>        Assets as columns, dates as index. Pricing data must<br>        span the factor analysis time period plus an additional buffer window<br>        that is greater than the maximum number of expected periods<br>        in the forward returns calculations.<br>    periods : sequence[int]<br>        periods to compute forward returns on.<br>    filter_zscore : int or float, optional<br>        Sets forward returns greater than X standard deviations<br>        from the the mean to nan. Set it to 'None' to avoid filtering.<br>        Caution: this outlier filtering incorporates lookahead bias.<br>    cumulative_returns : bool, optional<br>        If True, forward returns columns will contain cumulative returns.<br>        Setting this to False is useful if you want to analyze how predictive<br>        a factor is for a single forward day.<br><br>    Returns<br>    -------<br>    forward_returns : pd.DataFrame - MultiIndex<br>        A MultiIndex DataFrame indexed by timestamp (level 0) and asset<br>        (level 1), containing the forward returns for assets.<br>        Forward returns column names follow the format accepted by<br>        pd.Timedelta (e.g. '1D', '30m', '3h15m', '1D1h', etc).<br>        'date' index freq property (forward_returns.index.levels[0].freq)<br>        will be set to a trading calendar (pandas DateOffset) inferred<br>        from the input data (see infer_trading_calendar for more details).<br>    """<br><br>    factor_dateindex = factor.index.levels[0]<br>    if factor_dateindex.tz != prices.index.tz:<br>        raise NonMatchingTimezoneError("The timezone of 'factor' is not the "<br>                                       "same as the timezone of 'prices'. See "<br>                                       "the pandas methods tz_localize and "<br>                                       "tz_convert.")<br><br>    freq = infer_trading_calendar(factor_dateindex, prices.index)<br><br>    factor_dateindex = factor_dateindex.intersection(prices.index)<br><br>    if len(factor_dateindex) == 0:<br>        raise ValueError("Factor and prices indices don't match: make sure "<br>                         "they have the same convention in terms of datetimes "<br>                         "and symbol-names")<br><br>    # chop prices down to only the assets we care about (= unique assets in<br>    # `factor`).  we could modify `prices` in place, but that might confuse<br>    # the caller.<br>    prices = prices.filter(items=factor.index.levels[1])<br><br>    raw_values_dict = {}<br>    column_list = []<br><br>    for period in sorted(periods):<br>        if cumulative_returns:<br>            returns = prices.pct_change(period)<br>        else:<br>            returns = prices.pct_change()<br><br>        forward_returns = \<br>            returns.shift(-period).reindex(factor_dateindex)<br><br>        if filter_zscore is not None:<br>            mask = abs(<br>                forward_returns - forward_returns.mean()<br>            )  (filter_zscore  forward_returns.std())<br>            forward_returns[mask] = np.nan<br><br>        #<br>        # Find the period length, which will be the column name. We'll test<br>        # several entries in order to find out the most likely period length<br>        # (in case the user passed inconsinstent data)<br>        #<br>        days_diffs = []<br>        for i in range(30):<br>            if i = len(forward_returns.index):<br>                break<br>            p_idx = prices.index.get_loc(forward_returns.index[i])<br>            if p_idx is None or p_idx < 0 or (<br>                    p_idx + period) = len(prices.index):<br>                continue<br>            start = prices.index[p_idx]<br>            end = prices.index[p_idx + period]<br>            period_len = diff_custom_calendar_timedeltas(start, end, freq)<br>            days_diffs.append(period_len.components.days)<br><br>        delta_days = period_len.components.days - mode(days_diffs).mode[0]<br>        period_len -= pd.Timedelta(days=delta_days)<br>        label = timedelta_to_string(period_len)<br><br>        column_list.append(label)<br><br>        raw_values_dict[label] = np.concatenate(forward_returns.values)<br><br>    df = pd.DataFrame.from_dict(raw_values_dict)<br>    df.set_index(<br>        pd.MultiIndex.from_product(<br>            [factor_dateindex, prices.columns],<br>            names=['date', 'asset']<br>        ),<br>        inplace=True<br>    )<br>    df = df.reindex(factor.index)<br><br>    # now set the columns correctly<br>    df = df[column_list]<br><br>    df.index.levels[0].freq = freq<br>    df.index.levels[0].name = "date"<br>    df.index.levels[1].name = "asset"<br><br>    return df


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_713.txt
> def numSubarrayProductLessThanK(nums, k):<br>    """Given an array of integers `nums` and an integer `k`, return _the number of contiguous subarrays where the product of all the elements in the subarray is strictly less than_ `k`.<br><br>Example 1:<br><br>Input: nums = \[10,5,2,6\], k = 100<br>Output: 8<br>Explanation: The 8 subarrays that have product less than 100 are:<br>\[10\], \[5\], \[2\], \[6\], \[10, 5\], \[5, 2\], \[2, 6\], \[5, 2, 6\]<br>Note that \[10, 5, 2\] is not included as the product of 100 is not strictly less than k.<br><br>Example 2:<br><br>Input: nums = \[1,2,3\], k = 0<br>Output: 0<br><br>Constraints:<br><br>   `1 <= nums.length <= 3  104`<br>   `1 <= nums[i] <= 1000`<br>   `0 <= k <= 106`"""<br><br>    if k <= 1:<br>        return 0<br>    prod = 1<br>    count = 0<br>    left = 0<br>    for right in range(len(nums)):<br>        prod = nums[right]<br>        while prod = k:<br>            prod /= nums[left]<br>            left += 1<br>        count += right - left + 1<br>    return count

### GROUND TRUTH 1, ranked not in top 100, leetcode/leetcode_2348.txt
> def numberOfSubarraysWithZeroes(nums):<br>    """Given an integer array `nums`, return _the number of subarrays filled with_ `0`.<br><br>A subarray is a contiguous non-empty sequence of elements within an array.<br><br>Example 1:<br><br>Input: nums = \[1,3,0,0,2,0,0,4\]<br>Output: 6<br>Explanation: <br>There are 4 occurrences of \[0\] as a subarray.<br>There are 2 occurrences of \[0,0\] as a subarray.<br>There is no occurrence of a subarray with a size more than 2 filled with 0. Therefore, we return 6.<br><br>Example 2:<br><br>Input: nums = \[0,0,0,2,0,0\]<br>Output: 9<br>Explanation:<br>There are 5 occurrences of \[0\] as a subarray.<br>There are 3 occurrences of \[0,0\] as a subarray.<br>There is 1 occurrence of \[0,0,0\] as a subarray.<br>There is no occurrence of a subarray with a size more than 3 filled with 0. Therefore, we return 9.<br><br>Example 3:<br><br>Input: nums = \[2,10,2019\]<br>Output: 0<br>Explanation: There is no subarray filled with 0. Therefore, we return 0.<br><br>Constraints:<br><br>   `1 <= nums.length <= 105`<br>   `-109 <= nums[i] <= 109`"""<br><br>    count = 0<br>    zero_count = 0<br><br>    for num in nums:<br>        if num == 0:<br>            zero_count += 1<br>            count += zero_count<br>        else:<br>            zero_count = 0<br><br>    return count
