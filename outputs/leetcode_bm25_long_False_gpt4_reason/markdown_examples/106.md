# Query `106`

## Original Question

*Anti-theft security devices are activated inside a bank. You are given a 0-indexed binary string array `bank` representing the floor plan of the bank, which is an `m x n` 2D matrix. `bank[i]` represents the `ith` row, consisting of `'0'`s and `'1'`s. `'0'` means the cell is empty, while`'1'` means the cell has a security device.

There is one laser beam between any two security devices if both conditions are met:

   The two devices are located on two different rows: `r1` and `r2`, where `r1 < r2`.
   For each row `i` where `r1 < i < r2`, there are no security devices in the `ith` row.

Laser beams are independent, i.e., one beam does not interfere nor join with another.

Return _the total number of laser beams in the bank_.

Example 1:

Input: bank = \[ "011001 ", "000000 ", "010100 ", "001000 "\]
Output: 8
Explanation: Between each of the following device pairs, there is one beam. In total, there are 8 beams:
 \ bank\[0\]\[1\] -- bank\[2\]\[1\]
 \ bank\[0\]\[1\] -- bank\[2\]\[3\]
 \ bank\[0\]\[2\] -- bank\[2\]\[1\]
 \ bank\[0\]\[2\] -- bank\[2\]\[3\]
 \ bank\[0\]\[5\] -- bank\[2\]\[1\]
 \ bank\[0\]\[5\] -- bank\[2\]\[3\]
 \ bank\[2\]\[1\] -- bank\[3\]\[2\]
 \ bank\[2\]\[3\] -- bank\[3\]\[2\]
Note that there is no beam between any device on the 0th row with any on the 3rd row.
This is because the 2nd row contains security devices, which breaks the second condition.

Example 2:

Input: bank = \[ "000 ", "111 ", "000 "\]
Output: 0
Explanation: There does not exist two devices located on two different rows.

Constraints:

   `m == bank.length`
   `n == bank[i].length`
   `1 <= m, n <= 500`
   `bank[i][j]` is either `'0'` or `'1'`.*

## gpt4 Reasoning

```markdown
The essential problem here is to calculate the total number of laser beams formed between security devices in a bank, given certain conditions on their placement. The calculation is based on the arrangement of `'1'`s in a 2D matrix representation of the bank's floor plan, where `'1'` indicates the presence of a security device and `'0'` indicates an empty space. 

To address this problem, we can follow these steps:

1. **Count Security Devices in Each Row**: First, we need to know how many security devices are present in each row of the bank. This count is crucial because the potential number of laser beams formed with devices in another row is directly proportional to the number of devices in both rows. For a row `i` with `x` devices and another row `j` with `y` devices, the number of beams that can be formed between these two rows is `x * y`.

2. **Identify Rows with Devices**: Since laser beams can only form between devices in different rows and only if there are no devices in the rows in between them, it's important to identify which rows actually contain devices. This step will help in eliminating rows without devices from consideration, simplifying the subsequent calculations.

3. **Calculate Laser Beams Between Rows**: After identifying the rows with devices and knowing the count of devices in each of these rows, we can calculate the total number of laser beams by considering each pair of rows with devices, under the condition that there are no devices in the rows between them. However, the problem simplifies this by stating that beams only form between devices in different rows if there are no devices in the intermediate rows, meaning we just need to calculate the beams between consecutive rows with devices.

4. **Accumulate Total Beams**: Finally, by iterating through the list of rows with devices, we can calculate the total number of beams by multiplying the number of devices in consecutive pairs of rows with devices and summing up these products.

Here is a more detailed thought process for calculating beams:

- Initialize a list to keep track of the number of devices in each row that contains at least one device.
- Iterate through each row of the bank, count the number of `'1'`s in the row, and if the count is more than 0, add this count to the list.
- Initialize a variable to keep track of the total number of beams.
- Iterate through the list of device counts (except the last element because it doesn't have a subsequent row to form beams with) and for each element, calculate the product of the current element's count and the next element's count. This product represents the number of beams formed between these two rows.
- Sum these products to get the total number of beams.

This approach effectively uses the conditions given in the problem to simplify the calculation of the total number of laser beams in the bank. It focuses on the counts of devices in each relevant row and the simple combinatorial calculation needed to find the number of beams between them, without getting bogged down in unnecessary details about the specific positions of the devices within the rows.
```

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_5702.txt
> def beam_search(symbols_to_logits_fn,<br>                initial_ids,<br>                beam_size,<br>                decode_length,<br>                vocab_size,<br>                alpha,<br>                states=None,<br>                eos_id=EOS_ID,<br>                stop_early=True,<br>                use_tpu=False,<br>                use_top_k_with_unique=True):<br>  """Beam search with length penalties.<br><br>  Requires a function that can take the currently decoded symbols and return<br>  the logits for the next symbol. The implementation is inspired by<br>  https://arxiv.org/abs/1609.08144.<br><br>  When running, the beam search steps can be visualized by using tfdbg to watch<br>  the operations generating the output ids for each beam step.  These operations<br>  have the pattern:<br>    (alive|finished)_topk_(seq,scores)<br><br>  Operations marked `alive` represent the new beam sequences that will be<br>  processed in the next step.  Operations marked `finished` represent the<br>  completed beam sequences, which may be padded with 0s if no beams finished.<br><br>  Operations marked `seq` store the full beam sequence for the time step.<br>  Operations marked `scores` store the sequence's final log scores.<br><br>  The beam search steps will be processed sequentially in order, so when<br>  capturing observed from these operations, tensors, clients can make<br>  assumptions about which step is being recorded.<br><br>  WARNING: Assumes 2nd dimension of tensors in `states` and not invariant, this<br>  means that the shape of the 2nd dimension of these tensors will not be<br>  available (i.e. set to None) inside symbols_to_logits_fn.<br><br>  Args:<br>    symbols_to_logits_fn: Interface to the model, to provide logits.<br>        Shoud take [batch_size, decoded_ids] and return [batch_size, vocab_size]<br>    initial_ids: Ids to start off the decoding, this will be the first thing<br>        handed to symbols_to_logits_fn (after expanding to beam size)<br>        [batch_size]<br>    beam_size: Size of the beam.<br>    decode_length: Number of steps to decode for.<br>    vocab_size: Size of the vocab, must equal the size of the logits returned by<br>        symbols_to_logits_fn<br>    alpha: alpha for length penalty.<br>    states: dict (possibly nested) of decoding states.<br>    eos_id: ID for end of sentence.<br>    stop_early: a boolean - stop once best sequence is provably determined.<br>    use_tpu: A bool, whether to do beam search on TPU.<br>    use_top_k_with_unique: bool, whether to use a fast (but decreased precision)<br>      top_k during TPU beam search.<br><br>  Returns:<br>    Tuple of<br>    (decoded beams [batch_size, beam_size, decode_length]<br>     decoding probabilities [batch_size, beam_size])<br>  """<br>  batch_size = common_layers.shape_list(initial_ids)[0]<br><br>  # Assume initial_ids are prob 1.0<br>  initial_log_probs = tf.constant([[0.] + [-INF]  (beam_size - 1)])<br>  # Expand to beam_size (batch_size, beam_size)<br>  alive_log_probs = tf.tile(initial_log_probs, [batch_size, 1])<br><br>  # Expand each batch and state to beam_size<br>  alive_seq = _expand_to_beam_size(initial_ids, beam_size)<br>  alive_seq = tf.expand_dims(alive_seq, axis=2)  # (batch_size, beam_size, 1)<br>  if use_tpu:<br>    alive_seq = tf.tile(alive_seq, [1, 1, decode_length + 1])<br>  if states:<br>    states = nest.map_structure(<br>        lambda state: _expand_to_beam_size(state, beam_size), states)<br>  else:<br>    states = {}<br><br>  # Finished will keep track of all the sequences that have finished so far<br>  # Finished log probs will be negative infinity in the beginning<br>  # finished_flags will keep track of booleans<br>  finished_seq = tf.zeros(common_layers.shape_list(alive_seq), tf.int32)<br>  # Setting the scores of the initial to negative infinity.<br>  finished_scores = tf.ones([batch_size, beam_size])  -INF<br>  finished_flags = tf.zeros([batch_size, beam_size], tf.bool)<br><br>  def grow_finished(finished_seq, finished_scores, finished_flags, curr_seq,<br>                    curr_scores, curr_finished):<br>    """Given sequences and scores, will gather the top k=beam size sequences.<br><br>    Args:<br>      finished_seq: Current finished sequences.<br>        [batch_size, beam_size, current_decoded_length]<br>      finished_scores: scores for each of these sequences.<br>        [batch_size, beam_size]<br>      finished_flags: finished bools for each of these sequences.<br>        [batch_size, beam_size]<br>      curr_seq: current topk sequence that has been grown by one position.<br>        [batch_size, beam_size, current_decoded_length]<br>      curr_scores: scores for each of these sequences. [batch_size, beam_size]<br>      curr_finished: Finished flags for each of these sequences.<br>        [batch_size, beam_size]<br>    Returns:<br>      Tuple of<br>        (Topk sequences based on scores,<br>         log probs of these sequences,<br>         Finished flags of these sequences)<br>    """<br>    if not use_tpu:<br>      # First append a column of 0'ids to finished to make the same length with<br>      # finished scores<br>      finished_seq = tf.concat(<br>          [finished_seq,<br>           tf.zeros([batch_size, beam_size, 1], tf.int32)], axis=2)<br><br>    # Set the scores of the unfinished seq in curr_seq to large negative<br>    # values<br>    curr_scores += (1. - tf.to_float(curr_finished))  -INF<br>    # concatenating the sequences and scores along beam axis<br>    curr_finished_seq = tf.concat([finished_seq, curr_seq], axis=1)<br>    curr_finished_scores = tf.concat([finished_scores, curr_scores], axis=1)<br>    curr_finished_flags = tf.concat([finished_flags, curr_finished], axis=1)<br>    return compute_topk_scores_and_seq(<br>        curr_finished_seq,<br>        curr_finished_scores,<br>        curr_finished_scores,<br>        curr_finished_flags,<br>        beam_size,<br>        batch_size,<br>        "grow_finished",<br>        use_tpu=use_tpu,<br>        use_top_k_with_unique=use_top_k_with_unique)<br><br>  def grow_alive(curr_seq, curr_scores, curr_log_probs, curr_finished, states):<br>    """Given sequences and scores, will gather the top k=beam size sequences.<br><br>    Args:<br>      curr_seq: current topk sequence that has been grown by one position.<br>        [batch_size, beam_size, i+1]<br>      curr_scores: scores for each of these sequences. [batch_size, beam_size]<br>      curr_log_probs: log probs for each of these sequences.<br>        [batch_size, beam_size]<br>      curr_finished: Finished flags for each of these sequences.<br>        [batch_size, beam_size]<br>      states: dict (possibly nested) of decoding states.<br>    Returns:<br>      Tuple of<br>        (Topk sequences based on scores,<br>         log probs of these sequences,<br>         Finished flags of these sequences)<br>    """<br>    # Set the scores of the finished seq in curr_seq to large negative<br>    # values<br>    curr_scores += tf.to_float(curr_finished)  -INF<br>    return compute_topk_scores_and_seq(curr_seq, curr_scores, curr_log_probs,<br>                                       curr_finished, beam_size, batch_size,<br>                                       "grow_alive", states, use_tpu=use_tpu)<br><br>  def grow_topk(i, alive_seq, alive_log_probs, states):<br>    r"""Inner beam search loop.<br><br>    This function takes the current alive sequences, and grows them to topk<br>    sequences where k = 2beam. We use 2beam because, we could have beam_size<br>    number of sequences that might hit <EOS and there will be no alive<br>    sequences to continue. With 2beam_size, this will not happen. This relies<br>    on the assumption the vocab size is  beam size. If this is true, we'll<br>    have at least beam_size non <EOS extensions if we extract the next top<br>    2beam words.<br>    Length penalty is given by = (5+len(decode)/6) ^ -\alpha. Pls refer to<br>    https://arxiv.org/abs/1609.08144.<br><br>    Args:<br>      i: loop index<br>      alive_seq: Topk sequences decoded so far [batch_size, beam_size, i+1]<br>      alive_log_probs: probabilities of these sequences. [batch_size, beam_size]<br>      states: dict (possibly nested) of decoding states.<br>    Returns:<br>      Tuple of<br>        (Topk sequences extended by the next word,<br>         The log probs of these sequences,<br>         The scores with length penalty of these sequences,<br>         Flags indicating which of these sequences have finished decoding,<br>         dict of transformed decoding states)<br>    """<br>    # Get the logits for all the possible next symbols<br>    if use_tpu and states:<br>      flat_ids = tf.reshape(<br>          tf.slice(alive_seq, [0, 0, i], [batch_size, beam_size, 1]),<br>          [batch_size  beam_size, -1])<br>    else:<br>      flat_ids = tf.reshape(alive_seq, [batch_size  beam_size, -1])<br><br>    # (batch_size  beam_size, decoded_length)<br>    if states:<br>      flat_states = nest.map_structure(_merge_beam_dim, states)<br>      flat_logits, flat_states = symbols_to_logits_fn(flat_ids, i, flat_states)<br>      states = nest.map_structure(<br>          lambda t: _unmerge_beam_dim(t, batch_size, beam_size), flat_states)<br>    elif use_tpu:<br>      flat_logits = symbols_to_logits_fn(flat_ids, i)<br>    else:<br>      flat_logits = symbols_to_logits_fn(flat_ids)<br><br>    logits = tf.reshape(flat_logits, [batch_size, beam_size, -1])<br><br>    # Convert logits to normalized log probs<br>    candidate_log_probs = common_layers.log_prob_from_logits(logits)<br><br>    # Multiply the probabilities by the current probabilities of the beam.<br>    # (batch_size, beam_size, vocab_size) + (batch_size, beam_size, 1)<br>    log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)<br><br>    length_penalty = tf.pow(((5. + tf.to_float(i + 1)) / 6.), alpha)<br><br>    curr_scores = log_probs / length_penalty<br>    # Flatten out (beam_size, vocab_size) probs in to a list of possibilities<br>    flat_curr_scores = tf.reshape(curr_scores, [-1, beam_size  vocab_size])<br><br>    if use_tpu and use_top_k_with_unique:<br>      topk_scores, topk_ids = top_k_with_unique(<br>          flat_curr_scores, k=beam_size  2)<br>    else:<br>      topk_scores, topk_ids = tf.nn.top_k(flat_curr_scores, k=beam_size  2)<br><br>    # Recovering the log probs because we will need to send them back<br>    topk_log_probs = topk_scores  length_penalty<br><br>    # Work out what beam the top probs are in.<br>    topk_beam_index = topk_ids // vocab_size<br>    topk_ids %= vocab_size  # Unflatten the ids<br><br>    if not use_tpu:<br>      # The next three steps are to create coordinates for tf.gather_nd to pull<br>      # out the correct sequences from id's that we need to grow.<br>      # We will also use the coordinates to gather the booleans of the beam<br>      # items that survived.<br>      batch_pos = compute_batch_indices(batch_size, beam_size  2)<br><br>      # top beams will give us the actual coordinates to do the gather.<br>      # stacking will create a tensor of dimension batch  beam  2, where the<br>      # last dimension contains the i,j gathering coordinates.<br>      topk_coordinates = tf.stack([batch_pos, topk_beam_index], axis=2)<br><br>      # Gather up the most probable 2beams both for the ids and<br>      # finished_in_alive bools<br>      topk_seq = tf.gather_nd(alive_seq, topk_coordinates)<br>      if states:<br>        states = nest.map_structure(<br>            lambda state: tf.gather_nd(state, topk_coordinates), states)<br><br>      # Append the most probable alive<br>      topk_seq = tf.concat([topk_seq, tf.expand_dims(topk_ids, axis=2)], axis=2)<br>    else:<br>      # Gather up the most probable 2beams both for the ids and<br>      # finished_in_alive bools<br>      topk_seq = fast_tpu_gather(alive_seq, topk_beam_index)<br><br>      if states:<br>        states = nest.map_structure(<br>            lambda state: fast_tpu_gather(state, topk_beam_index), states)<br><br>      # Update the most probable alive<br>      topk_seq = tf.transpose(topk_seq, perm=[2, 0, 1])<br>      topk_seq = inplace_ops.alias_inplace_update(topk_seq, i + 1, topk_ids)<br>      topk_seq = tf.transpose(topk_seq, perm=[1, 2, 0])<br><br>    topk_finished = tf.equal(topk_ids, eos_id)<br><br>    return topk_seq, topk_log_probs, topk_scores, topk_finished, states<br><br>  def inner_loop(i, alive_seq, alive_log_probs, finished_seq, finished_scores,<br>                 finished_flags, states):<br>    """Inner beam search loop.<br><br>    There are three groups of tensors, alive, finished, and topk.<br>    The alive group contains information about the current alive sequences<br>    The topk group contains information about alive + topk current decoded words<br>    the finished group contains information about finished sentences, that is,<br>    the ones that have decoded to <EOS. These are what we return.<br>    The general beam search algorithm is as follows:<br>    While we haven't terminated (pls look at termination condition)<br>      1. Grow the current alive to get beam2 topk sequences<br>      2. Among the topk, keep the top beam_size ones that haven't reached EOS<br>      into alive<br>      3. Among the topk, keep the top beam_size ones have reached EOS into<br>      finished<br>    Repeat<br>    To make things simple with using fixed size tensors, we will end<br>    up inserting unfinished sequences into finished in the beginning. To stop<br>    that we add -ve INF to the score of the unfinished sequence so that when a<br>    true finished sequence does appear, it will have a higher score than all the<br>    unfinished ones.<br><br>    Args:<br>      i: loop index<br>      alive_seq: Topk sequences decoded so far [batch_size, beam_size, i+1]<br>      alive_log_probs: probabilities of the beams. [batch_size, beam_size]<br>      finished_seq: Current finished sequences.<br>        [batch_size, beam_size, i+1]<br>      finished_scores: scores for each of these sequences.<br>        [batch_size, beam_size]<br>      finished_flags: finished bools for each of these sequences.<br>        [batch_size, beam_size]<br>      states: dict (possibly nested) of decoding states.<br><br>    Returns:<br>      Tuple of<br>        (Incremented loop index<br>         New alive sequences,<br>         Log probs of the alive sequences,<br>         New finished sequences,<br>         Scores of the new finished sequences,<br>         Flags indicating which sequence in finished as reached EOS,<br>         dict of final decoding states)<br>    """<br><br>    # Each inner loop, we carry out three steps:<br>    # 1. Get the current topk items.<br>    # 2. Extract the ones that have finished and haven't finished<br>    # 3. Recompute the contents of finished based on scores.<br>    topk_seq, topk_log_probs, topk_scores, topk_finished, states = grow_topk(<br>        i, alive_seq, alive_log_probs, states)<br>    alive_seq, alive_log_probs, _, states = grow_alive(<br>        topk_seq, topk_scores, topk_log_probs, topk_finished, states)<br>    finished_seq, finished_scores, finished_flags, _ = grow_finished(<br>        finished_seq, finished_scores, finished_flags, topk_seq, topk_scores,<br>        topk_finished)<br><br>    return (i + 1, alive_seq, alive_log_probs, finished_seq, finished_scores,<br>            finished_flags, states)<br><br>  def _is_finished(i, unused_alive_seq, alive_log_probs, unused_finished_seq,<br>                   finished_scores, unused_finished_in_finished, unused_states):<br>    """Checking termination condition.<br><br>    We terminate when we decoded up to decode_length or the lowest scoring item<br>    in finished has a greater score that the highest prob item in alive divided<br>    by the max length penalty<br><br>    Args:<br>      i: loop index<br>      alive_log_probs: probabilities of the beams. [batch_size, beam_size]<br>      finished_scores: scores for each of these sequences.<br>        [batch_size, beam_size]<br><br>    Returns:<br>      Bool.<br>    """<br>    max_length_penalty = tf.pow(((5. + tf.to_float(decode_length)) / 6.), alpha)<br>    # The best possible score of the most likely alive sequence.<br>    lower_bound_alive_scores = alive_log_probs[:, 0] / max_length_penalty<br><br>    if not stop_early:<br>      # by considering the min score (in the top N beams) we ensure that<br>      # the decoder will keep decoding until there is at least one beam<br>      # (in the top N) that can be improved (w.r.t. the alive beams).<br>      # any unfinished beam will have score -INF - thus the min<br>      # will always be -INF if there is at least one unfinished beam -<br>      # which means the bound_is_met condition cannot be true in this case.<br>      lowest_score_of_finished_in_finished = tf.reduce_min(finished_scores)<br>    else:<br>      # by taking the max score we only care about the first beam;<br>      # as soon as this first beam cannot be beaten from the alive beams<br>      # the beam decoder can stop.<br>      # similarly to the above, if the top beam is not completed, its<br>      # finished_score is -INF, thus it will not activate the<br>      # bound_is_met condition. (i.e., decoder will keep going on).<br>      # note we need to find the max for every sequence eparately - so, we need<br>      # to keep the batch dimension (see axis=1)<br>      lowest_score_of_finished_in_finished = tf.reduce_max(finished_scores,<br>                                                           axis=1)<br><br>    bound_is_met = tf.reduce_all(<br>        tf.greater(lowest_score_of_finished_in_finished,<br>                   lower_bound_alive_scores))<br><br>    return tf.logical_and(<br>        tf.less(i, decode_length), tf.logical_not(bound_is_met))<br><br>  inner_shape = tf.TensorShape([None, None, None])<br>  if use_tpu:<br>    inner_shape = tf.TensorShape([batch_size, beam_size, decode_length + 1])<br>  if use_tpu:<br>    state_struc = nest.map_structure(lambda state: state.get_shape(), states)<br>  else:<br>    state_struc = nest.map_structure(get_state_shape_invariants, states)<br>  (_, alive_seq, alive_log_probs, finished_seq, finished_scores,<br>   finished_flags, states) = tf.while_loop(<br>       _is_finished,<br>       inner_loop, [<br>           tf.constant(0), alive_seq, alive_log_probs, finished_seq,<br>           finished_scores, finished_flags, states<br>       ],<br>       shape_invariants=[<br>           tf.TensorShape([]),<br>           inner_shape,<br>           alive_log_probs.get_shape(),<br>           inner_shape,<br>           finished_scores.get_shape(),<br>           finished_flags.get_shape(),<br>           state_struc<br>       ],<br>       parallel_iterations=1,<br>       back_prop=False)<br><br>  alive_seq.set_shape((None, beam_size, None))<br>  finished_seq.set_shape((None, beam_size, None))<br><br>  # Accounting for corner case: It's possible that no sequence in alive for a<br>  # particular batch item ever reached EOS. In that case, we should just copy<br>  # the contents of alive for that batch item. tf.reduce_any(finished_flags, 1)<br>  # if 0, means that no sequence for that batch index had reached EOS. We need<br>  # to do the same for the scores as well.<br>  finished_seq = tf.where(<br>      tf.reduce_any(finished_flags, 1), finished_seq, alive_seq)<br>  finished_scores = tf.where(<br>      tf.reduce_any(finished_flags, 1), finished_scores, alive_log_probs)<br>  return finished_seq, finished_scores, states

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_6408.txt
> def search(self,<br>               start_predictions: torch.Tensor,<br>               start_state: StateType,<br>               step: StepFunctionType) - Tuple[torch.Tensor, torch.Tensor]:<br>        """<br>        Given a starting state and a step function, apply beam search to find the<br>        most likely target sequences.<br><br>        Notes<br>        -----<br>        If your step function returns ``-inf`` for some log probabilities<br>        (like if you're using a masked log-softmax) then some of the "best"<br>        sequences returned may also have ``-inf`` log probability. Specifically<br>        this happens when the beam size is smaller than the number of actions<br>        with finite log probability (non-zero probability) returned by the step function.<br>        Therefore if you're using a mask you may want to check the results from ``search``<br>        and potentially discard sequences with non-finite log probability.<br><br>        Parameters<br>        ----------<br>        start_predictions : ``torch.Tensor``<br>            A tensor containing the initial predictions with shape ``(batch_size,)``.<br>            Usually the initial predictions are just the index of the "start" token<br>            in the target vocabulary.<br>        start_state : ``StateType``<br>            The initial state passed to the ``step`` function. Each value of the state dict<br>            should be a tensor of shape ``(batch_size, )``, where ```` means any other<br>            number of dimensions.<br>        step : ``StepFunctionType``<br>            A function that is responsible for computing the next most likely tokens,<br>            given the current state and the predictions from the last time step.<br>            The function should accept two arguments. The first being a tensor<br>            of shape ``(group_size,)``, representing the index of the predicted<br>            tokens from the last time step, and the second being the current state.<br>            The ``group_size`` will be ``batch_size  beam_size``, except in the initial<br>            step, for which it will just be ``batch_size``.<br>            The function is expected to return a tuple, where the first element<br>            is a tensor of shape ``(group_size, target_vocab_size)`` containing<br>            the log probabilities of the tokens for the next step, and the second<br>            element is the updated state. The tensor in the state should have shape<br>            ``(group_size, )``, where ```` means any other number of dimensions.<br><br>        Returns<br>        -------<br>        Tuple[torch.Tensor, torch.Tensor]<br>            Tuple of ``(predictions, log_probabilities)``, where ``predictions``<br>            has shape ``(batch_size, beam_size, max_steps)`` and ``log_probabilities``<br>            has shape ``(batch_size, beam_size)``.<br>        """<br>        batch_size = start_predictions.size()[0]<br><br>        # List of (batch_size, beam_size) tensors. One for each time step. Does not<br>        # include the start symbols, which are implicit.<br>        predictions: List[torch.Tensor] = []<br><br>        # List of (batch_size, beam_size) tensors. One for each time step. None for<br>        # the first.  Stores the index n for the parent prediction, i.e.<br>        # predictions[t-1][i][n], that it came from.<br>        backpointers: List[torch.Tensor] = []<br><br>        # Calculate the first timestep. This is done outside the main loop<br>        # because we are going from a single decoder input (the output from the<br>        # encoder) to the top `beam_size` decoder outputs. On the other hand,<br>        # within the main loop we are going from the `beam_size` elements of the<br>        # beam to `beam_size`^2 candidates from which we will select the top<br>        # `beam_size` elements for the next iteration.<br>        # shape: (batch_size, num_classes)<br>        start_class_log_probabilities, state = step(start_predictions, start_state)<br><br>        num_classes = start_class_log_probabilities.size()[1]<br><br>        # Make sure `per_node_beam_size` is not larger than `num_classes`.<br>        if self.per_node_beam_size  num_classes:<br>            raise ConfigurationError(f"Target vocab size ({num_classes:d}) too small "<br>                                     f"relative to per_node_beam_size ({self.per_node_beam_size:d}).\n"<br>                                     f"Please decrease beam_size or per_node_beam_size.")<br><br>        # shape: (batch_size, beam_size), (batch_size, beam_size)<br>        start_top_log_probabilities, start_predicted_classes = \<br>                start_class_log_probabilities.topk(self.beam_size)<br>        if self.beam_size == 1 and (start_predicted_classes == self._end_index).all():<br>            warnings.warn("Empty sequences predicted. You may want to increase the beam size or ensure "<br>                          "your step function is working properly.",<br>                          RuntimeWarning)<br>            return start_predicted_classes.unsqueeze(-1), start_top_log_probabilities<br><br>        # The log probabilities for the last time step.<br>        # shape: (batch_size, beam_size)<br>        last_log_probabilities = start_top_log_probabilities<br><br>        # shape: [(batch_size, beam_size)]<br>        predictions.append(start_predicted_classes)<br><br>        # Log probability tensor that mandates that the end token is selected.<br>        # shape: (batch_size  beam_size, num_classes)<br>        log_probs_after_end = start_class_log_probabilities.new_full(<br>                (batch_size  self.beam_size, num_classes),<br>                float("-inf")<br>        )<br>        log_probs_after_end[:, self._end_index] = 0.<br><br>        # Set the same state for each element in the beam.<br>        for key, state_tensor in state.items():<br>            _, last_dims = state_tensor.size()<br>            # shape: (batch_size  beam_size, )<br>            state[key] = state_tensor.\<br>                    unsqueeze(1).\<br>                    expand(batch_size, self.beam_size, last_dims).\<br>                    reshape(batch_size  self.beam_size, last_dims)<br><br>        for timestep in range(self.max_steps - 1):<br>            # shape: (batch_size  beam_size,)<br>            last_predictions = predictions[-1].reshape(batch_size  self.beam_size)<br><br>            # If every predicted token from the last step is `self._end_index`,<br>            # then we can stop early.<br>            if (last_predictions == self._end_index).all():<br>                break<br><br>            # Take a step. This get the predicted log probs of the next classes<br>            # and updates the state.<br>            # shape: (batch_size  beam_size, num_classes)<br>            class_log_probabilities, state = step(last_predictions, state)<br><br>            # shape: (batch_size  beam_size, num_classes)<br>            last_predictions_expanded = last_predictions.unsqueeze(-1).expand(<br>                    batch_size  self.beam_size,<br>                    num_classes<br>            )<br><br>            # Here we are finding any beams where we predicted the end token in<br>            # the previous timestep and replacing the distribution with a<br>            # one-hot distribution, forcing the beam to predict the end token<br>            # this timestep as well.<br>            # shape: (batch_size  beam_size, num_classes)<br>            cleaned_log_probabilities = torch.where(<br>                    last_predictions_expanded == self._end_index,<br>                    log_probs_after_end,<br>                    class_log_probabilities<br>            )<br><br>            # shape (both): (batch_size  beam_size, per_node_beam_size)<br>            top_log_probabilities, predicted_classes = \<br>                cleaned_log_probabilities.topk(self.per_node_beam_size)<br><br>            # Here we expand the last log probabilities to (batch_size  beam_size, per_node_beam_size)<br>            # so that we can add them to the current log probs for this timestep.<br>            # This lets us maintain the log probability of each element on the beam.<br>            # shape: (batch_size  beam_size, per_node_beam_size)<br>            expanded_last_log_probabilities = last_log_probabilities.\<br>                    unsqueeze(2).\<br>                    expand(batch_size, self.beam_size, self.per_node_beam_size).\<br>                    reshape(batch_size  self.beam_size, self.per_node_beam_size)<br><br>            # shape: (batch_size  beam_size, per_node_beam_size)<br>            summed_top_log_probabilities = top_log_probabilities + expanded_last_log_probabilities<br><br>            # shape: (batch_size, beam_size  per_node_beam_size)<br>            reshaped_summed = summed_top_log_probabilities.\<br>                    reshape(batch_size, self.beam_size  self.per_node_beam_size)<br><br>            # shape: (batch_size, beam_size  per_node_beam_size)<br>            reshaped_predicted_classes = predicted_classes.\<br>                    reshape(batch_size, self.beam_size  self.per_node_beam_size)<br><br>            # Keep only the top `beam_size` beam indices.<br>            # shape: (batch_size, beam_size), (batch_size, beam_size)<br>            restricted_beam_log_probs, restricted_beam_indices = reshaped_summed.topk(self.beam_size)<br><br>            # Use the beam indices to extract the corresponding classes.<br>            # shape: (batch_size, beam_size)<br>            restricted_predicted_classes = reshaped_predicted_classes.gather(1, restricted_beam_indices)<br><br>            predictions.append(restricted_predicted_classes)<br><br>            # shape: (batch_size, beam_size)<br>            last_log_probabilities = restricted_beam_log_probs<br><br>            # The beam indices come from a `beam_size  per_node_beam_size` dimension where the<br>            # indices with a common ancestor are grouped together. Hence<br>            # dividing by per_node_beam_size gives the ancestor. (Note that this is integer<br>            # division as the tensor is a LongTensor.)<br>            # shape: (batch_size, beam_size)<br>            backpointer = restricted_beam_indices / self.per_node_beam_size<br><br>            backpointers.append(backpointer)<br><br>            # Keep only the pieces of the state tensors corresponding to the<br>            # ancestors created this iteration.<br>            for key, state_tensor in state.items():<br>                _, last_dims = state_tensor.size()<br>                # shape: (batch_size, beam_size, )<br>                expanded_backpointer = backpointer.\<br>                        view(batch_size, self.beam_size, ([1]  len(last_dims))).\<br>                        expand(batch_size, self.beam_size, last_dims)<br><br>                # shape: (batch_size  beam_size, )<br>                state[key] = state_tensor.\<br>                        reshape(batch_size, self.beam_size, last_dims).\<br>                        gather(1, expanded_backpointer).\<br>                        reshape(batch_size  self.beam_size, last_dims)<br><br>        if not torch.isfinite(last_log_probabilities).all():<br>            warnings.warn("Infinite log probabilities encountered. Some final sequences may not make sense. "<br>                          "This can happen when the beam size is larger than the number of valid (non-zero "<br>                          "probability) transitions that the step function produces.",<br>                          RuntimeWarning)<br><br>        # Reconstruct the sequences.<br>        # shape: [(batch_size, beam_size, 1)]<br>        reconstructed_predictions = [predictions[-1].unsqueeze(2)]<br><br>        # shape: (batch_size, beam_size)<br>        cur_backpointers = backpointers[-1]<br><br>        for timestep in range(len(predictions) - 2, 0, -1):<br>            # shape: (batch_size, beam_size, 1)<br>            cur_preds = predictions[timestep].gather(1, cur_backpointers).unsqueeze(2)<br><br>            reconstructed_predictions.append(cur_preds)<br><br>            # shape: (batch_size, beam_size)<br>            cur_backpointers = backpointers[timestep - 1].gather(1, cur_backpointers)<br><br>        # shape: (batch_size, beam_size, 1)<br>        final_preds = predictions[0].gather(1, cur_backpointers).unsqueeze(2)<br><br>        reconstructed_predictions.append(final_preds)<br><br>        # shape: (batch_size, beam_size, max_steps)<br>        all_predictions = torch.cat(list(reversed(reconstructed_predictions)), 2)<br><br>        return all_predictions, last_log_probabilities

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_189663.txt
> def dict_from_mmcif(mmcif, path=True):<br>    """Parse mmcif file into a dictionary.<br><br>    Notes<br>    -----<br>    Full list of keys/value types, and further information on them can be viewed here:<br>            http://mmcif.wwpdb.org/docs/pdb_to_pdbx_correspondences.html<br>    All values in the returned dict are str or list(str).<br>    This means that some of the data values are string representations of integers<br>    - parse these outside of this function if desired.<br>    An alternative approach to this can be found in Biopython (via the function Bio.PDB.MMCIF2Dict.MMCIF2Dict).<br>    mmcif files are subject to the usual "here be dragons" problems of the PDB and difficult file formats.<br>    As such, this function is likely to be in a permanent state of flux as more dragons are found.<br><br>    Parameters<br>    ----------<br>    mmcif : str<br>        mmcif string or a path to an mmcif file.<br>    path : bool<br>        True if mmcif is a path.<br><br>    Returns<br>    -------<br>    cif_data : dict<br>        Keys are cif data names, e.g. '_struct_keywords.text'.<br>        Values are str or list(str).<br><br>    """<br>    if path:<br>        with open(mmcif, 'r') as foo:<br>            lines = foo.readlines()<br>    else:<br>        lines = mmcif.splitlines()<br>    lines = [' '.join(x.strip().split()) for x in lines]<br><br>    # Some of the data in a .cif files are stored between 'loop_' to initiate a loop, and '#' to terminate it.<br>    # The variable 'loop' is a flag to keep track of this behaviour.<br>    loop = False<br><br>    # Set up the dictionary to populate as the lines of the .cif file are iterated over.<br>    cif_data = {}<br><br>    for i, line in enumerate(lines):<br><br>        if not line:<br>            continue<br><br>        # hash signifies end of a loop. Ensure loop flag is set to False.<br>        if line == '#':<br>            loop = False<br>            continue<br><br>        if not loop:<br>            # This line initiates a loop section, in which keys are listed first,<br>            # followed by lines of data in which the values are listed in the same order as the above keys.<br>            # The values in the loop section will be stored as lists - there are multiple values for one key.<br>            # An example of this type of data is the 'REVDAT' section, which stores details on the (potentially<br>            # numerous) various revisions made to the PDB file during its history.<br><br>            if line[:5] == 'loop_':<br>                loop = True<br>                key_list = []<br>                continue<br><br>            # Lines beginning with '_' start with data names, i.e. keys in the cif_data dictionary.<br>            elif line[0] == '_':<br>                # If line consists only of a key, then subsequent lines may contain the associated value.<br>                if len(line.split()) == 1:<br>                    current_key = line<br>                    count = 1<br>                    while True:<br>                        # Look forward until a key is found, keeping count of the number of lines in between.<br>                        try:<br>                            if lines[i + count][0] != '_':<br>                                count += 1<br><br>                            # prevent infinite loop.<br>                            elif i + count  len(lines):<br>                                break<br><br>                            else:<br>                                if count  1:<br>                                    try:<br>                                        cif_data[current_key] = ' '.join(lines[i + 1: i + count])<br>                                    except IndexError:<br>                                        cif_data[current_key] = None<br>                                else:<br>                                    cif_data[current_key] = None<br>                                break<br>                        except IndexError:<br>                            break<br>                    continue<br><br>                # Simplest case. Line is a key-value pair, with the key identified by its first character, '_'.<br>                elif len(line.split())  1:<br>                    line = line.split()<br>                    try:<br>                        cif_data[line[0]] = ' '.join(line[1:])<br>                    except IndexError:<br>                        cif_data[line[0]] = None<br>                    continue<br><br>            # Line is one of multiple lines that are combined into a value in the while True: loop above.<br>            else:<br>                continue<br><br>        else:<br>            # Within a loop section, keys are identified by their first character '_'.<br>            # Add them to the list of keys in the loop.<br>            if line[0] == '_':<br>                if len(line.split()) == 1:<br>                    key_list.append(line)<br>                    if line not in cif_data.keys():<br>                        cif_data[line] = []<br><br>            # Within a loop section, the values are listed within a single space-separated line in the same order<br>            # that the keys were listed at the start of the loop.<br>            else:<br>                # Cannot do a simple split if any of the values themselves are strings containing at least one space.<br>                if '\"' in line and line.count('\"') % 2 == 0:<br>                    line_parts = [x.strip() for x in line.split('\"') if x]<br>                    line = []<br>                    for part in line_parts:<br>                        if line_parts.index(part) % 2 == 0:<br>                            for x in part.split():<br>                                line.append(x)<br>                        else:<br>                            line.append(part)<br><br>                elif '\'' in line and line.count('\'') % 2 == 0:<br>                    line = [x.strip() for x in line.split('\'') if x]<br><br>                elif len(key_list) == len(line.split()):<br>                    line = line.split()<br><br>                if len(key_list) == len(line):<br>                    for j, v in enumerate(line):<br>                        cif_data[key_list[j]].append(line[j])<br>                else:<br>                    # CURRENTLY THERE IS A PROBLEM WITH REALLY LONG LOOPS eg _pdbx_refine_tls, _pdbx_struct_oper_list<br>                    # The values span multiple lines, and therefore do not satisfy<br>                    # the condition of the above 'if' statement.<br>                    # A correction for this needs to keep track of the value count on subsequent lines,<br>                    # until the 'if' condition is met.<br>                    continue<br><br>    return cif_data

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_204077.txt
> def get_page_impositions(imposition_type,<br>                         sheets_per_sig=None, fold_then_collate=False,<br>                         thickness=0.2835, signature_mark=0,<br>                         elements=[]):<br>    """<br>    Calculates impositions for the elements in the content list and<br>    returns a set of page-layouts for each as manager instances.<br><br>    An imposition is a way of laying out pages on a larger sheet, such<br>    that they can be folded and cut to form a booklet.<br>    The signature size of an imposition controls how many of these<br>    large sheets can fold together as one. Larger books are made up of<br>    multiple signatures sewn together in a codex.<br><br>    Arguments:<br><br>    ``imposition_type``<br>        One of the FORMAT_ constants defined in this module: controls<br>        the kind of imposition to make.<br><br>    ``sheets_per_sig``<br>        How many sheets of paper will be folded, cut and bind<br>        together. This is often 1 for commercial presses, but DIY<br>        laser printouts often have all the pages in one signature (for<br>        stapling). If this value isn't provided, the function will<br>        assume the latter.<br><br>    ``fold_then_collate``<br>        If this is true then each printed sheet will be folded and<br>        cut, then the bundle of resulting pages will be combined with<br>        the others from its signature. This kindof defeats the object<br>        of printing in signatures, but is needed when building<br>        multi-signature books using a laser printer. It only makes<br>        sense if you have more than one sheet per sig.<br><br>    ``thickness``<br>        When many pages are folded together, the thickness of the<br>        sheets adds up. Giving a page thickness to this argument moves<br>        the printed pages around slightly to compensate.<br><br>    ``signature_mark``<br>        When working with many signatures, it can be fiddly to sort<br>        the signatures into the correct order for binding. To help we<br>        can add 'signature marks' - a triangle on the spine of each<br>        signature indicating up, and offset from signature to<br>        signature. When the signatures are correctly collated, the<br>        marks form a diagonal pattern: it is obvious when they are not<br>        correct. The value of this argument indicates the width of<br>        this pattern. Zero indicates that no mark should be added.<br><br>    ``elements``<br>        The list of individual page element to impose. There can be<br>        fewer pages to layout than spaces on the layout (i.e. in a 16<br>        page imposition, we could only have 13 pages), any additional<br>        space is left blank.<br><br>    The returned page layouts can be given to a PageLM for rendering<br>    onto individual pages of output. This method isn't a layout<br>    manager in its own right.<br>    """<br>    # Take a copy of the elements list so we can mess with it.<br>    elements = elements[:]<br><br>    # Sanity check the type<br>    s = set(imposition_type[2])<br>    assert len(s) == imposition_type[0]  imposition_type[1]  2<br>    assert max(list(s)) == imposition_type[0]  imposition_type[1]  2<br>    assert min(list(s)) == 1<br><br>    # Calculate the sheets per signature.<br>    if sheets_per_sig is None:<br>        if imposition_type == FORMAT_4_PAGE:<br>            sheets_per_sig = int(math.ceil(len(elements) / 4.0))<br>        else:<br>            sheets_per_sig = 1<br><br>    # Find basic data.<br>    cols, rows, pattern = imposition_type<br>    pages_per_side = cols  rows<br>    pages_per_sheet = pages_per_side  2<br>    pages_per_signature = sheets_per_sig  pages_per_sheet<br>    num_pages = len(elements)<br>    sheets_needed = int(math.ceil(num_pages / float(pages_per_sheet)))<br>    signatures_needed = int(math.ceil(sheets_needed / float(sheets_per_sig)))<br><br>    # Add effective margins to account for page thickness.<br>    if thickness  0:<br>        total_offset = (<br>            sheets_per_sig  pages_per_side  thickness  math.pi  0.25<br>            )<br><br>        for index, element in enumerate(elements):<br>            # Work out which spread this page is on.<br>            index_in_sig = index % pages_per_signature<br>            if (index_in_sig = pages_per_signature / 2):<br>                index_in_sig = pages_per_signature - 1 - index_in_sig<br><br>            # How many thicknesses are we from the outside of the sig<br>            out_d = (index_in_sig + 1) // 2<br><br>            # And what offset is that from the inside.<br>            outer_extra = out_d  thickness  math.pi  0.5<br>            inner_extra = total_offset - outer_extra<br><br>            # Work it out in terms of right and left<br>            if index % 2 == 0:<br>                left, right = inner_extra, outer_extra<br>            else:<br>                left, right = outer_extra, inner_extra<br><br>            # Add the new margins<br>            margined = margins.MarginsLM(0, right, 0, left, element)<br>            elements[index] = margined<br><br>    # Come up with the output plan: the order of pages to output.<br>    output_elements = []<br>    for sig in range(signatures_needed):<br>        sig_offset = sig  pages_per_signature<br><br>        # Add the signature mark to the start page of the<br>        # signature.<br>        if signature_mark  0:<br>            elements[sig_offset] = overlay.OverlayLM(<br>                elements = [<br>                    elements[sig_offset],<br>                    mark.SignatureMark(sig, signatures_needed, signature_mark)<br>                    ]<br>                )<br><br>        if fold_then_collate:<br>            # We fold and cut each sheet, then combine them into<br>            # their signatures.<br>            for sheet in range(sheets_per_sig):<br>                offset = sig_offset + sheet  pages_per_side<br>                second_half_offset = \<br>                    offset + pages_per_signature - (sheet+1)pages_per_sheet<br><br>                for index, slot in enumerate(pattern):<br>                    if slot  pages_per_side:<br>                        page_index = second_half_offset + slot - 1<br>                    else:<br>                        page_index = offset + slot - 1<br><br>                    if page_index = num_pages:<br>                        output_elements.append(None)<br>                    else:<br>                        output_elements.append(elements[page_index])<br><br>        else:<br>            # We collate the sheets together, then fold them as a whole<br>            # and cut to form the signature.<br>            sig_elements = [None]  pages_per_signature<br>            page_number = 0<br>            increasing = True<br>            for pattern_index in range(0, pages_per_sheet, 2):<br>                # We're looping for each double folio in the sheet<br><br>                # Find where these folios are in the pattern<br>                location_in_pattern = [<br>                    pattern.index(pattern_index+1),<br>                    pattern.index(pattern_index+2)<br>                    ]<br><br>                # Go through each sheet in the signature<br>                for sheet in range(sheets_per_sig):<br>                    sheet_number = \<br>                        sheet if increasing else sheets_per_sig-1-sheet<br><br>                    # Place the next page at this given location<br>                    for page in range(2):<br>                        slot_index = (<br>                            sheet_number  pages_per_sheet +<br>                            location_in_pattern[page]<br>                            )<br>                        page_index = page_number + sig_offset<br>                        if page_index < num_pages:<br>                            sig_elements[slot_index] = elements[page_index]<br><br>                        page_number += 1<br><br>                # Next time through, go in the reverse order.<br>                increasing = not increasing<br><br>            # Add all the elements for this signature to the whole<br>            output_elements.extend(sig_elements)<br><br><br><br>    # Invert the relevant pages<br>    for index, element in enumerate(output_elements):<br>        # Check if it needs to be upside down.<br>        row_from_bottom = rows - 1 - (index % pages_per_side) // cols<br>        if element is not None and row_from_bottom % 2  0:<br>            output_elements[index] = transform.RotateLM(2, element)<br><br>    # Split them into simple grids and pages.<br>    pages = []<br>    for index in range(0, len(output_elements), pages_per_side):<br>        side_elements = output_elements[index:index+pages_per_side]<br>        pages.append(grid.SimpleGridLM(<br>                cols, rows, margin=0, elements=side_elements<br>                ))<br>    return pages

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_6070.txt
> def get_agenda(self,<br>                   conservative: bool = False):<br>        """<br>        Returns an agenda that can be used guide search.<br><br>        Parameters<br>        ----------<br>        conservative : ``bool``<br>            Setting this flag will return a subset of the agenda items that correspond to high<br>            confidence lexical matches. You'll need this if you are going to use this agenda to<br>            penalize a model for producing logical forms that do not contain some items in it. In<br>            that case, you'll want this agenda to have close to perfect precision, at the cost of a<br>            lower recall. You may not want to set this flag if you are sorting the output from a<br>            search procedure based on how much of this agenda is satisfied.<br>        """<br>        agenda_items = []<br>        question_tokens = [token.text for token in self.table_context.question_tokens]<br>        question = " ".join(question_tokens)<br><br>        added_number_filters = False<br>        if self._table_has_number_columns:<br>            if "at least" in question:<br>                agenda_items.append("filter_number_greater_equals")<br>            if "at most" in question:<br>                agenda_items.append("filter_number_lesser_equals")<br><br>            comparison_triggers = ["greater", "larger", "more"]<br>            if any(f"no {word} than" in question for word in comparison_triggers):<br>                agenda_items.append("filter_number_lesser_equals")<br>            elif any(f"{word} than" in question for word in comparison_triggers):<br>                agenda_items.append("filter_number_greater")<br><br>            # We want to keep track of this because we do not want to add both number and date<br>            # filters to the agenda if we want to be conservative.<br>            if agenda_items:<br>                added_number_filters = True<br>        for token in question_tokens:<br>            if token in ["next", "below"] or (token == "after" and not conservative):<br>                agenda_items.append("next")<br>            if token in ["previous", "above"] or (token == "before" and not conservative):<br>                agenda_items.append("previous")<br>            if token in ["first", "top"]:<br>                agenda_items.append("first")<br>            if token in ["last", "bottom"]:<br>                agenda_items.append("last")<br>            if token == "same":<br>                agenda_items.append("same_as")<br><br>            if self._table_has_number_columns:<br>                # "total" does not always map to an actual summing operation.<br>                if token == "total" and not conservative:<br>                    agenda_items.append("sum")<br>                if token == "difference" or "how many more" in question or "how much more" in question:<br>                    agenda_items.append("diff")<br>                if token == "average":<br>                    agenda_items.append("average")<br>                if token in ["least", "smallest", "shortest", "lowest"] and "at least" not in question:<br>                    # This condition is too brittle. But for most logical forms with "min", there are<br>                    # semantically equivalent ones with "argmin". The exceptions are rare.<br>                    if "what is the least" not in question:<br>                        agenda_items.append("argmin")<br>                if token in ["most", "largest", "highest", "longest", "greatest"] and "at most" not in question:<br>                    # This condition is too brittle. But for most logical forms with "max", there are<br>                    # semantically equivalent ones with "argmax". The exceptions are rare.<br>                    if "what is the most" not in question:<br>                        agenda_items.append("argmax")<br><br>            if self._table_has_date_columns:<br>                if token in MONTH_NUMBERS or (token.isdigit() and len(token) == 4 and<br>                                              int(token) < 2100 and int(token)  1100):<br>                    # Token is either a month or an year. We'll add date functions.<br>                    if not added_number_filters or not conservative:<br>                        if "after" in question_tokens:<br>                            agenda_items.append("filter_date_greater")<br>                        elif "before" in question_tokens:<br>                            agenda_items.append("filter_date_lesser")<br>                        elif "not" in question_tokens:<br>                            agenda_items.append("filter_date_not_equals")<br>                        else:<br>                            agenda_items.append("filter_date_equals")<br><br>            if "what is the least" in question and self._table_has_number_columns:<br>                agenda_items.append("min_number")<br>            if "what is the most" in question and self._table_has_number_columns:<br>                agenda_items.append("max_number")<br>            if "when" in question_tokens and self._table_has_date_columns:<br>                if "last" in question_tokens:<br>                    agenda_items.append("max_date")<br>                elif "first" in question_tokens:<br>                    agenda_items.append("min_date")<br>                else:<br>                    agenda_items.append("select_date")<br><br><br>        if "how many" in question:<br>            if "sum" not in agenda_items and "average" not in agenda_items:<br>                # The question probably just requires counting the rows. But this is not very<br>                # accurate. The question could also be asking for a value that is in the table.<br>                agenda_items.append("count")<br>        agenda = []<br>        # Adding productions from the global set.<br>        for agenda_item in set(agenda_items):<br>            # Some agenda items may not be present in the terminal productions because some of these<br>            # terminals are table-content specific. For example, if the question triggered "sum",<br>            # and the table does not have number columns, we should not add "<r,<f,n - sum" to<br>            # the agenda.<br>            if agenda_item in self.terminal_productions:<br>                agenda.append(self.terminal_productions[agenda_item])<br><br>        if conservative:<br>            # Some of the columns in the table have multiple types, and thus occur in the KG as<br>            # different columns. We do not want to add them all to the agenda if their names,<br>            # because it is unlikely that logical forms use them all. In fact, to be conservative,<br>            # we won't add any of them. So we'll first identify such column names.<br>            refined_column_productions: Dict[str, str] = {}<br>            for column_name, signature in self._column_productions_for_agenda.items():<br>                column_type, name = column_name.split(":")<br>                if column_type == "string_column":<br>                    if f"number_column:{name}" not in self._column_productions_for_agenda and \<br>                       f"date_column:{name}" not in self._column_productions_for_agenda:<br>                        refined_column_productions[column_name] = signature<br><br>                elif column_type == "number_column":<br>                    if f"string_column:{name}" not in self._column_productions_for_agenda and \<br>                       f"date_column:{name}" not in self._column_productions_for_agenda:<br>                        refined_column_productions[column_name] = signature<br><br>                else:<br>                    if f"string_column:{name}" not in self._column_productions_for_agenda and \<br>                       f"number_column:{name}" not in self._column_productions_for_agenda:<br>                        refined_column_productions[column_name] = signature<br>            # Similarly, we do not want the same spans in the question to be added to the agenda as<br>            # both string and number productions.<br>            refined_entities: List[str] = []<br>            refined_numbers: List[str] = []<br>            for entity in self._question_entities:<br>                if entity.replace("string:", "") not in self._question_numbers:<br>                    refined_entities.append(entity)<br>            for number in self._question_numbers:<br>                if f"string:{number}" not in self._question_entities:<br>                    refined_numbers.append(number)<br>        else:<br>            refined_column_productions = dict(self._column_productions_for_agenda)<br>            refined_entities = list(self._question_entities)<br>            refined_numbers = list(self._question_numbers)<br><br>        # Adding column names that occur in question.<br>        question_with_underscores = "_".join(question_tokens)<br>        normalized_question = re.sub("[^a-z0-9_]", "", question_with_underscores)<br>        # We keep track of tokens that are in column names being added to the agenda. We will not<br>        # add string productions to the agenda if those tokens were already captured as column<br>        # names.<br>        # Note: If the same string occurs multiple times, this may cause string productions being<br>        # omitted from the agenda unnecessarily. That is fine, as we want to err on the side of<br>        # adding fewer rules to the agenda.<br>        tokens_in_column_names: Set[str] = set()<br>        for column_name_with_type, signature in refined_column_productions.items():<br>            column_name = column_name_with_type.split(":")[1]<br>            # Underscores ensure that the match is of whole words.<br>            if f"_{column_name}_" in normalized_question:<br>                agenda.append(signature)<br>                for token in column_name.split("_"):<br>                    tokens_in_column_names.add(token)<br><br>        # Adding all productions that lead to entities and numbers extracted from the question.<br>        for entity in refined_entities:<br>            if entity.replace("string:", "") not in tokens_in_column_names:<br>                agenda.append(f"str - {entity}")<br><br>        for number in refined_numbers:<br>            # The reason we check for the presence of the number in the question again is because<br>            # some of these numbers are extracted from number words like month names and ordinals<br>            # like "first". On looking at some agenda outputs, I found that they hurt more than help<br>            # in the agenda.<br>            if f"_{number}_" in normalized_question:<br>                agenda.append(f"Number - {number}")<br>        return agenda


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_73.txt
> def setZeroes(matrix):<br>    """Given an `m x n` integer matrix `matrix`, if an element is `0`, set its entire row and column to `0`'s.<br><br>You must do it [in place](https://en.wikipedia.org/wiki/In-place_algorithm).<br><br>Example 1:<br><br>Input: matrix = \[\[1,1,1\],\[1,0,1\],\[1,1,1\]\]<br>Output: \[\[1,0,1\],\[0,0,0\],\[1,0,1\]\]<br><br>Example 2:<br><br>Input: matrix = \[\[0,1,2,0\],\[3,4,5,2\],\[1,3,1,5\]\]<br>Output: \[\[0,0,0,0\],\[0,4,5,0\],\[0,3,1,0\]\]<br><br>Constraints:<br><br>   `m == matrix.length`<br>   `n == matrix[0].length`<br>   `1 <= m, n <= 200`<br>   `-231 <= matrix[i][j] <= 231 - 1`<br><br>Follow up:<br><br>   A straightforward solution using `O(mn)` space is probably a bad idea.<br>   A simple improvement uses `O(m + n)` space, but still not the best solution.<br>   Could you devise a constant space solution?"""<br><br>    rows = len(matrix)<br>    cols = len(matrix[0])<br>    firstRow, firstCol = False, False<br><br>    for i in range(rows):<br>        for j in range(cols):<br>            if matrix[i][j] == 0:<br>                if i == 0: firstRow = True<br>                if j == 0: firstCol = True<br>                matrix[i][0] = 0<br>                matrix[0][j] = 0<br><br>    for i in range(1, rows):<br>        for j in range(1, cols):<br>            if matrix[i][0] == 0 or matrix[0][j] == 0:<br>                matrix[i][j] = 0<br><br>    if firstRow:<br>        for j in range(cols):<br>            matrix[0][j] = 0<br><br>    if firstCol:<br>        for i in range(rows):<br>            matrix[i][0] = 0
