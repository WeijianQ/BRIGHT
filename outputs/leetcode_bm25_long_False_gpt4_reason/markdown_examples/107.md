# Query `107`

## Original Question

*You are given a 0-indexed 2D integer array `grid` of size `m x n` that represents a map of the items in a shop. The integers in the grid represent the following:

   `0` represents a wall that you cannot pass through.
   `1` represents an empty cell that you can freely move to and from.
   All other positive integers represent the price of an item in that cell. You may also freely move to and from these item cells.

It takes `1` step to travel between adjacent grid cells.

You are also given integer arrays `pricing` and `start` where `pricing = [low, high]` and `start = [row, col]` indicates that you start at the position `(row, col)` and are interested only in items with a price in the range of `[low, high]` (inclusive). You are further given an integer `k`.

You are interested in the positions of the `k` highest-ranked items whose prices are within the given price range. The rank is determined by the first of these criteria that is different:

1.  Distance, defined as the length of the shortest path from the `start` (shorter distance has a higher rank).
2.  Price (lower price has a higher rank, but it must be in the price range).
3.  The row number (smaller row number has a higher rank).
4.  The column number (smaller column number has a higher rank).

Return _the_ `k` _highest-ranked items within the price range sorted by their rank (highest to lowest)_. If there are fewer than `k` reachable items within the price range, return _all of them_.

Example 1:

Input: grid = \[\[1,2,0,1\],\[1,3,0,1\],\[0,2,5,1\]\], pricing = \[2,5\], start = \[0,0\], k = 3
Output: \[\[0,1\],\[1,1\],\[2,1\]\]
Explanation: You start at (0,0).
With a price range of \[2,5\], we can take items from (0,1), (1,1), (2,1) and (2,2).
The ranks of these items are:
- (0,1) with distance 1
- (1,1) with distance 2
- (2,1) with distance 3
- (2,2) with distance 4
Thus, the 3 highest ranked items in the price range are (0,1), (1,1), and (2,1).

Example 2:

Input: grid = \[\[1,2,0,1\],\[1,3,3,1\],\[0,2,5,1\]\], pricing = \[2,3\], start = \[2,3\], k = 2
Output: \[\[2,1\],\[1,2\]\]
Explanation: You start at (2,3).
With a price range of \[2,3\], we can take items from (0,1), (1,1), (1,2) and (2,1).
The ranks of these items are:
- (2,1) with distance 2, price 2
- (1,2) with distance 2, price 3
- (1,1) with distance 3
- (0,1) with distance 4
Thus, the 2 highest ranked items in the price range are (2,1) and (1,2).

Example 3:

Input: grid = \[\[1,1,1\],\[0,0,1\],\[2,3,4\]\], pricing = \[2,3\], start = \[0,0\], k = 3
Output: \[\[2,1\],\[2,0\]\]
Explanation: You start at (0,0).
With a price range of \[2,3\], we can take items from (2,0) and (2,1). 
The ranks of these items are: 
- (2,1) with distance 5
- (2,0) with distance 6
Thus, the 2 highest ranked items in the price range are (2,1) and (2,0). 
Note that k = 3 but there are only 2 reachable items within the price range.

Constraints:

   `m == grid.length`
   `n == grid[i].length`
   `1 <= m, n <= 105`
   `1 <= m  n <= 105`
   `0 <= grid[i][j] <= 105`
   `pricing.length == 2`
   `2 <= low <= high <= 105`
   `start.length == 2`
   `0 <= row <= m - 1`
   `0 <= col <= n - 1`
   `grid[row][col] > 0`
   `1 <= k <= m  n`*

## gpt4 Reasoning

```markdown
The essential problem here is to identify and rank the items within a given price range on a 2D grid, based on their distance from a starting point, their price, and their position (row and column numbers). The goal is to return the positions of the `k` highest-ranked items according to these criteria. This involves several key steps and considerations:

1. **Breadth-First Search (BFS) for Distance Calculation**: The problem necessitates finding the shortest path from the starting point to each item, which is a classic application for the Breadth-First Search algorithm. BFS is ideal here because it explores all neighbors at the current depth level before moving on to the nodes at the next depth level, thereby ensuring the shortest path to each cell is found first.

2. **Price Filtering**: Only items with prices within the specified `[low, high]` range should be considered. This filtering should be done as items are encountered during the BFS to avoid unnecessary exploration and computations.

3. **Ranking Criteria**: Items need to be ranked based on distance, price, row, and columnâ€”in that order of priority. This multi-criteria ranking implies that after BFS is used to find distances, items must be sorted according to these criteria.

4. **Data Structure for Storing Results**: To efficiently sort and retrieve the top `k` items, a suitable data structure is needed. A min-heap or priority queue is ideal for this task, as it can keep items sorted according to the composite ranking criteria and allow for efficient retrieval of the top `k` items.

5. **Edge Cases and Efficiency**: Special attention should be given to edge cases such as walls (represented by `0`), ensuring they are properly handled during BFS to prevent traversal through them. Additionally, considering the potential size of the grid (up to `105 x 105`), efficiency in traversing the grid and in sorting/filtering operations is paramount.

To address the problem in detail:

- **Initialize a BFS Queue**: Start with a queue for BFS that initially contains the starting position along with an initial distance of 0.

- **Perform BFS with Distance Calculation**: As you perform BFS, calculate the distance for each cell from the start position. When an item within the price range is encountered, store its details (distance, price, row, column) for later ranking.

- **Use a Priority Queue for Ranking**: As items are found, insert them into a priority queue that is ordered first by distance, then by price, then by row, and finally by column. This ensures that as items are added, they are automatically ranked according to the criteria.

- **Retrieve Top `k` Items**: Once BFS is complete or the entire grid has been explored, extract the top `k` items from the priority queue for the final result. If there are fewer than `k` items in the priority queue, return all of them.

- **Return Results**: Format the output as a list of `[row, column]` positions for the top `k` ranked items.

This approach systematically addresses the problem by combining BFS for distance calculation with strategic data structures for sorting and retrieval, ensuring efficiency and correctness in finding and ranking the items as per the given criteria.
```

### Metrics

```
recip_rank: 0.0024
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_15659.txt
> def create(dataset, label=None, features=None, distance=None, method='auto',<br>           verbose=True, kwargs):<br>    """<br>    Create a nearest neighbor model, which can be searched efficiently and<br>    quickly for the nearest neighbors of a query observation. If the `method`<br>    argument is specified as `auto`, the type of model is chosen automatically<br>    based on the type of data in `dataset`.<br><br>    .. warning::<br><br>        The 'dot_product' distance is deprecated and will be removed in future<br>        versions of Turi Create. Please use 'transformed_dot_product'<br>        distance instead, although note that this is more than a name change;<br>        it is a different transformation of the dot product of two vectors.<br>        Please see the distances module documentation for more details.<br><br>    Parameters<br>    ----------<br>    dataset : SFrame<br>        Reference data. If the features for each observation are numeric, they<br>        may be in separate columns of 'dataset' or a single column with lists<br>        of values. The features may also be in the form of a column of sparse<br>        vectors (i.e. dictionaries), with string keys and numeric values.<br><br>    label : string, optional<br>        Name of the SFrame column with row labels. If 'label' is not specified,<br>        row numbers are used to identify reference dataset rows when the model<br>        is queried.<br><br>    features : list[string], optional<br>        Name of the columns with features to use in computing distances between<br>        observations and the query points. 'None' (the default) indicates that<br>        all columns except the label should be used as features. Each column<br>        can be one of the following types:<br><br>        - Numeric: values of numeric type integer or float.<br><br>        - Array: list of numeric (integer or float) values. Each list element<br>          is treated as a separate variable in the model.<br><br>        - Dictionary: key-value pairs with numeric (integer or float) values.<br>          Each key indicates a separate variable in the model.<br><br>        - List: list of integer or string values. Each element is treated as<br>          a separate variable in the model.<br><br>        - String: string values.<br><br>        Please note: if a composite distance is also specified, this parameter<br>        is ignored.<br><br>    distance : string, function, or list[list], optional<br>        Function to measure the distance between any two input data rows. This<br>        may be one of three types:<br><br>        - String: the name of a standard distance function. One of<br>          'euclidean', 'squared_euclidean', 'manhattan', 'levenshtein',<br>          'jaccard', 'weighted_jaccard', 'cosine', 'dot_product' (deprecated),<br>          or 'transformed_dot_product'.<br><br>        - Function: a function handle from the<br>          :mod:`~turicreate.toolkits.distances` module.<br><br>        - Composite distance: the weighted sum of several standard distance<br>          functions applied to various features. This is specified as a list of<br>          distance components, each of which is itself a list containing three<br>          items:<br><br>          1. list or tuple of feature names (strings)<br><br>          2. standard distance name (string)<br><br>          3. scaling factor (int or float)<br><br>        For more information about Turi Create distance functions, please<br>        see the :py:mod:`~turicreate.toolkits.distances` module.<br><br>        If 'distance' is left unspecified or set to 'auto', a composite<br>        distance is constructed automatically based on feature types.<br><br>    method : {'auto', 'ball_tree', 'brute_force', 'lsh'}, optional<br>        Method for computing nearest neighbors. The options are:<br><br>        - auto (default): the method is chosen automatically, based on the<br>          type of data and the distance. If the distance is 'manhattan' or<br>          'euclidean' and the features are numeric or vectors of numeric<br>          values, then the 'ball_tree' method is used. Otherwise, the<br>          'brute_force' method is used.<br><br>        - ball_tree: use a tree structure to find the k-closest neighbors to<br>          each query point. The ball tree model is slower to construct than the<br>          brute force model, but queries are faster than linear time. This<br>          method is not applicable for the cosine and dot product distances.<br>          See `Liu, et al (2004)<br>          <http://papers.nips.cc/paper/2666-an-investigation-of-p<br>          ractical-approximat e-nearest-neighbor-algorithms`_ for<br>          implementation details.<br><br>        - brute_force: compute the distance from a query point to all<br>          reference observations. There is no computation time for model<br>          creation with the brute force method (although the reference data is<br>          held in the model, but each query takes linear time.<br><br>        - lsh: use Locality Sensitive Hashing (LSH) to find approximate<br>          nearest neighbors efficiently. The LSH model supports 'euclidean',<br>          'squared_euclidean', 'manhattan', 'cosine', 'jaccard', 'dot_product'<br>          (deprecated), and 'transformed_dot_product' distances. Two options<br>          are provided for LSH -- ``num_tables`` and<br>          ``num_projections_per_table``. See the notes below for details.<br><br>    verbose: bool, optional<br>        If True, print progress updates and model details.<br><br>    kwargs : optional<br>        Options for the distance function and query method.<br><br>        - leaf_size: for the ball tree method, the number of points in each<br>          leaf of the tree. The default is to use the max of 1,000 and<br>          n/(2^11), which ensures a maximum tree depth of 12.<br><br>        - num_tables: For the LSH method, the number of hash tables<br>          constructed. The default value is 20. We recommend choosing values<br>          from 10 to 30.<br><br>        - num_projections_per_table: For the LSH method, the number of<br>          projections/hash functions for each hash table. The default value is<br>          4 for 'jaccard' distance, 16 for 'cosine' distance and 8 for other<br>          distances. We recommend using number 2 ~ 6 for 'jaccard' distance, 8<br>          ~ 20 for 'cosine' distance and 4 ~ 12 for other distances.<br><br>    Returns<br>    -------<br>    out : NearestNeighborsModel<br>        A structure for efficiently computing the nearest neighbors in 'dataset'<br>        of new query points.<br><br>    See Also<br>    --------<br>    NearestNeighborsModel.query, turicreate.toolkits.distances<br><br>    Notes<br>    -----<br>    - Missing data is not allowed in the 'dataset' provided to this function.<br>      Please use the :func:`turicreate.SFrame.fillna` and<br>      :func:`turicreate.SFrame.dropna` utilities to handle missing data before<br>      creating a nearest neighbors model.<br><br>    - Missing keys in sparse vectors are assumed to have value 0.<br><br>    - The `composite_params` parameter was removed as of Turi Create<br>      version 1.5. The `distance` parameter now accepts either standard or<br>      composite distances. Please see the :mod:`~turicreate.toolkits.distances`<br>      module documentation for more information on composite distances.<br><br>    - If the features should be weighted equally in the distance calculations<br>      but are measured on different scales, it is important to standardize the<br>      features. One way to do this is to subtract the mean of each column and<br>      divide by the standard deviation.<br><br>    Locality Sensitive Hashing (LSH)<br><br>    There are several efficient nearest neighbors search algorithms that work<br>    well for data with low dimensions :math:`d` (approximately 50). However,<br>    most of the solutions suffer from either space or query time that is<br>    exponential in :math:`d`. For large :math:`d`, they often provide little,<br>    if any, improvement over the 'brute_force' method. This is a well-known<br>    consequence of the phenomenon called `The Curse of Dimensionality`.<br><br>    `Locality Sensitive Hashing (LSH)<br>    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing`_ is an approach<br>    that is designed to efficiently solve the approximate nearest neighbor<br>    search problem for high dimensional data. The key idea of LSH is to hash<br>    the data points using several hash functions, so that the probability of<br>    collision is much higher for data points which are close to each other than<br>    those which are far apart.<br><br>    An LSH family is a family of functions :math:`h` which map points from the<br>    metric space to a bucket, so that<br><br>    - if :math:`d(p, q) \\leq R`, then :math:`h(p) = h(q)` with at least probability :math:`p_1`.<br>    - if :math:`d(p, q) \\geq cR`, then :math:`h(p) = h(q)` with probability at most :math:`p_2`.<br><br>    LSH for efficient approximate nearest neighbor search:<br><br>    - We define a new family of hash functions :math:`g`, where each<br>      function :math:`g` is obtained by concatenating :math:`k` functions<br>      :math:`h_1, ..., h_k`, i.e., :math:`g(p)=[h_1(p),...,h_k(p)]`.<br>      The algorithm constructs :math:`L` hash tables, each of which<br>      corresponds to a different randomly chosen hash function :math:`g`.<br>      There are :math:`k \\cdot L` hash functions used in total.<br><br>    - In the preprocessing step, we hash all :math:`n` reference points<br>      into each of the :math:`L` hash tables.<br><br>    - Given a query point :math:`q`, the algorithm iterates over the<br>      :math:`L` hash functions :math:`g`. For each :math:`g` considered, it<br>      retrieves the data points that are hashed into the same bucket as q.<br>      These data points from all the :math:`L` hash tables are considered as<br>      candidates that are then re-ranked by their real distances with the query<br>      data.<br><br>    Note that the number of tables :math:`L` and the number of hash<br>    functions per table :math:`k` are two main parameters. They can be set<br>    using the options ``num_tables`` and ``num_projections_per_table``<br>    respectively.<br><br>    Hash functions for different distances:<br><br>    - `euclidean` and `squared_euclidean`:<br>      :math:`h(q) = \\lfloor \\frac{a \\cdot q + b}{w} \\rfloor` where<br>      :math:`a` is a vector, of which the elements are independently<br>      sampled from normal distribution, and :math:`b` is a number<br>      uniformly sampled from :math:`[0, r]`. :math:`r` is a parameter for the<br>      bucket width. We set :math:`r` using the average all-pair `euclidean`<br>      distances from a small randomly sampled subset of the reference data.<br><br>    - `manhattan`: The hash function of `manhattan` is similar with that of<br>      `euclidean`. The only difference is that the elements of `a` are sampled<br>      from Cauchy distribution, instead of normal distribution.<br><br>    - `cosine`: Random Projection is designed to approximate the cosine<br>      distance between vectors. The hash function is :math:`h(q) = sgn(a \\cdot<br>      q)`, where :math:`a` is randomly sampled normal unit vector.<br><br>    - `jaccard`: We use a recently proposed method one permutation hashing by<br>      Shrivastava and Li. See the paper `[Shrivastava and Li, UAI 2014]<br>      <http://www.auai.org/uai2014/proceedings/individuals/225.pdf`_ for<br>      details.<br><br>    - `dot_product`: The reference data points are first transformed to<br>      fixed-norm vectors, and then the minimum `dot_product` distance search<br>      problem can be solved via finding the reference data with smallest<br>      `cosine` distances. See the paper `[Neyshabur and Srebro, ICML 2015]<br>      <http://proceedings.mlr.press/v37/neyshabur15.html`_ for details.<br><br>    References<br>    ----------<br>    - `Wikipedia - nearest neighbor<br>      search <http://en.wikipedia.org/wiki/Nearest_neighbor_search`_<br><br>    - `Wikipedia - ball tree <http://en.wikipedia.org/wiki/Ball_tree`_<br><br>    - Ball tree implementation: Liu, T., et al. (2004) `An Investigation of<br>      Practical Approximate Nearest Neighbor Algorithms<br>      <http://papers.nips.cc/paper/2666-an-investigation-of-p<br>      ractical-approximat e-nearest-neighbor-algorithms`_. Advances in Neural<br>      Information Processing Systems pp. 825-832.<br><br>    - `Wikipedia - Jaccard distance<br>      <http://en.wikipedia.org/wiki/Jaccard_index`_<br><br>    - Weighted Jaccard distance: Chierichetti, F., et al. (2010) `Finding the<br>      Jaccard Median<br>      <http://theory.stanford.edu/~sergei/papers/soda10-jaccard.pdf`_.<br>      Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete<br>      Algorithms. Society for Industrial and Applied Mathematics.<br><br>    - `Wikipedia - Cosine distance<br>      <http://en.wikipedia.org/wiki/Cosine_similarity`_<br><br>    - `Wikipedia - Levenshtein distance<br>      <http://en.wikipedia.org/wiki/Levenshtein_distance`_<br><br>    - Locality Sensitive Hashing : Chapter 3 of the book `Mining Massive<br>      Datasets <http://infolab.stanford.edu/~ullman/mmds/ch3.pdf`_.<br><br>    Examples<br>    --------<br>    Construct a nearest neighbors model with automatically determined method<br>    and distance:<br><br>     sf = turicreate.SFrame({'X1': [0.98, 0.62, 0.11],<br>    ...                       'X2': [0.69, 0.58, 0.36],<br>    ...                       'str_feature': ['cat', 'dog', 'fossa']})<br>     model = turicreate.nearest_neighbors.create(sf, features=['X1', 'X2'])<br><br>    For datasets with a large number of rows and up to about 100 variables, the<br>    ball tree method often leads to much faster queries.<br><br>     model = turicreate.nearest_neighbors.create(sf, features=['X1', 'X2'],<br>    ...                                           method='ball_tree')<br><br>    Often the final determination of a neighbor is based on several distance<br>    computations over different sets of features. Each part of this composite<br>    distance may have a different relative weight.<br><br>     my_dist = [[['X1', 'X2'], 'euclidean', 2.],<br>    ...            [['str_feature'], 'levenshtein', 3.]]<br>    ...<br>     model = turicreate.nearest_neighbors.create(sf, distance=my_dist)<br>    """<br><br>    ## Validate the 'dataset' input<br>    _tkutl._raise_error_if_not_sframe(dataset, "dataset")<br>    _tkutl._raise_error_if_sframe_empty(dataset, "dataset")<br><br>    ## Basic validation of the features input<br>    if features is not None and not isinstance(features, list):<br>        raise TypeError("If specified, input 'features' must be a list of " +<br>                        "strings.")<br><br>    ## Clean the method options and create the options dictionary<br>    allowed_kwargs = ['leaf_size', 'num_tables', 'num_projections_per_table']<br>    _method_options = {}<br><br>    for k, v in kwargs.items():<br>        if k in allowed_kwargs:<br>            _method_options[k] = v<br>        else:<br>            raise _ToolkitError("'{}' is not a valid keyword argument".format(k) +<br>                                " for the nearest neighbors model. Please " +<br>                                "check for capitalization and other typos.")<br><br><br>    ## Exclude inappropriate combinations of method an distance<br>    if method == 'ball_tree' and (distance == 'cosine'<br>                                  or distance == _turicreate.distances.cosine<br>                                  or distance == 'dot_product'<br>                                  or distance == _turicreate.distances.dot_product<br>                                  or distance == 'transformed_dot_product'<br>                                  or distance == _turicreate.distances.transformed_dot_product):<br>        raise TypeError("The ball tree method does not work with 'cosine' " +<br>                        "'dot_product', or 'transformed_dot_product' distance." +<br>                        "Please use the 'brute_force' method for these distances.")<br><br><br>    if method == 'lsh' and ('num_projections_per_table' not in _method_options):<br>        if distance == 'jaccard' or distance == _turicreate.distances.jaccard:<br>            _method_options['num_projections_per_table'] = 4<br>        elif distance == 'cosine' or distance == _turicreate.distances.cosine:<br>            _method_options['num_projections_per_table'] = 16<br>        else:<br>            _method_options['num_projections_per_table'] = 8<br><br>    ## Initial validation and processing of the label<br>    if label is None:<br>        _label = _robust_column_name('__id', dataset.column_names())<br>        _dataset = dataset.add_row_number(_label)<br>    else:<br>        _label = label<br>        _dataset = _copy.copy(dataset)<br><br>    col_type_map = {c:_dataset[c].dtype for c in _dataset.column_names()}<br>    _validate_row_label(_label, col_type_map)<br>    ref_labels = _dataset[_label]<br><br><br>    ## Determine the internal list of available feature names (may still include<br>    #  the row label name).<br>    if features is None:<br>        _features = _dataset.column_names()<br>    else:<br>        _features = _copy.deepcopy(features)<br><br><br>    ## Check if there's only one feature and it's the same as the row label.<br>    #  This would also be trapped by the composite distance validation, but the<br>    #  error message is not very informative for the user.<br>    free_features = set(_features).difference([_label])<br>    if len(free_features) < 1:<br>        raise _ToolkitError("The only available feature is the same as the " +<br>                            "row label column. Please specify features " +<br>                            "that are not also row labels.")<br><br><br>    ### Validate and preprocess the distance function<br>    ### ---------------------------------------------<br>    # - The form of the 'distance' controls how we interact with the 'features'<br>    #   parameter as well.<br>    # - At this point, the row label 'label' may still be in the list(s) of<br>    #   features.<br><br>    ## Convert any distance function input into a single composite distance.<br>    # distance is already a composite distance<br>    if isinstance(distance, list):<br>        distance = _copy.deepcopy(distance)<br><br>    # distance is a single name (except 'auto') or function handle.<br>    elif (hasattr(distance, '__call__') or<br>        (isinstance(distance, str) and not distance == 'auto')):<br>        distance = [[_features, distance, 1]]<br><br>    # distance is unspecified and needs to be constructed.<br>    elif distance is None or distance == 'auto':<br>        sample = _dataset.head()<br>        distance = _construct_auto_distance(_features,<br>                                            _dataset.column_names(),<br>                                            _dataset.column_types(),<br>                                            sample)<br><br>    else:<br>        raise TypeError("Input 'distance' not understood. The 'distance' "<br>                        " argument must be a string, function handle, or " +<br>                        "composite distance.")<br><br>    ## Basic composite distance validation, remove the row label from all<br>    #  feature lists, and convert string distance names into distance functions.<br>    distance = _scrub_composite_distance_features(distance, [_label])<br>    distance = _convert_distance_names_to_functions(distance)<br>    _validate_composite_distance(distance)<br><br>    ## Raise an error if any distances are used with non-lists<br>    list_features_to_check = []<br>    sparse_distances = ['jaccard', 'weighted_jaccard', 'cosine', 'dot_product', 'transformed_dot_product']<br>    sparse_distances = [_turicreate.distances.__dict__[k] for k in sparse_distances]<br>    for d in distance:<br>        feature_names, dist, _ = d<br>        list_features = [f for f in feature_names if _dataset[f].dtype == list]<br>        for f in list_features:<br>            if dist in sparse_distances:<br>                list_features_to_check.append(f)<br>            else:<br>                raise TypeError("The chosen distance cannot currently be used " +<br>                                "on list-typed columns.")<br>    for f in list_features_to_check:<br>        only_str_lists = _validate_lists(_dataset[f], [str])<br>        if not only_str_lists:<br>            raise TypeError("Distances for sparse data, such as jaccard " +<br>                            "and weighted_jaccard, can only be used on " +<br>                            "lists containing only strings. Please modify " +<br>                            "any list features accordingly before creating " +<br>                            "the nearest neighbors model.")<br><br>    ## Raise an error if any component has string features are in single columns<br>    for d in distance:<br>        feature_names, dist, _ = d<br><br>        if (len(feature_names)  1) and (dist == _turicreate.distances.levenshtein):<br>            raise ValueError("Levenshtein distance cannot be used with multiple " +<br>                             "columns. Please concatenate strings into a single " +<br>                             "column before creating the nearest neighbors model.")<br><br>    ## Get the union of feature names and make a clean dataset.<br>    clean_features = _get_composite_distance_features(distance)<br>    sf_clean = _tkutl._toolkits_select_columns(_dataset, clean_features)<br><br><br>    ## Decide which method to use<br>    ## - If more than one distance component (specified either directly or<br>    #  generated automatically because distance set to 'auto'), then do brute<br>    #  force.<br>    if len(distance)  1:<br>        _method = 'brute_force'<br><br>        if method != 'brute_force' and verbose is True:<br>            print("Defaulting to brute force instead of ball tree because " +\<br>                "there are multiple distance components.")<br><br>    else:<br>        if method == 'auto':<br><br>            # get the total number of variables. Assume the number of elements in<br>            # array type columns does not change<br>            num_variables = sum([len(x) if hasattr(x, '__iter__') else 1<br>                for x in _six.itervalues(sf_clean[0])])<br><br>            # flag if all the features in the single composite are of numeric<br>            # type.<br>            numeric_type_flag = all([x in [int, float, list, array.array]<br>                for x in sf_clean.column_types()])<br><br>            ## Conditions necessary for ball tree to work and be worth it<br>            if ((distance[0][1] in ['euclidean',<br>                                    'manhattan',<br>                                    _turicreate.distances.euclidean,<br>                                    _turicreate.distances.manhattan])<br>                    and numeric_type_flag is True<br>                    and num_variables <= 200):<br><br>                    _method = 'ball_tree'<br><br>            else:<br>                _method = 'brute_force'<br><br>        else:<br>            _method = method<br><br><br>    ## Pick the right model name for the method<br>    if _method == 'ball_tree':<br>        model_name = 'nearest_neighbors_ball_tree'<br><br>    elif _method == 'brute_force':<br>        model_name = 'nearest_neighbors_brute_force'<br><br>    elif _method == 'lsh':<br>        model_name = 'nearest_neighbors_lsh'<br><br>    else:<br>        raise ValueError("Method must be 'auto', 'ball_tree', 'brute_force', " +<br>                         "or 'lsh'.")<br><br><br>    ## Package the model options<br>    opts = {}<br>    opts.update(_method_options)<br>    opts.update(<br>        {'model_name': model_name,<br>        'ref_labels': ref_labels,<br>        'label': label,<br>        'sf_features': sf_clean,<br>        'composite_params': distance})<br><br>    ## Construct the nearest neighbors model<br>    with QuietProgress(verbose):<br>        result = _turicreate.extensions._nearest_neighbors.train(opts)<br><br>    model_proxy = result['model']<br>    model = NearestNeighborsModel(model_proxy)<br><br>    return model

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_110614.txt
> def breadth_first_search(graph, root=None, filter=null()):<br>    """<br>    Breadth-first search.<br><br>    @type  graph: graph, digraph<br>    @param graph: Graph.<br><br>    @type  root: node<br>    @param root: Optional root node (will explore only root's connected component)<br><br>    @rtype:  tuple<br>    @return: A tuple containing a dictionary and a list.<br>        1. Generated spanning tree<br>        2. Graph's level-based ordering<br>    """<br><br>    def bfs():<br>        """<br>        Breadth-first search subfunction.<br>        """<br>        while (queue != []):<br>            node = queue.pop(0)<br>            <br>            for other in graph[node]:<br>                if (other not in spanning_tree and filter(other, node)):<br>                    queue.append(other)<br>                    ordering.append(other)<br>                    spanning_tree[other] = node<br>    <br>    queue = []            # Visiting queue<br>    spanning_tree = {}    # Spanning tree<br>    ordering = []<br>    filter.configure(graph, spanning_tree)<br>    <br>    # BFS from one node only<br>    if (root is not None):<br>        if filter(root, None):<br>            queue.append(root)<br>            ordering.append(root)<br>            spanning_tree[root] = None<br>            bfs()<br>        return spanning_tree, ordering<br><br>    # Algorithm<br>    for each in graph:<br>        if (each not in spanning_tree):<br>            if filter(each, None):<br>                queue.append(each)<br>                ordering.append(each)<br>                spanning_tree[each] = None<br>                bfs()<br><br>    return spanning_tree, ordering

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_236900.txt
> def process_edge_search(self, current, neighbor, pred, q, component, algo,<br>                            kargs):<br>        '''<br>        API: process_edge_search(self, current, neighbor, pred, q, component,<br>                                 algo, kargs)<br>        Description:<br>        Used by search() method. Processes edges according to the underlying<br>        algortihm. User does not need to call this method directly.<br>        Input:<br>            current: Name of the current node.<br>            neighbor: Name of the neighbor node.<br>            pred: Predecessor tree.<br>            q: Data structure that holds nodes to be processed in a queue.<br>            component: component number.<br>            algo: Search algorithm. See search() documentation.<br>            kwargs: Keyword arguments.<br>        Post:<br>            'color', 'distance', 'component' attribute of nodes and edges may<br>            change.<br>        '''<br>        if algo == 'Dijkstra':<br>            return self.process_edge_dijkstra(current, neighbor, pred, q,<br>                                              component)<br>        if algo == 'Prim':<br>            return self.process_edge_prim(current, neighbor, pred, q,<br>                                          component)<br>        neighbor_node = self.get_node(neighbor)<br>        if current == None:<br>            neighbor_node.set_attr('distance', 0)<br>            if isinstance(q, PriorityQueue):<br>                q.push(neighbor, 0)<br>            else:<br>                q.push(neighbor)<br>            if component != None:<br>                neighbor_node.set_attr('component', component)<br>                neighbor_node.set_attr('label', component)<br>            else:<br>                neighbor_node.set_attr('label', 0)<br>            return<br>        if isinstance(q, PriorityQueue):<br>            current_priority = q.get_priority(neighbor)<br>            if algo == 'UnweightedSPT' or algo == 'BFS':<br>                priority = self.get_node(current).get_attr('distance') + 1<br>            if algo == 'DFS':<br>                priority = -self.get_node(current).get_attr('distance') - 1<br>            if current_priority is not None and priority = current_priority:<br>                return<br>            q.push(neighbor, priority)<br>            if algo == 'UnweightedSPT' or algo == 'BFS':<br>                neighbor_node.set_attr('distance', priority)<br>            if algo == 'DFS':<br>                neighbor_node.set_attr('depth', -priority)<br>        else:<br>            distance = self.get_node(current).get_attr('distance') + 1<br>            if ((algo == 'UnweightedSPT' or algo == 'BFS') and<br>                neighbor_node.get_attr('distance') is not None):<br>                return<br>            neighbor_node.set_attr('distance', distance)<br>            neighbor_node.set_attr('label', str(distance))<br>            q.push(neighbor)<br>        pred[neighbor] = current<br>        neighbor_node.set_attr('color', 'red')<br>        if component != None:<br>            neighbor_node.set_attr('component', component)<br>            neighbor_node.set_attr('label', component)<br>        self.display()

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_110749.txt
> def heuristic_search(graph, start, goal, heuristic):<br>    """<br>    A search algorithm.<br>    <br>    A set of heuristics is available under C{graph.algorithms.heuristics}. User-created heuristics<br>    are allowed too.<br>    <br>    @type graph: graph, digraph<br>    @param graph: Graph<br>    <br>    @type start: node<br>    @param start: Start node<br>    <br>    @type goal: node<br>    @param goal: Goal node<br>    <br>    @type heuristic: function<br>    @param heuristic: Heuristic function<br>    <br>    @rtype: list<br>    @return: Optimized path from start to goal node <br>    """<br>    <br>    # The queue stores priority, node, cost to reach, and parent.<br>    queue = [ (0, start, 0, None) ]<br><br>    # This dictionary maps queued nodes to distance of discovered paths<br>    # and the computed heuristics to goal. We avoid to compute the heuristics<br>    # more than once and to insert too many times the node in the queue.<br>    g = {}<br><br>    # This maps explored nodes to parent closest to the start<br>    explored = {}<br>    <br>    while queue:<br>        _, current, dist, parent = heappop(queue)<br>        <br>        if current == goal:<br>            path = [current] + [ n for n in _reconstruct_path( parent, explored ) ]<br>            path.reverse()<br>            return path<br><br>        if current in explored:<br>            continue<br><br>        explored[current] = parent<br><br>        for neighbor in graph[current]:<br>            if neighbor in explored:<br>                continue<br>            <br>            ncost = dist + graph.edge_weight((current, neighbor))<br><br>            if neighbor in g:<br>                qcost, h = g[neighbor]<br>                if qcost <= ncost:<br>                    continue<br>                # if ncost < qcost, a longer path to neighbor remains<br>                # g. Removing it would need to filter the whole<br>                # queue, it's better just to leave it there and ignore<br>                # it when we visit the node a second time.<br>            else:<br>                h = heuristic(neighbor, goal)<br>            <br>            g[neighbor] = ncost, h<br>            heappush(queue, (ncost + h, neighbor, ncost, current))<br><br>    raise NodeUnreachable( start, goal )

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_236863.txt
> def bfs(self, root = None, display = None, priority = 'L'):<br>        '''<br>        API: bfs(self, root=None, display=None, priority='L', order='in')<br>        Description:<br>            Searches tree starting from node named root using breadth-first<br>            strategy if root argument is provided. Starts search from root node<br>            of the tree otherwise.<br>        Input:<br>            root: Starting node.<br>            display: Display mode.<br>            priority: Priority used when exploring children of the node.<br>            Acceptable arguments are 'L' and 'R'.<br>        '''<br>        if root == None:<br>            root = self.root<br>        self.traverse(root, display, Queue(), priority)


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_215.txt
> def findKthLargest(nums: List[int], k: int) - int:<br>    """Given an integer array `nums` and an integer `k`, return _the_ `kth` _largest element in the array_.<br><br>Note that it is the `kth` largest element in the sorted order, not the `kth` distinct element.<br><br>You must solve it in `O(n)` time complexity.<br><br>Example 1:<br><br>Input: nums = \[3,2,1,5,6,4\], k = 2<br>Output: 5<br><br>Example 2:<br><br>Input: nums = \[3,2,3,1,2,4,5,5,6\], k = 4<br>Output: 4<br><br>Constraints:<br><br>   `1 <= k <= nums.length <= 105`<br>   `-104 <= nums[i] <= 104`"""<br><br>    nums.sort(reverse=True)<br>    return nums[k-1]

### GROUND TRUTH 1, ranked not in top 100, leetcode/leetcode_1162.txt
> from collections import deque<br>    """Given an `n x n` `grid` containing only values `0` and `1`, where `0` represents water and `1` represents land, find a water cell such that its distance to the nearest land cell is maximized, and return the distance. If no land or water exists in the grid, return `-1`.<br><br>The distance used in this problem is the Manhattan distance: the distance between two cells `(x0, y0)` and `(x1, y1)` is `|x0 - x1| + |y0 - y1|`.<br><br>Example 1:<br><br>Input: grid = \[\[1,0,1\],\[0,0,0\],\[1,0,1\]\]<br>Output: 2<br>Explanation: The cell (1, 1) is as far as possible from all the land with distance 2.<br><br>Example 2:<br><br>Input: grid = \[\[1,0,0\],\[0,0,0\],\[0,0,0\]\]<br>Output: 4<br>Explanation: The cell (2, 2) is as far as possible from all the land with distance 4.<br><br>Constraints:<br><br>   `n == grid.length`<br>   `n == grid[i].length`<br>   `1 <= n <= 100`<br>   `grid[i][j]` is `0` or `1`"""<br><br><br>def maxDistance(grid):<br>    distance = -1<br>    water_cells = deque()<br>    n = len(grid)<br><br>    for i in range(n):<br>        for j in range(n):<br>            if grid[i][j] == 1:<br>                water_cells.append((i, j))<br><br>    if not water_cells or len(water_cells) == n  n:<br>        return distance<br><br>    directions = ((0, 1), (0, -1), (1, 0), (-1, 0))<br><br>    while water_cells:<br>        size = len(water_cells)<br>        for _ in range(size):<br>            x, y = water_cells.popleft()<br>            for dx, dy in directions:<br>                newX, newY = x + dx, y + dy<br>                if 0 <= newX < n and 0 <= newY < n and grid[newX][newY] == 0:<br>                    grid[newX][newY] = 1<br>                    water_cells.append((newX, newY))<br>        distance += 1<br>        <br>    return distance - 1

### GROUND TRUTH 2, ranked not in top 100, leetcode/leetcode_2512.txt
> def top_k_students(positive_feedback, negative_feedback, report, student_id, k):<br>    """You are given two string arrays `positive_feedback` and `negative_feedback`, containing the words denoting positive and negative feedback, respectively. Note that no word is both positive and negative.<br><br>Initially every student has `0` points. Each positive word in a feedback report increases the points of a student by `3`, whereas each negative word decreases the points by `1`.<br><br>You are given `n` feedback reports, represented by a 0-indexed string array `report` and a 0-indexed integer array `student_id`, where `student_id[i]` represents the ID of the student who has received the feedback report `report[i]`. The ID of each student is unique.<br><br>Given an integer `k`, return _the top_ `k` _students after ranking them in non-increasing order by their points_. In case more than one student has the same points, the one with the lower ID ranks higher.<br><br>Example 1:<br><br>Input: positive\_feedback = \[ "smart ", "brilliant ", "studious "\], negative\_feedback = \[ "not "\], report = \[ "this student is studious ", "the student is smart "\], student\_id = \[1,2\], k = 2<br>Output: \[1,2\]<br>Explanation: <br>Both the students have 1 positive feedback and 3 points but since student 1 has a lower ID he ranks higher.<br><br>Example 2:<br><br>Input: positive\_feedback = \[ "smart ", "brilliant ", "studious "\], negative\_feedback = \[ "not "\], report = \[ "this student is not studious ", "the student is smart "\], student\_id = \[1,2\], k = 2<br>Output: \[2,1\]<br>Explanation: <br>- The student with ID 1 has 1 positive feedback and 1 negative feedback, so he has 3-1=2 points. <br>- The student with ID 2 has 1 positive feedback, so he has 3 points. <br>Since student 2 has more points, \[2,1\] is returned.<br><br>Constraints:<br><br>   `1 <= positive_feedback.length, negative_feedback.length <= 104`<br>   `1 <= positive_feedback[i].length, negative_feedback[j].length <= 100`<br>   Both `positive_feedback[i]` and `negative_feedback[j]` consists of lowercase English letters.<br>   No word is present in both `positive_feedback` and `negative_feedback`.<br>   `n == report.length == student_id.length`<br>   `1 <= n <= 104`<br>   `report[i]` consists of lowercase English letters and spaces `' '`.<br>   There is a single space between consecutive words of `report[i]`.<br>   `1 <= report[i].length <= 100`<br>   `1 <= student_id[i] <= 109`<br>   All the values of `student_id[i]` are unique.<br>   `1 <= k <= n`"""<br><br>    points = {}<br>    for i in range(len(report)):<br>        pos_count, neg_count = 0, 0<br>        for pos in positive_feedback:<br>            pos_index = report[i].find(pos)<br>            while pos_index != -1:<br>                pos_count += 1<br>                pos_index = report[i].find(pos, pos_index + len(pos))<br>        for neg in negative_feedback:<br>            neg_index = report[i].find(neg)<br>            while neg_index != -1:<br>                neg_count += 1<br>                neg_index = report[i].find(neg, neg_index + len(neg))<br>        <br>        points[student_id[i]] = points.get(student_id[i], 0) + 3  pos_count - neg_count<br><br>    students = sorted(student_id, key=lambda x: (-points[x], x))<br>    return students[:k]
