# Query `78`

## Original Question

*You are given an integer array `jobs`, where `jobs[i]` is the amount of time it takes to complete the `ith` job.

There are `k` workers that you can assign jobs to. Each job should be assigned to exactly one worker. The working time of a worker is the sum of the time it takes to complete all jobs assigned to them. Your goal is to devise an optimal assignment such that the maximum working time of any worker is minimized.

_Return the minimum possible maximum working time of any assignment._

Example 1:

Input: jobs = \[3,2,3\], k = 3
Output: 3
Explanation: By assigning each person one job, the maximum time is 3.

Example 2:

Input: jobs = \[1,2,4,7,8\], k = 2
Output: 11
Explanation: Assign the jobs the following way:
Worker 1: 1, 2, 8 (working time = 1 + 2 + 8 = 11)
Worker 2: 4, 7 (working time = 4 + 7 = 11)
The maximum working time is 11.

Constraints:

   `1 <= k <= jobs.length <= 12`
   `1 <= jobs[i] <= 107`*

## gpt4 Reasoning

```markdown
The essential problem here is an optimization problem that falls under the category of job assignment or task scheduling. The goal is to distribute the jobs among `k` workers in such a way that the maximum working time of any worker is minimized. This problem can be challenging because it requires finding an optimal distribution of jobs that balances the workload among all workers as evenly as possible.

### Key Considerations

1. **Division of Jobs**: The primary challenge is how to divide the jobs among the workers. Since each job has a different time requirement, simply dividing the jobs based on their number may not yield the optimal solution.

2. **Variability in Job Times**: The significant variability in job times can make some distributions much less efficient than others. For example, giving one worker all the longest jobs would result in a high maximum working time, which we want to avoid.

3. **Number of Workers vs. Jobs**: The relationship between the number of jobs and the number of workers plays a crucial role. If there are as many workers as jobs, the solution is straightforwardâ€”each worker gets one job, and the maximum working time is the time of the longest job. However, when there are fewer workers than jobs, the problem becomes more complex.

### Strategies for Solution

1. **Backtracking**: One way to approach this problem is by using a backtracking algorithm that tries different distributions of jobs among workers and keeps track of the minimum possible maximum working time found. This method ensures exploring all possible distributions but can be time-consuming for larger inputs.

2. **Dynamic Programming**: Another approach could involve dynamic programming to break down the problem into smaller, more manageable sub-problems. However, the complexity of the job assignments might make formulating an effective dynamic programming solution challenging.

3. **Binary Search and Greedy Algorithms**: Since we're looking to minimize the maximum working time, a binary search on the answer (the maximum working time) can be applied. For a given potential maximum working time, we can use a greedy approach to check if it's possible to assign jobs without exceeding this time limit for any worker. This method combines binary search to narrow down the possible maximum working times and a greedy algorithm to validate each candidate maximum time.

### Relevant Information

- **Total Workload**: The sum of all job times can provide a baseline for understanding the minimum possible maximum working time, especially when considering the workload distribution across workers.

- **Job Time Distribution**: Understanding the distribution of job times (e.g., how many long vs. short jobs there are) can help in devising a strategy for evenly distributing work.

- **Optimization Techniques**: Familiarity with optimization techniques like backtracking, binary search, and greedy algorithms is crucial for devising a solution.

### Conclusion

The problem requires an optimized strategy to ensure that jobs are distributed among workers in a way that minimizes the maximum working time. Given the constraints and the potential complexity of finding the optimal distribution, a combination of binary search for narrowing down the target maximum working time and a greedy algorithm for assignment verification seems like a promising approach. This method balances the need for thoroughness in exploring possible distributions with the efficiency required to handle the problem within reasonable computational limits.
```

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_154085.txt
> def _conc_alloc_size(self, sim_size):<br>        """<br>        Concretizes a size argument, if necessary, to something that makes sense when allocating space. Here we just<br>        maximize its potential size up to the maximum variable size specified in the libc plugin.<br><br>        TODO:<br>        Further consideration of the tradeoffs of this approach is probably warranted. SimHeapPTMalloc especially makes<br>        a lot of different concretization strategy assumptions, but this function handles one of the more important<br>        problems that any heap implementation will face: how to decide the amount of space to allocate upon request for<br>        a symbolic size. Either we do as we do here and silently constrain the amount returned to a default max value,<br>        or we could add a path constraint to the state to prevent exploration of any paths that would have legitimately<br>        occurred given a larger allocation size.<br><br>        The first approach (the silent maximum) has its benefit in that the explored state space will not be<br>        constrained. Sometimes this could work out, as when an allocation is returned that is smaller than requested but<br>        which the program doesn't end up making full use of anyways. Alternatively, this lack of fidelity could cause<br>        the program to overwrite other allocations made, since it should be able to assume the allocation is as large as<br>        it requested it be.<br><br>        The second approach (the path constraint) has its benefit in that no paths will be explored that could fail<br>        when an allocation is made too small. On the other hand, as stated above, some of these paths might not have<br>        failed anyways, and doing this causes us to lose the opportunity to explore those paths.<br><br>        Perhaps these behaviors could be parameterized in the future?<br>        """<br>        if self.state.solver.symbolic(sim_size):<br>            size = self.state.solver.max_int(sim_size)<br>            if size  self.state.libc.max_variable_size:<br>                l.warning("Allocation request of %d bytes exceeded maximum of %d bytes; allocating %d bytes",<br>                          size, self.state.libc.max_variable_size, size)<br>                size = self.state.libc.max_variable_size<br>        else:<br>            size = self.state.solver.eval(sim_size)<br>        return size

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_343895.txt
> def create_service(cluster=None, serviceName=None, taskDefinition=None, loadBalancers=None, desiredCount=None, clientToken=None, role=None, deploymentConfiguration=None, placementConstraints=None, placementStrategy=None):<br>    """<br>    Runs and maintains a desired number of tasks from a specified task definition. If the number of tasks running in a service drops below desiredCount , Amazon ECS spawns another copy of the task in the specified cluster. To update an existing service, see  UpdateService .<br>    In addition to maintaining the desired count of tasks in your service, you can optionally run your service behind a load balancer. The load balancer distributes traffic across the tasks that are associated with the service. For more information, see Service Load Balancing in the Amazon EC2 Container Service Developer Guide .<br>    You can optionally specify a deployment configuration for your service. During a deployment (which is triggered by changing the task definition or the desired count of a service with an  UpdateService operation), the service scheduler uses the minimumHealthyPercent and maximumPercent parameters to determine the deployment strategy.<br>    The minimumHealthyPercent represents a lower limit on the number of your service's tasks that must remain in the RUNNING state during a deployment, as a percentage of the desiredCount (rounded up to the nearest integer). This parameter enables you to deploy without using additional cluster capacity. For example, if your service has a desiredCount of four tasks and a minimumHealthyPercent of 50%, the scheduler can stop two existing tasks to free up cluster capacity before starting two new tasks. Tasks for services that do not use a load balancer are considered healthy if they are in the RUNNING state. Tasks for services that do use a load balancer are considered healthy if they are in the RUNNING state and the container instance they are hosted on is reported as healthy by the load balancer. The default value for minimumHealthyPercent is 50% in the console and 100% for the AWS CLI, the AWS SDKs, and the APIs.<br>    The maximumPercent parameter represents an upper limit on the number of your service's tasks that are allowed in the RUNNING or PENDING state during a deployment, as a percentage of the desiredCount (rounded down to the nearest integer). This parameter enables you to define the deployment batch size. For example, if your service has a desiredCount of four tasks and a maximumPercent value of 200%, the scheduler can start four new tasks before stopping the four older tasks (provided that the cluster resources required to do this are available). The default value for maximumPercent is 200%.<br>    When the service scheduler launches new tasks, it determines task placement in your cluster using the following logic:<br>    See also: AWS API Documentation<br>    <br>    Examples<br>    This example creates a service in your default region called ecs-simple-service. The service uses the hello_world task definition and it maintains 10 copies of that task.<br>    Expected Output:<br>    This example creates a service in your default region called ecs-simple-service-elb. The service uses the ecs-demo task definition and it maintains 10 copies of that task. You must reference an existing load balancer in the same region by its name.<br>    Expected Output:<br>    <br>    :example: response = client.create_service(<br>        cluster='string',<br>        serviceName='string',<br>        taskDefinition='string',<br>        loadBalancers=[<br>            {<br>                'targetGroupArn': 'string',<br>                'loadBalancerName': 'string',<br>                'containerName': 'string',<br>                'containerPort': 123<br>            },<br>        ],<br>        desiredCount=123,<br>        clientToken='string',<br>        role='string',<br>        deploymentConfiguration={<br>            'maximumPercent': 123,<br>            'minimumHealthyPercent': 123<br>        },<br>        placementConstraints=[<br>            {<br>                'type': 'distinctInstance'|'memberOf',<br>                'expression': 'string'<br>            },<br>        ],<br>        placementStrategy=[<br>            {<br>                'type': 'random'|'spread'|'binpack',<br>                'field': 'string'<br>            },<br>        ]<br>    )<br>    <br>    <br>    :type cluster: string<br>    :param cluster: The short name or full Amazon Resource Name (ARN) of the cluster on which to run your service. If you do not specify a cluster, the default cluster is assumed.<br><br>    :type serviceName: string<br>    :param serviceName: [REQUIRED]<br>            The name of your service. Up to 255 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed. Service names must be unique within a cluster, but you can have similarly named services in multiple clusters within a region or across multiple regions.<br>            <br><br>    :type taskDefinition: string<br>    :param taskDefinition: [REQUIRED]<br>            The family and revision (family:revision ) or full Amazon Resource Name (ARN) of the task definition to run in your service. If a revision is not specified, the latest ACTIVE revision is used.<br>            <br><br>    :type loadBalancers: list<br>    :param loadBalancers: A load balancer object representing the load balancer to use with your service. Currently, you are limited to one load balancer or target group per service. After you create a service, the load balancer name or target group ARN, container name, and container port specified in the service definition are immutable.<br>            For Elastic Load Balancing Classic load balancers, this object must contain the load balancer name, the container name (as it appears in a container definition), and the container port to access from the load balancer. When a task from this service is placed on a container instance, the container instance is registered with the load balancer specified here.<br>            For Elastic Load Balancing Application load balancers, this object must contain the load balancer target group ARN, the container name (as it appears in a container definition), and the container port to access from the load balancer. When a task from this service is placed on a container instance, the container instance and port combination is registered as a target in the target group specified here.<br>            (dict) --Details on a load balancer that is used with a service.<br>            targetGroupArn (string) --The full Amazon Resource Name (ARN) of the Elastic Load Balancing target group associated with a service.<br>            loadBalancerName (string) --The name of a Classic load balancer.<br>            containerName (string) --The name of the container (as it appears in a container definition) to associate with the load balancer.<br>            containerPort (integer) --The port on the container to associate with the load balancer. This port must correspond to a containerPort in the service's task definition. Your container instances must allow ingress traffic on the hostPort of the port mapping.<br>            <br>            <br><br>    :type desiredCount: integer<br>    :param desiredCount: [REQUIRED]<br>            The number of instantiations of the specified task definition to place and keep running on your cluster.<br>            <br><br>    :type clientToken: string<br>    :param clientToken: Unique, case-sensitive identifier you provide to ensure the idempotency of the request. Up to 32 ASCII characters are allowed.<br><br>    :type role: string<br>    :param role: The name or full Amazon Resource Name (ARN) of the IAM role that allows Amazon ECS to make calls to your load balancer on your behalf. This parameter is required if you are using a load balancer with your service. If you specify the role parameter, you must also specify a load balancer object with the loadBalancers parameter.<br>            If your specified role has a path other than / , then you must either specify the full role ARN (this is recommended) or prefix the role name with the path. For example, if a role with the name bar has a path of /foo/ then you would specify /foo/bar as the role name. For more information, see Friendly Names and Paths in the IAM User Guide .<br>            <br><br>    :type deploymentConfiguration: dict<br>    :param deploymentConfiguration: Optional deployment parameters that control how many tasks run during the deployment and the ordering of stopping and starting tasks.<br>            maximumPercent (integer) --The upper limit (as a percentage of the service's desiredCount ) of the number of tasks that are allowed in the RUNNING or PENDING state in a service during a deployment. The maximum number of tasks during a deployment is the desiredCount multiplied by maximumPercent /100, rounded down to the nearest integer value.<br>            minimumHealthyPercent (integer) --The lower limit (as a percentage of the service's desiredCount ) of the number of running tasks that must remain in the RUNNING state in a service during a deployment. The minimum healthy tasks during a deployment is the desiredCount multiplied by minimumHealthyPercent /100, rounded up to the nearest integer value.<br>            <br><br>    :type placementConstraints: list<br>    :param placementConstraints: An array of placement constraint objects to use for tasks in your service. You can specify a maximum of 10 constraints per task (this limit includes constraints in the task definition and those specified at run time).<br>            (dict) --An object representing a constraint on task placement. For more information, see Task Placement Constraints in the Amazon EC2 Container Service Developer Guide .<br>            type (string) --The type of constraint. Use distinctInstance to ensure that each task in a particular group is running on a different container instance. Use memberOf to restrict selection to a group of valid candidates. Note that distinctInstance is not supported in task definitions.<br>            expression (string) --A cluster query language expression to apply to the constraint. Note you cannot specify an expression if the constraint type is distinctInstance . For more information, see Cluster Query Language in the Amazon EC2 Container Service Developer Guide .<br>            <br>            <br><br>    :type placementStrategy: list<br>    :param placementStrategy: The placement strategy objects to use for tasks in your service. You can specify a maximum of 5 strategy rules per service.<br>            (dict) --The task placement strategy for a task or service. For more information, see Task Placement Strategies in the Amazon EC2 Container Service Developer Guide .<br>            type (string) --The type of placement strategy. The random placement strategy randomly places tasks on available candidates. The spread placement strategy spreads placement across available candidates evenly based on the field parameter. The binpack strategy places tasks on available candidates that have the least available amount of the resource that is specified with the field parameter. For example, if you binpack on memory, a task is placed on the instance with the least amount of remaining memory (but still enough to run the task).<br>            field (string) --The field to apply the placement strategy against. For the spread placement strategy, valid values are instanceId (or host , which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone . For the binpack placement strategy, valid values are cpu and memory . For the random placement strategy, this field is not used.<br>            <br>            <br><br>    :rtype: dict<br>    :return: {<br>        'service': {<br>            'serviceArn': 'string',<br>            'serviceName': 'string',<br>            'clusterArn': 'string',<br>            'loadBalancers': [<br>                {<br>                    'targetGroupArn': 'string',<br>                    'loadBalancerName': 'string',<br>                    'containerName': 'string',<br>                    'containerPort': 123<br>                },<br>            ],<br>            'status': 'string',<br>            'desiredCount': 123,<br>            'runningCount': 123,<br>            'pendingCount': 123,<br>            'taskDefinition': 'string',<br>            'deploymentConfiguration': {<br>                'maximumPercent': 123,<br>                'minimumHealthyPercent': 123<br>            },<br>            'deployments': [<br>                {<br>                    'id': 'string',<br>                    'status': 'string',<br>                    'taskDefinition': 'string',<br>                    'desiredCount': 123,<br>                    'pendingCount': 123,<br>                    'runningCount': 123,<br>                    'createdAt': datetime(2015, 1, 1),<br>                    'updatedAt': datetime(2015, 1, 1)<br>                },<br>            ],<br>            'roleArn': 'string',<br>            'events': [<br>                {<br>                    'id': 'string',<br>                    'createdAt': datetime(2015, 1, 1),<br>                    'message': 'string'<br>                },<br>            ],<br>            'createdAt': datetime(2015, 1, 1),<br>            'placementConstraints': [<br>                {<br>                    'type': 'distinctInstance'|'memberOf',<br>                    'expression': 'string'<br>                },<br>            ],<br>            'placementStrategy': [<br>                {<br>                    'type': 'random'|'spread'|'binpack',<br>                    'field': 'string'<br>                },<br>            ]<br>        }<br>    }<br>    <br>    <br>    :returns: <br>    cluster (string) -- The short name or full Amazon Resource Name (ARN) of the cluster on which to run your service. If you do not specify a cluster, the default cluster is assumed.<br>    serviceName (string) -- [REQUIRED]<br>    The name of your service. Up to 255 letters (uppercase and lowercase), numbers, hyphens, and underscores are allowed. Service names must be unique within a cluster, but you can have similarly named services in multiple clusters within a region or across multiple regions.<br>    <br>    taskDefinition (string) -- [REQUIRED]<br>    The family and revision (family:revision ) or full Amazon Resource Name (ARN) of the task definition to run in your service. If a revision is not specified, the latest ACTIVE revision is used.<br>    <br>    loadBalancers (list) -- A load balancer object representing the load balancer to use with your service. Currently, you are limited to one load balancer or target group per service. After you create a service, the load balancer name or target group ARN, container name, and container port specified in the service definition are immutable.<br>    For Elastic Load Balancing Classic load balancers, this object must contain the load balancer name, the container name (as it appears in a container definition), and the container port to access from the load balancer. When a task from this service is placed on a container instance, the container instance is registered with the load balancer specified here.<br>    For Elastic Load Balancing Application load balancers, this object must contain the load balancer target group ARN, the container name (as it appears in a container definition), and the container port to access from the load balancer. When a task from this service is placed on a container instance, the container instance and port combination is registered as a target in the target group specified here.<br>    <br>    (dict) --Details on a load balancer that is used with a service.<br>    <br>    targetGroupArn (string) --The full Amazon Resource Name (ARN) of the Elastic Load Balancing target group associated with a service.<br>    <br>    loadBalancerName (string) --The name of a Classic load balancer.<br>    <br>    containerName (string) --The name of the container (as it appears in a container definition) to associate with the load balancer.<br>    <br>    containerPort (integer) --The port on the container to associate with the load balancer. This port must correspond to a containerPort in the service's task definition. Your container instances must allow ingress traffic on the hostPort of the port mapping.<br>    <br>    <br>    <br>    <br>    <br>    desiredCount (integer) -- [REQUIRED]<br>    The number of instantiations of the specified task definition to place and keep running on your cluster.<br>    <br>    clientToken (string) -- Unique, case-sensitive identifier you provide to ensure the idempotency of the request. Up to 32 ASCII characters are allowed.<br>    role (string) -- The name or full Amazon Resource Name (ARN) of the IAM role that allows Amazon ECS to make calls to your load balancer on your behalf. This parameter is required if you are using a load balancer with your service. If you specify the role parameter, you must also specify a load balancer object with the loadBalancers parameter.<br>    If your specified role has a path other than / , then you must either specify the full role ARN (this is recommended) or prefix the role name with the path. For example, if a role with the name bar has a path of /foo/ then you would specify /foo/bar as the role name. For more information, see Friendly Names and Paths in the IAM User Guide .<br>    <br>    deploymentConfiguration (dict) -- Optional deployment parameters that control how many tasks run during the deployment and the ordering of stopping and starting tasks.<br>    <br>    maximumPercent (integer) --The upper limit (as a percentage of the service's desiredCount ) of the number of tasks that are allowed in the RUNNING or PENDING state in a service during a deployment. The maximum number of tasks during a deployment is the desiredCount multiplied by maximumPercent /100, rounded down to the nearest integer value.<br>    <br>    minimumHealthyPercent (integer) --The lower limit (as a percentage of the service's desiredCount ) of the number of running tasks that must remain in the RUNNING state in a service during a deployment. The minimum healthy tasks during a deployment is the desiredCount multiplied by minimumHealthyPercent /100, rounded up to the nearest integer value.<br>    <br>    <br>    <br>    placementConstraints (list) -- An array of placement constraint objects to use for tasks in your service. You can specify a maximum of 10 constraints per task (this limit includes constraints in the task definition and those specified at run time).<br>    <br>    (dict) --An object representing a constraint on task placement. For more information, see Task Placement Constraints in the Amazon EC2 Container Service Developer Guide .<br>    <br>    type (string) --The type of constraint. Use distinctInstance to ensure that each task in a particular group is running on a different container instance. Use memberOf to restrict selection to a group of valid candidates. Note that distinctInstance is not supported in task definitions.<br>    <br>    expression (string) --A cluster query language expression to apply to the constraint. Note you cannot specify an expression if the constraint type is distinctInstance . For more information, see Cluster Query Language in the Amazon EC2 Container Service Developer Guide .<br>    <br>    <br>    <br>    <br>    <br>    placementStrategy (list) -- The placement strategy objects to use for tasks in your service. You can specify a maximum of 5 strategy rules per service.<br>    <br>    (dict) --The task placement strategy for a task or service. For more information, see Task Placement Strategies in the Amazon EC2 Container Service Developer Guide .<br>    <br>    type (string) --The type of placement strategy. The random placement strategy randomly places tasks on available candidates. The spread placement strategy spreads placement across available candidates evenly based on the field parameter. The binpack strategy places tasks on available candidates that have the least available amount of the resource that is specified with the field parameter. For example, if you binpack on memory, a task is placed on the instance with the least amount of remaining memory (but still enough to run the task).<br>    <br>    field (string) --The field to apply the placement strategy against. For the spread placement strategy, valid values are instanceId (or host , which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone . For the binpack placement strategy, valid values are cpu and memory . For the random placement strategy, this field is not used.<br>    <br>    <br>    <br>    <br>    <br>    <br>    """<br>    pass

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_13757.txt
> def _get_arg_parser():<br>    """Main function that is called when TPOT is run on the command line."""<br>    parser = argparse.ArgumentParser(<br>        description=(<br>            'A Python tool that automatically creates and optimizes machine '<br>            'learning pipelines using genetic programming.'<br>        ),<br>        add_help=False<br>    )<br><br>    parser.add_argument(<br>        'INPUT_FILE',<br>        type=str,<br>        help=(<br>            'Data file to use in the TPOT optimization process. Ensure that '<br>            'the class label column is labeled as "class".'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-h',<br>        '--help',<br>        action='help',<br>        help='Show this help message and exit.'<br>    )<br><br>    parser.add_argument(<br>        '-is',<br>        action='store',<br>        dest='INPUT_SEPARATOR',<br>        default='\t',<br>        type=str,<br>        help='Character used to separate columns in the input file.'<br>    )<br><br>    parser.add_argument(<br>        '-target',<br>        action='store',<br>        dest='TARGET_NAME',<br>        default='class',<br>        type=str,<br>        help='Name of the target column in the input file.'<br>    )<br><br>    parser.add_argument(<br>        '-mode',<br>        action='store',<br>        dest='TPOT_MODE',<br>        choices=['classification', 'regression'],<br>        default='classification',<br>        type=str,<br>        help=(<br>            'Whether TPOT is being used for a supervised classification or '<br>            'regression problem.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-o',<br>        action='store',<br>        dest='OUTPUT_FILE',<br>        default=None,<br>        type=str,<br>        help='File to export the code for the final optimized pipeline.'<br>    )<br><br>    parser.add_argument(<br>        '-g',<br>        action='store',<br>        dest='GENERATIONS',<br>        default=100,<br>        type=positive_integer,<br>        help=(<br>            'Number of iterations to run the pipeline optimization process. '<br>            'Generally, TPOT will work better when you give it more '<br>            'generations (and therefore time) to optimize the pipeline. TPOT '<br>            'will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE '<br>            'pipelines in total.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-p',<br>        action='store',<br>        dest='POPULATION_SIZE',<br>        default=100,<br>        type=positive_integer,<br>        help=(<br>            'Number of individuals to retain in the GP population every '<br>            'generation. Generally, TPOT will work better when you give it '<br>            'more individuals (and therefore time) to optimize the pipeline. '<br>            'TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE '<br>            'pipelines in total.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-os',<br>        action='store',<br>        dest='OFFSPRING_SIZE',<br>        default=None,<br>        type=positive_integer,<br>        help=(<br>            'Number of offspring to produce in each GP generation. By default,'<br>            'OFFSPRING_SIZE = POPULATION_SIZE.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-mr',<br>        action='store',<br>        dest='MUTATION_RATE',<br>        default=0.9,<br>        type=float_range,<br>        help=(<br>            'GP mutation rate in the range [0.0, 1.0]. This tells the GP '<br>            'algorithm how many pipelines to apply random changes to every '<br>            'generation. We recommend using the default parameter unless you '<br>            'understand how the mutation rate affects GP algorithms.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-xr',<br>        action='store',<br>        dest='CROSSOVER_RATE',<br>        default=0.1,<br>        type=float_range,<br>        help=(<br>            'GP crossover rate in the range [0.0, 1.0]. This tells the GP '<br>            'algorithm how many pipelines to "breed" every generation. We '<br>            'recommend using the default parameter unless you understand how '<br>            'the crossover rate affects GP algorithms.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-scoring',<br>        action='store',<br>        dest='SCORING_FN',<br>        default=None,<br>        type=str,<br>        help=(<br>            'Function used to evaluate the quality of a given pipeline for the '<br>            'problem. By default, accuracy is used for classification problems '<br>            'and mean squared error (mse) is used for regression problems. '<br><br>            'Note: If you wrote your own function, set this argument to mymodule.myfunction'<br>            'and TPOT will import your module and take the function from there.'<br>            'TPOT will assume the module can be imported from the current workdir.'<br><br>            'TPOT assumes that any function with "error" or "loss" in the name '<br>            'is meant to be minimized, whereas any other functions will be '<br>            'maximized. Offers the same options as cross_val_score: '<br>            'accuracy, '<br>            'adjusted_rand_score, '<br>            'average_precision, '<br>            'f1, '<br>            'f1_macro, '<br>            'f1_micro, '<br>            'f1_samples, '<br>            'f1_weighted, '<br>            'neg_log_loss, '<br>            'neg_mean_absolute_error, '<br>            'neg_mean_squared_error, '<br>            'neg_median_absolute_error, '<br>            'precision, '<br>            'precision_macro, '<br>            'precision_micro, '<br>            'precision_samples, '<br>            'precision_weighted, '<br>            'r2, '<br>            'recall, '<br>            'recall_macro, '<br>            'recall_micro, '<br>            'recall_samples, '<br>            'recall_weighted, '<br>            'roc_auc'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-cv',<br>        action='store',<br>        dest='NUM_CV_FOLDS',<br>        default=5,<br>        type=int,<br>        help=(<br>            'Number of folds to evaluate each pipeline over in stratified k-fold '<br>            'cross-validation during the TPOT optimization process.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-sub',<br>        action='store',<br>        dest='SUBSAMPLE',<br>        default=1.0,<br>        type=float,<br>        help=(<br>            'Subsample ratio of the training instance. Setting it to 0.5 means that TPOT '<br>            'use a random subsample of half of training data for the pipeline optimization process.'<br>        )<br>    )<br><br><br>    parser.add_argument(<br>        '-njobs',<br>        action='store',<br>        dest='NUM_JOBS',<br>        default=1,<br>        type=int,<br>        help=(<br>            'Number of CPUs for evaluating pipelines in parallel during the '<br>            'TPOT optimization process. Assigning this to -1 will use as many '<br>            'cores as available on the computer. For n_jobs below -1, '<br>            '(n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-maxtime',<br>        action='store',<br>        dest='MAX_TIME_MINS',<br>        default=None,<br>        type=int,<br>        help=(<br>            'How many minutes TPOT has to optimize the pipeline. This setting '<br>            'will override the GENERATIONS parameter and allow TPOT to run '<br>            'until it runs out of time.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-maxeval',<br>        action='store',<br>        dest='MAX_EVAL_MINS',<br>        default=5,<br>        type=float,<br>        help=(<br>            'How many minutes TPOT has to evaluate a single pipeline. Setting '<br>            'this parameter to higher values will allow TPOT to explore more '<br>            'complex pipelines but will also allow TPOT to run longer.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-s',<br>        action='store',<br>        dest='RANDOM_STATE',<br>        default=None,<br>        type=int,<br>        help=(<br>            'Random number generator seed for reproducibility. Set this seed '<br>            'if you want your TPOT run to be reproducible with the same seed '<br>            'and data set in the future.'<br>        )<br>    )<br><br><br>    parser.add_argument(<br>        '-config',<br>        action='store',<br>        dest='CONFIG_FILE',<br>        default=None,<br>        type=str,<br>        help=(<br>            'Configuration file for customizing the operators and parameters '<br>            'that TPOT uses in the optimization process. Must be a Python '<br>            'module containing a dict export named "tpot_config" or the name of '<br>            'built-in configuration.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-template',<br>        action='store',<br>        dest='TEMPLATE',<br>        default='RandomTree',<br>        type=str,<br>        help=(<br>             'Template of predefined pipeline structure. The option is for specifying a desired structure'<br>             'for the machine learning pipeline evaluated in TPOT. So far this option only supports'<br>             'linear pipeline structure. Each step in the pipeline should be a main class of operators'<br>             '(Selector, Transformer, Classifier or Regressor) or a specific operator'<br>             '(e.g. SelectPercentile) defined in TPOT operator configuration. If one step is a main class,'<br>             'TPOT will randomly assign all subclass operators (subclasses of SelectorMixin,'<br>             'TransformerMixin, ClassifierMixin or RegressorMixin in scikit-learn) to that step.'<br>             'Steps in the template are delimited by "-", e.g. "SelectPercentile-Transformer-Classifier".'<br>             'By default value of template is "RandomTree", TPOT generates tree-based pipeline randomly.'<br>        )<br>    )<br><br><br>    parser.add_argument(<br>        '-memory',<br>        action='store',<br>        dest='MEMORY',<br>        default=None,<br>        type=str,<br>        help=(<br>            'Path of a directory for pipeline caching or \"auto\" for using a temporary '<br>            'caching directory during the optimization process. If supplied, pipelines will '<br>            'cache each transformer after fitting them. This feature is used to avoid '<br>            'repeated computation by transformers within a pipeline if the parameters and '<br>            'input data are identical with another fitted pipeline during optimization process.'<br>        )<br>    )<br><br><br>    parser.add_argument(<br>        '-cf',<br>        action='store',<br>        dest='CHECKPOINT_FOLDER',<br>        default=None,<br>        type=str,<br>        help=('If supplied, a folder in which tpot will periodically '<br>        'save the best pipeline so far while optimizing. '<br>        'This is useful in multiple cases: '<br>        'sudden death before tpot could save an optimized pipeline, '<br>        'progress tracking, '<br>        "grabbing a pipeline while it's still optimizing etc."<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-es',<br>        action='store',<br>        dest='EARLY_STOP',<br>        default=None,<br>        type=int,<br>        help=(<br>            'How many generations TPOT checks whether there is no improvement '<br>            'in optimization process. End optimization process if there is no improvement '<br>            'in the set number of generations.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '-v',<br>        action='store',<br>        dest='VERBOSITY',<br>        default=1,<br>        choices=[0, 1, 2, 3],<br>        type=int,<br>        help=(<br>            'How much information TPOT communicates while it is running: '<br>            '0 = none, 1 = minimal, 2 = high, 3 = all. A setting of 2 or '<br>            'higher will add a progress bar during the optimization procedure.'<br>        )<br>    )<br><br>    parser.add_argument(<br>        '--no-update-check',<br>        action='store_true',<br>        dest='DISABLE_UPDATE_CHECK',<br>        default=False,<br>        help='Flag indicating whether the TPOT version checker should be disabled.'<br>    )<br><br>    parser.add_argument(<br>        '--version',<br>        action='version',<br>        version='TPOT {version}'.format(version=__version__),<br>        help='Show the TPOT version number and exit.'<br>    )<br><br>    return parser

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_385093.txt
> def generate_ANSI_to_move_cursor(cur_x, cur_y, target_x, target_y):<br>    """<br>        Note that x positions are absolute (0=SOL) while y positions are relative. That is,<br>        we move the y position the relative distance between cur_y and target_y. It doesn't<br>        mean that cur_y=0 means we are on the first line of the screen. We have no way of<br>        knowing how tall the screen is, etc. at draw-time so we can't know this. <br>    """<br><br><br>    """<br>        SIZE - this code (in concert with its caller) implements what I would call "local optimizations"<br>        to try to minimize the number and size of cursor movements outputted. It does not attempt "global<br>        optimizations" which I think are rarely going to be worthwhile. See the DESIGN NOTE on global<br>        optimizations in this file for more details <br>    """        <br><br><br>    string = ""<br><br>    if cur_y < target_y:    # MOVE DOWN<br>        y_dist = target_y - cur_y<br><br>        # See if we can optimize moving x and y together<br>        if cur_x == target_x: <br>        <br>            # Need to move in y only<br>            if target_x != 0: <br>                # Already in correct x position which is NOT SOL. Just output code to move cursor <br>                # down. No special optimization is possible since \n would take us to SOL and then <br>                # we'd also need to output a move for x. <br>                return "\x1b[{0}B".format(y_dist)  # ANSI code to move down N lines<br>            else:<br>                # Already in correct x position which is SOL. Output efficient code to move down.<br>                return generate_optimized_y_move_down_x_SOL(y_dist)<br>        else:<br>        <br>            # Need to move in x and y<br>            if target_x != 0: <br>                # x move is going to be required so we'll move y efficiently and as a side<br>                # effect, x will become 0. Code below will move x to the right place<br>                string += generate_optimized_y_move_down_x_SOL(y_dist)<br>                cur_x = 0<br>            else:<br>                # Output move down that brings x to SOL. Then we're done.<br>                return generate_optimized_y_move_down_x_SOL(y_dist)<br>                <br>    elif cur_y  target_y:  # MOVE UP<br>        if target_x == 0:        <br>            # We want to move up and be at the SOL. That can be achieved with one command so we're<br>            # done and we return it. However note that some terminals may not support this so we<br>            # might have to remove this optimization or make it optional if that winds up mattering for terminals we care about.  <br>            return "\x1b[{0}F".format(cur_y - target_y)     # ANSI code to move up N lines and move x to SOL<br>        else:<br>            string += "\x1b[{0}A".format(cur_y - target_y)  # ANSI code to move up N lines <br><br>    if cur_x < target_x:    # MOVE RIGHT<br>        # SIZE - Note that when the bgcolor is specified (not None) and not overdrawing another drawing (as in an animation case)<br>        # an optimization could be performed to draw spaces rather than output cursor advances. This would use less<br>        # size when advancing less than 3 columns since the min escape sequence here is len 4. Not implementing this now<br>        # \t (tab) could also be a cheap way to move forward, but not clear we can determine how far it goes or if that would<br>        # be consistent, nor whether it is ever destructive.<br>        string += "\x1b[{0}C".format(target_x - cur_x)  # ANSI code to move cursor right N columns<br>    elif cur_x  target_x:  # MOVE LEFT<br>        # SIZE - potential optimizations: \b (backspace) could be a cheaper way to move backwards when there is only a short<br>        # way to go. However, not sure if it is ever destructive so not bothering with it now.    <br>        # If we need to move to x=0, \r could be a cheap way to get there. However not entirely clear whether some terminals<br>        # will move to next line as well, and might sometimes be destructive. Not going to research this so not doing it now. <br>        string += "\x1b[{0}D".format(cur_x - target_x)  # ANSI code to move cursor left N columns <br><br>    return string

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_5201.txt
> def basic_params1():<br>  """A set of basic hyperparameters."""<br>  return hparam.HParams(<br>      # If the problem consists of variable-length sequences<br>      # (see problem.batch_size_means_tokens()), then this is the number<br>      # of tokens per batch per GPU or per TPU core.  Otherwise, this is<br>      # the number of examples per GPU or per TPU core.<br>      batch_size=4096,<br>      batch_shuffle_size=512,<br>      # If True, then if the features are of variable length, the batch_size is<br>      # used as the actual batch size (and not tokens per batch).<br>      use_fixed_batch_size=False,<br>      num_hidden_layers=4,<br>      kernel_height=3,<br>      kernel_width=1,<br>      hidden_size=64,<br>      compress_steps=0,<br>      # All hyperparameters ending in "dropout" are automatically set to 0.0<br>      # when not in training mode.<br>      dropout=0.2,<br>      clip_grad_norm=2.0,<br>      grad_noise_scale=0.0,<br>      summarize_grads=False,<br>      # Flag for whether mlperf mode is on<br>      mlperf_mode=False,<br>      # Whether to log the name and size of every variable<br>      summarize_vars=False,<br>      initializer="orthogonal",<br>      initializer_gain=1.5,<br>      label_smoothing=0.1,<br>      optimizer="adam",<br>      optimizer_adam_epsilon=1e-6,<br>      optimizer_adam_beta1=0.85,<br>      optimizer_adam_beta2=0.997,<br>      optimizer_momentum_momentum=0.9,<br>      optimizer_momentum_nesterov=False,<br>      optimizer_adafactor_beta1=0.0,<br>      optimizer_adafactor_beta2=0.999,<br>      optimizer_adafactor_factored=True,<br>      optimizer_adafactor_decay_type="pow",<br>      optimizer_adafactor_memory_exponent=0.8,<br>      optimizer_adafactor_clipping_threshold=1.0,<br>      optimizer_adafactor_multiply_by_parameter_scale=True,<br>      # Number of accumulating steps for multi step optimizers.<br>      optimizer_multistep_accumulate_steps=0,<br>      # Loss scaling used.<br>      # Generally only necessary with mixed precision training.<br>      # Mixed precision training only supports exponential scaling currently<br>      # To disable the scaler, see to 0/False<br>      mixed_precision_optimizer_loss_scaler="exponential",<br>      # Determines the initial loss scaling value for mixed precision<br>      mixed_precision_optimizer_init_loss_scale=215,<br>      # Whether to zero gradients that were not computed, so that the<br>      # appropriate slots are created. Useful for sharing checkpoints between<br>      # models with different sets of heads.<br>      optimizer_zero_grads=False,<br>      weight_decay=1e-6,<br>      weight_noise=0.0,<br>      # Defines the learning rate as a product of named functions.<br>      # Available functions are listed in learning_rate._LEARNING_RATE_FUNCTIONS<br>      # e.g. "constantlinear_warmuprsqrt_decayrsqrt_hidden_size"<br>      learning_rate_schedule="legacy",<br>      learning_rate_constant=1.0,<br>      # If learning_rate_schedule=="legacy",<br>      # then we specify decay scheme here.  Warmup is always exponential,<br>      # except with "noam" learning rate decay scheme.<br>      # see optimize.legacy_learning_rate_schedule()<br>      # TODO(noam): migrate everyone away from this.<br>      learning_rate_decay_scheme="none",<br>      # decay_steps and decay_staircase for learning_rate_decay_scheme=="exp"<br>      learning_rate_decay_steps=5000,<br>      learning_rate_decay_staircase=False,<br>      learning_rate_minimum=None,<br>      learning_rate_decay_rate=1.0,<br>      learning_rate_warmup_steps=100,<br>      learning_rate_cosine_cycle_steps=250000,<br>      learning_rate=0.1,<br>      sampling_method="argmax",  # "argmax" or "random"<br>      sampling_temp=1.0,  # temperature for sampling<br>      sampling_keep_top_k=-1,  # If 0, ignore all but the top k logits<br>      # expand the logits a piece at a time - saves memory.<br>      factored_logits=False,<br>      multiply_embedding_mode="sqrt_depth",<br>      # Parameters related to mixtures of experts.<br>      moe_hidden_sizes="2048",  # hidden layer sizes (comma-separated)<br>      moe_num_experts=64,  # number of experts per layer<br>      moe_k=2,  # how many experts to use for each batch element<br>      moe_loss_coef=1e-2,<br>      # Sequences of operations to perform on layer input and layer output.<br>      # Used by common_layers.layer_preprocess, common_layers.layer_postprocess<br>      # Each character represents an operation:<br>      # none: no preprocessing<br>      #    d: apply dropout<br>      #    n: apply normalization (see norm_type and norm_epsilon)<br>      #    a: add layer input (residual connection - only during postprocess)<br>      # The special string "none" is used instead of the empty string<br>      # to indicate no pre/postprocessing, since the empty string causes<br>      # trouble for hyperparameter tuning.<br>      # TODO(noam): The current settings ("", "dan") are the published version<br>      # of the transformer.  ("n", "da") seems better for harder-to-learn<br>      # models, so it should probably be the default.<br>      layer_preprocess_sequence="none",<br>      layer_postprocess_sequence="dan",<br>      # dropout rate to use during layer_preprocess and layer_postprocess<br>      layer_prepostprocess_dropout=0.1,<br>      # broadcast dimensions for layer_prepostprocess_dropout<br>      # a comma-separated list of integers.<br>      # see common_layers.dropout_with_broadcast_dims()<br>      # Change this to "1" to save memory.<br>      layer_prepostprocess_dropout_broadcast_dims="",<br>      # dropout some symbols (set them to 0) before embedding.<br>      symbol_dropout=0.0,<br>      # What type of normalization to use<br>      norm_type="layer",  # "batch", layer", "noam", "none".<br>      # epsilon parameter to normalization function<br>      norm_epsilon=1e-6,<br>      # pad vocabularies so that this value divides the vocabulary size.<br>      vocab_divisor=1,<br>      # During training, we drop sequences whose inputs and targets are shorter<br>      # than min_length<br>      min_length=0,<br>      # During training, we drop sequences whose inputs or targets are longer<br>      # than max_length.<br>      # If max_length==0, we use hparams.batch_size instead.<br>      max_length=0,<br>      # Pack examples on the fly.<br>      pack_dataset=False,<br>      # Use custom ops not included in standard tensorflow.<br>      use_custom_ops=True,<br>      # Split targets on the first axis into chunks of this length.<br>      split_targets_chunk_length=0,<br>      split_targets_max_chunks=100,<br>      split_targets_strided_training=False,<br>      # Maximum length in the smallest length bucket.  Setting this<br>      # flag too high will result in wasteful padding of short<br>      # sequences.  Due to some (hopefully) temporary hacks in the<br>      # data reading and batching code, setting this flag too low<br>      # results in a very long batch-shuffling queue.<br>      # TODO(noam): change this once the Datasets API changes.<br>      min_length_bucket=8,<br>      # This flag controls the number of length buckets in the data<br>      # reader.  The buckets have maximum lengths from<br>      # min_bucket_length to (max_length or batch_size), increasing<br>      # (approximately) by factors of length_bucket_step.<br>      length_bucket_step=1.1,<br>      # If set to True, drop sequences longer than max_length during eval.<br>      # This affects the validity of the evaluation metrics.<br>      eval_drop_long_sequences=False,<br>      # If True, run the model autoregressively instead of teacher-forcing<br>      # during eval<br>      eval_run_autoregressive=False,<br>      # (For features with symbol modality) If True, share all of the<br>      # input embeddings, target embeddings, and softmax weights.<br>      shared_embedding_and_softmax_weights=False,<br>      # (For features with symbol modality) If True, share the input embeddings<br>      # and target embeddings.<br>      shared_embedding=False,<br>      # (For features with symbol modality) Number to shard embeddings by.<br>      symbol_modality_num_shards=1,<br>      # Feature transformations are optional dictionaries comprising key-value<br>      # pairs of a feature name (str) and its transformation (function). If not<br>      # specified, T2TModel applies a default transformation according to the<br>      # feature's modality. Bottom is applicable to all features; loss, top, and<br>      # weights_fn are only applicable to target features.<br>      # TODO(trandustin): `name` is an optional hparam for legacy reasons,<br>      # defining variable scope names. Remove this hparam in the future.<br>      bottom={},<br>      loss={},<br>      name={},<br>      top={},<br>      weights_fn={},<br>      # The maximum length of "input" sequence.<br>      # Sequences longer than this value will be truncated. 0 or negative values<br>      # mean there is no maximum or truncation.<br>      # You can change this behavior by overriding preprocess_example() method<br>      # in your problem class.<br>      max_input_seq_length=0,<br>      # The maximum length of "target" sequence.<br>      # Sequences longer than this value will be truncated. 0 or negative values<br>      # mean there is no maximum or truncation.<br>      # You can change this behavior by overriding preprocess_example() method<br>      # in your problem class.<br>      max_target_seq_length=0,<br>      # if nonzero, we split the target sequences on example read.<br>      # This is for use with language modeling problems with fixed length<br>      # examples.  e.g.  The examples may be written with length 65536, but we<br>      # want to split each example into 64 examples of length 1024.<br>      split_to_length=0,<br>      # Video settings: how many frames to batch on input and targets.<br>      video_num_input_frames=1,<br>      video_num_target_frames=1,<br>      # This flag allows us to optionally treat a seq-to-seq problem<br>      # as a language model.  Legal values are:<br>      #<br>      # "none" - Do not prepend the inputs to the targets.<br>      # "prepend_inputs_masked_attention"<br>      #     replace "targets" in preprocessing with<br>      #     tf.concat([inputs, [0], targets], axis=1)<br>      #     i.e. we prepend the inputs to the targets with a single<br>      #     padding token in between.  Use masked self-attention on the<br>      #     entire resulting sequence.  During training, we compute losses on<br>      #     the combined sequence.  During eval, we compute the metrics<br>      #     on only the targets portion.<br>      # "prepend_inputs_full_attention"<br>      #     similar to the previous option except that each<br>      #     position in the inputs portion can see the<br>      #     entire inputs portion.  This removes the challenge of<br>      #     autoregressively predicting the inputs portion.<br>      prepend_mode="none",<br>      # Scheduled sampling is interesting for auto-regressive models.<br>      # It runs an additional step using the generated output as autoregressive<br>      # targets, which can improve the models inference results later. The<br>      # parameter scheduled_sampling_prob determines with what probability<br>      # will such additional step be run. It's turned off (0.0) by default.<br>      # This probability will exponentially warm up for the number of<br>      # steps determined by scheduled_sampling_warmup_steps.<br>      # The tensor used for the n-th pass will consist of outputs from<br>      # the (n-1)-th pass mixed with gold truth, with the proportion of gold<br>      # determined by scheduled_sampling_gold_mixin_prob. Control the number<br>      # of passes with scheduled_sampling_num_passes.<br>      scheduled_sampling_prob=0.0,<br>      scheduled_sampling_warmup_steps=50000,<br>      scheduled_sampling_gold_mixin_prob=0.5,<br>      # TODO(duckworthd): Uncomment when we can ascertain why adding an<br>      # extra field to HParam causes test failures.<br>      # scheduled_sampling_num_passes=1,<br><br>      # This setting controls whether to copy variables around in a daisy chain<br>      # (if true) or leave their placement to TensorFlow. It only affects multi<br>      # device training and mostly should be turned on for performance. One<br>      # exception are recurrent models: with dynamic loops it must be off.<br>      daisy_chain_variables=True,<br>      # If True in PREDICT mode, then last-position-only optimizations are not<br>      # used.<br>      force_full_predict=False,<br>      # Set this for pure model parallelism.  There is only one data shard.<br>      no_data_parallelism=False,<br>      # dtype used for activations. - "float32" or "bfloat16"<br>      # activation_dtype="bfloat16" currently only works on TPU.<br>      #    It lowers activation-memory usage<br>      #    and does not appear to affect quality.<br>      #    You can train on TPU with activation_dtype="bfloat16" and evaluate<br>      #    on CPU/GPU with activation_dtype="float32"<br>      activation_dtype="float32",<br>      # dtype used for parameters: "float32" or "bfloat16"<br>      # bfloat16 currently only works with optimizer="adafactor".<br>      #   The savings in memory allow for training larger models.<br>      #   Weights are encoded as (w128)^8, using pseudostochastic<br>      #   roundoff.  Initial experiments show that model quality is similar<br>      #   to baseline for about 3M training steps, but worse thereafter.<br>      weight_dtype="float32",<br>      # Directory containing a checkpoint for a pretrained model. This will only<br>      # be used if a new run is being started. Parameters not found in the<br>      # pretrained model will be randomly initialized. Superfluous parameters in<br>      # the pretrained model will be ignored.<br>      pretrained_model_dir="",<br>      # Threshold used for two cases: the primary task probability for the<br>      # constant mixing schedule, and the exponential schedule limit for when<br>      # mixing should stop (eg: 0.5 means stop at 50-50 mixing, 0.8 means stop<br>      # at 20-80 mixing for the primary-others mixing case.)<br>      multiproblem_schedule_threshold=0.5,<br>      # For more than 2 tasks, we may want to specify per-task thresholds here.<br>      # In that case, this needs to be a string with as many floating point<br>      # numbers as the number of tasks in the multi-problem. These numbers<br>      # are later normalized to add up to 1 and taken as probabilities for<br>      # each task. This enforces a constant mixing schedule and if this is<br>      # empty then the threshold from above is used for the first task and<br>      # the other tasks get the remaining probability split uniformly.<br>      multiproblem_per_task_threshold="",<br>      # The number of examples at which the proportion of the mixed in datasets<br>      # is multiproblem_schedule_threshold<br>      multiproblem_schedule_max_examples=1e7,<br>      # When training multiproblems, we can mix the data according to different<br>      # schedules. Example: a constant schedule mixing 20-80 between the primary<br>      # and other tasks.<br>      # A list of supported schedules can be found in<br>      # `data_generators.multi_problem.py`.<br>      multiproblem_mixing_schedule="constant",<br>      # A boolean that decides whether input sequence losses and target label<br>      # losses in classification problems should be reweighted.<br>      multiproblem_reweight_label_loss=False,<br>      # How much weight the targets in classification problems receive. Inputs<br>      # receive 1 minus this weight.<br>      multiproblem_label_weight=0.5,<br>      # Hyperparameters for relative attention.<br>      # The maximum relative positional distance to learn an embedding for.<br>      max_relative_position=0,<br>      # If heads share the same relative embedding.<br>      heads_share_relative_embedding=False,<br>      # If relative embedding terms are added to values too.<br>      add_relative_to_values=False,<br>      # If enable the host_call which is executed every training step.<br>      # There could be a performance drop if host_call function is slow and<br>      # cannot keep up with the TPU-side computation.<br>      tpu_enable_host_call=False,<br>      # Pad batch dim of inputs to nearest multiple of batch multiple.<br>      pad_batch=False,<br>      # When true, do not evaluate on the language model data when running the<br>      # multiproblem since it can take a while. If False, set eval_steps to<br>      # something large like 6000 or 10000.<br>      multiproblem_target_eval_only=False,<br>      # Max out the vocab size to a power of 2 for efficiency and to reserve<br>      # extra space in the vocabulary for new task ids and label classes.<br>      multiproblem_vocab_size=-1,<br>      # When using multiproblem with generation tasks, need to truncate the<br>      # inputs and targets manually before concatenating them.<br>      multiproblem_max_input_length=-1,<br>      multiproblem_max_target_length=-1,<br>      # If positive, makes training targets fixed-length in MultiProblem.<br>      multiproblem_fixed_train_length=-1,<br>      # Load weights from a second model. For instance, when using<br>      # pre-trained weights, you might want to initialize the encoder<br>      # and decoder by loading different models.<br>      warm_start_from_second="",<br>      # Area attention hyper parameters<br>      area_value_mode="none",<br>      area_key_mode="none",<br>      # Using area attention for the number of layers from the bottom<br>      num_area_layers=0,<br>      max_area_width=1,<br>      max_area_height=1,<br>      memory_height=1<br>  )


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_1986.txt
> from collections import defaultdict<br>    """There are `n` tasks assigned to you. The task times are represented as an integer array `tasks` of length `n`, where the `ith` task takes `tasks[i]` hours to finish. A work session is when you work for at most `sessionTime` consecutive hours and then take a break.<br><br>You should finish the given tasks in a way that satisfies the following conditions:<br><br>   If you start a task in a work session, you must complete it in the same work session.<br>   You can start a new task immediately after finishing the previous one.<br>   You may complete the tasks in any order.<br><br>Given `tasks` and `sessionTime`, return _the minimum number of work sessions needed to finish all the tasks following the conditions above._<br><br>The tests are generated such that `sessionTime` is greater than or equal to the maximum element in `tasks[i]`.<br><br>Example 1:<br><br>Input: tasks = \[1,2,3\], sessionTime = 3<br>Output: 2<br>Explanation: You can finish the tasks in two work sessions.<br>- First work session: finish the first and the second tasks in 1 + 2 = 3 hours.<br>- Second work session: finish the third task in 3 hours.<br><br>Example 2:<br><br>Input: tasks = \[3,1,3,1,1\], sessionTime = 8<br>Output: 2<br>Explanation: You can finish the tasks in two work sessions.<br>- First work session: finish all the tasks except the last one in 3 + 1 + 3 + 1 = 8 hours.<br>- Second work session: finish the last task in 1 hour.<br><br>Example 3:<br><br>Input: tasks = \[1,2,3,4,5\], sessionTime = 15<br>Output: 1<br>Explanation: You can finish all the tasks in one work session.<br><br>Constraints:<br><br>   `n == tasks.length`<br>   `1 <= n <= 14`<br>   `1 <= tasks[i] <= 10`<br>   `max(tasks[i]) <= sessionTime <= 15`"""<br><br><br><br>def largestPathValue(colors, edges):<br>    def dfs(node):<br>        if not cache[node][ord(colors[node]) - ord('a')]:<br>            cache[node][ord(colors[node]) - ord('a')] = 1<br>            for neighbor in graph[node]:<br>                cache[node][ord(colors[node]) - ord('a')] = max(cache[node][ord(colors[node]) - ord('a')],<br>                                                               1 + dfs(neighbor))<br>        return cache[node][ord(colors[node]) - ord('a')]<br><br>    n = len(colors)<br>    graph = [set() for _ in range(n)]<br>    for a, b in edges:<br>        graph[a].add(b)<br><br>    cache = [[0]  26 for _ in range(n)]<br>    in_degree = [0]  n<br>    for a, b in edges:<br>        in_degree[b] += 1<br>    res = -1<br>    for i in range(n):<br>        if in_degree[i] == 0:<br>            res = max(res, dfs(i))<br>    return res
