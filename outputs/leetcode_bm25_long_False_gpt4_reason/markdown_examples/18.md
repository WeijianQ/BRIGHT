# Query `18`

## Original Question

*We are playing the Guessing Game. The game will work as follows:

1.  I pick a number between `1` and `n`.
2.  You guess a number.
3.  If you guess the right number, you win the game.
4.  If you guess the wrong number, then I will tell you whether the number I picked is higher or lower, and you will continue guessing.
5.  Every time you guess a wrong number `x`, you will pay `x` dollars. If you run out of money, you lose the game.

Given a particular `n`, return _the minimum amount of money you need to guarantee a win regardless of what number I pick_.

Example 1:

Input: n = 10
Output: 16
Explanation: The winning strategy is as follows:
- The range is \[1,10\]. Guess 7.
    - If this is my number, your total is $0. Otherwise, you pay $7.
    - If my number is higher, the range is \[8,10\]. Guess 9.
        - If this is my number, your total is $7. Otherwise, you pay $9.
        - If my number is higher, it must be 10. Guess 10. Your total is $7 + $9 = $16.
        - If my number is lower, it must be 8. Guess 8. Your total is $7 + $9 = $16.
    - If my number is lower, the range is \[1,6\]. Guess 3.
        - If this is my number, your total is $7. Otherwise, you pay $3.
        - If my number is higher, the range is \[4,6\]. Guess 5.
            - If this is my number, your total is $7 + $3 = $10. Otherwise, you pay $5.
            - If my number is higher, it must be 6. Guess 6. Your total is $7 + $3 + $5 = $15.
            - If my number is lower, it must be 4. Guess 4. Your total is $7 + $3 + $5 = $15.
        - If my number is lower, the range is \[1,2\]. Guess 1.
            - If this is my number, your total is $7 + $3 = $10. Otherwise, you pay $1.
            - If my number is higher, it must be 2. Guess 2. Your total is $7 + $3 + $1 = $11.
The worst case in all these scenarios is that you pay $16. Hence, you only need $16 to guarantee a win.

Example 2:

Input: n = 1
Output: 0
Explanation: There is only one possible number, so you can guess 1 and not have to pay anything.

Example 3:

Input: n = 2
Output: 1
Explanation: There are two possible numbers, 1 and 2.
- Guess 1.
    - If this is my number, your total is $0. Otherwise, you pay $1.
    - If my number is higher, it must be 2. Guess 2. Your total is $1.
The worst case is that you pay $1.

Constraints:

   `1 <= n <= 200`*

## gpt4 Reasoning

```markdown
The essential problem here is to find the minimum amount of money required to guarantee a win in a number guessing game, where the cost of a wrong guess is equal to the guessed number itself, and the range of possible numbers is from 1 to n. This problem requires a strategy that minimizes the worst-case scenario of financial loss while ensuring a victory regardless of the number chosen by the opponent.

To tackle this problem, we need to consider a few key points:

1. **Dynamic Programming Approach**: The problem has an optimal substructure, meaning the optimal solution to the problem can be constructed from optimal solutions of its subproblems. This makes it a good candidate for dynamic programming (DP), where we break down the problem into smaller subproblems, solve each subproblem just once, and store their solutions.

2. **Minimize Maximum Loss**: The goal is to minimize the maximum amount of money that could be lost in the worst-case scenario. This involves a balance between choosing a number that splits the range into two approximately equal parts (to minimize the maximum number of guesses needed) and minimizing the cost associated with each guess.

3. **Recursion with Memoization or Bottom-Up DP**: We can use a recursive approach with memoization or a bottom-up DP approach to calculate the minimum cost for each range `[i, j]` within `1` to `n`. The cost for a range `[i, j]` is calculated by considering each possible guess `k` within the range, then taking the cost of that guess plus the maximum of two scenarios: the cost if the actual number is lower than `k` (so, the range becomes `[i, k-1]`) or if the actual number is higher than `k` (making the range `[k+1, j]`). We are interested in minimizing our loss, so we choose the guess `k` that minimizes this maximum cost.

4. **Implementation Details**:
    - **Base Case**: If the range is just one number (i.e., `i == j`), the cost is 0 because no guess is needed.
    - **Memoization Table**: If using recursion with memoization, we need a 2D table to store the minimum cost for each range `[i, j]` to avoid recalculating it.
    - **Iterative Approach**: For a bottom-up DP approach, we iteratively fill in the table starting from the smallest ranges to the largest, eventually solving for the range `[1, n]`.

5. **Optimization**: To further optimize, we can notice patterns in the guesses and the resulting costs, which might allow for pruning of the search space or identifying a more efficient guessing strategy.

In summary, the solution involves calculating, for every possible range of numbers, the minimum cost of ensuring a win within that range. This requires considering all possible guesses within each range, calculating the cost for each guess, and choosing the strategy that minimizes the worst-case cost. Implementing this efficiently requires a dynamic programming approach, either through recursion with memoization or a bottom-up dynamic programming table.
```

### Metrics

```
recip_rank: 0.0062
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_309463.txt
> def parallelNelderMead(objFunc,guess,perturb=None,P=1,ftol=0.000001,xtol=0.00000001,maxiter=np.inf,maxeval=np.inf,r_param=1.0,e_param=1.0,c_param=0.5,s_param=0.5,maxcores=None,name=None,resume=False,savefreq=None,verbose=1):<br>    '''<br>    A parallel implementation of the Nelder-Mead minimization algorithm, as<br>    described in Lee and Wiswall.  For long optimization procedures, it can<br>    save progress between iterations and resume later.<br><br>    Parameters<br>    ----------<br>    objFunc : function<br>        The objective function to be minimized. Takes a single 1D array as input.<br>    guess : np.array<br>        Initial starting point for the simplex, representing an input for objFunc.<br>    perturb : np.array<br>        Perturbation vector for the simplex, of the same length as an input to<br>        objFunc.  If perturb[j] is non-zero, a simplex point will be created<br>        that perturbs the j-th element of guess by perturb[j]; if it is zero,<br>        then the j-th parameter of objFunc will not be optimized over.  By<br>        default, guess=None, indicating that all parameters should be optimized,<br>        with an initial perturbation of 0.1guess.<br>    P : int<br>        Degree of parallelization: the number of vertices of the simplex to try<br>        to update on each iteration of the process.<br>    ftol : float<br>        Absolute tolerance of the objective function for convergence.  If suc-<br>        cessive iterations return minimum function values that differ by less<br>        than ftol, the process terminates successfully.<br>    xtol : float<br>        Absolute tolerance of the input values for convergence.  If the maximum<br>        distance between the current minimum point and the worst point in the<br>        simplex is less than xtol, then the process terminates successfully.<br>    maxiter : int<br>        Maximum number of Nelder-Mead iterations; reaching iters=maxiter is<br>        reported as an "unsuccessful" minimization.<br>    maxeval : int<br>        Maximum number of evaluations of objFunc (across all processes); reaching<br>        evals=maxeval is reported as an "unsuccessful" minimization.<br>    r_param: float<br>        Parameter indicating magnitude of the reflection point calculation.<br>    e_param: float<br>        Parameter indicating magnitude of the expansion point calculation.<br>    c_param: float<br>        Parameter indicating magnitude of the contraction point calculation.<br>    s_param: float<br>        Parameter indicating magnitude of the shrink calculation.<br>    maxcores : int<br>        The maximum number of CPU cores that the optimization should use,<br>        regardless of the size of the problem.<br>    name : string<br>        A filename for (optionally) saving the progress of the Nelder-Mead search,<br>        and for resuming a previous search (when resume=True).  Useful for long<br>        searches that could potentially be interrupted by computer down time.<br>    resume : boolean<br>        An indicator for whether the search should resume from earlier progress.<br>        When True, the process will load a progress file named in input name.<br>    savefreq : int<br>        When not None, search progress will be saved to name.txt every savefreq<br>        iterations, to be loaded later with resume=True).<br>    verbose : int<br>        Indicator for the verbosity of the optimization routine.  Higher values<br>        generate more text output; verbose=0 produces no text output.<br><br>    Returns<br>    -------<br>    min_point : np.array<br>        The input that minimizes objFunc, as found by the minimization.<br>    fmin : float<br>        The minimum of objFunc; fmin = objFunc(min_point).<br>    '''<br>    # If this is a resumed search, load the data<br>    if resume:<br>        simplex, fvals, iters, evals = loadNelderMeadData(name)<br>        dim_count = fvals.size - 1<br>        N = dim_count+1 # Number of points in simplex<br>        K = simplex.shape[1] # Total number of parameters<br>    # Otherwise, construct the initial simplex and array of function values<br>    else:<br>        if perturb is None: # Default: perturb each parameter by 10%<br>            perturb = 0.1guess<br>            guess[guess == 0] = 0.1<br>        params_to_opt = np.where(perturb != 0)[0] # Indices of which parameters to optimize<br>        dim_count = params_to_opt.size # Number of parameters to search over<br>        N = dim_count+1 # Number of points in simplex<br>        K = guess.size # Total number of parameters<br>        simplex = np.tile(guess,(N,1))<br>        for j in range(dim_count): # Perturb each parameter to optimize by the specified distance<br>            simplex[j+1,params_to_opt[j]] = simplex[j+1,params_to_opt[j]] + perturb[params_to_opt[j]]<br>        # Initialize a few<br>        fvals = np.zeros(dim_count+1) + np.nan<br>        iters = 0<br>        evals = 0<br><br>    # Create the pool of worker processes<br>    cpu_cores = multiprocessing.cpu_count() # Total number of available CPU cores<br>    cores_to_use = min(cpu_cores,dim_count)<br>    if maxcores is not None: # Cap the number of cores if desired<br>        cores_to_use = min(cores_to_use,maxcores)<br>    parallel = Parallel(n_jobs=cores_to_use)<br><br>    # Begin a new Nelder-Mead search<br>    if not resume:<br>        temp_simplex = list(simplex) # Evaluate the initial simplex<br>        fvals = np.array(parallel(delayed(objFunc)(params) for params in temp_simplex))<br>        evals += N<br>        # Reorder the initial simplex<br>        order = np.argsort(fvals)<br>        fvals = fvals[order]<br>        simplex = simplex[order,:]<br>        fmin = fvals[0]<br>        f_dist = np.abs(fmin - fvals[-1])<br>        x_dist = np.max(np.sqrt(np.sum(simplex2.0 - np.tile(simplex[0,:],(N,1))2.0,axis=1)))<br>        if verbose  0:<br>            print('Evaluated the initial simplex: fmin=' + str(fmin) + ', f_dist=' + str(f_dist) + ', x_dist=' + str(x_dist))<br>    else: # Resume an existing search that was cut short<br>        if verbose  0:<br>            print('Resuming search after ' + str(iters) + ' iterations and ' + str(evals) + ' function evaluations.')<br><br>    # Initialize some inputs for the multithreader<br>    j_list = list(range(N-P,N))<br>    opt_params= [r_param,c_param,e_param]<br><br>    # Run the Nelder-Mead algorithm until a terminal condition is met<br>    go = True<br>    while go:<br>        t_start = clock()<br>        iters += 1<br>        if verbose  0:<br>            print('Beginning iteration #' + str(iters) + ' now.')<br><br>        # Update the P worst points of the simplex<br>        output = parallel(delayed(parallelNelderMeadWorker)(objFunc,simplex,fvals,j,P,opt_params) for j in j_list)<br>        new_subsimplex = np.zeros((P,K)) + np.nan<br>        new_vals = np.zeros(P) + np.nan<br>        new_evals = 0<br>        for i in range(P):<br>            new_subsimplex[i,:] = output[i][0]<br>            new_vals[i] = output[i][1]<br>            new_evals += output[i][2]<br>        evals += new_evals<br><br>        # Check whether any updates actually happened<br>        old_subsimplex = simplex[(N-P):N,:]<br>        if np.max(np.abs(new_subsimplex - old_subsimplex)) == 0:<br>            if verbose  0:<br>                print('Updated the simplex, but must perform a shrink step.')<br>            # If every attempted update was unsuccessful, must shrink the simplex<br>            simplex = s_paramnp.tile(simplex[0,:],(N,1)) + (1.0-s_param)simplex<br>            temp_simplex = list(simplex[1:N,:])<br>            fvals = np.array([fvals[0]] + parallel(delayed(objFunc)(params) for params in temp_simplex))<br>            new_evals += N-1<br>            evals += N-1<br>        else:<br>            if verbose  0:<br>                print('Updated the simplex successfully.')<br>            # Otherwise, update the simplex with the new results<br>            simplex[(N-P):N,:] = new_subsimplex<br>            fvals[(N-P):N] = new_vals<br><br>        # Reorder the simplex from best to worst<br>        order = np.argsort(fvals)<br>        fvals = fvals[order]<br>        simplex = simplex[order,:]<br>        fmin = fvals[0]<br>        f_dist = np.abs(fmin - fvals[-1])<br>        x_dist = np.max(np.sqrt(np.sum(simplex2.0 - np.tile(simplex[0,:],(N,1))2.0,axis=1)))<br>        t_end = clock()<br>        if verbose  0:<br>            t_iter = t_end - t_start<br>            print('Finished iteration #' + str(iters) +' with ' + str(new_evals) + ' evaluations (' + str(evals) + ' cumulative) in ' + str(t_iter) + ' seconds.')<br>            print('Simplex status: fmin=' + str(fmin) + ', f_dist=' + str(f_dist) + ', x_dist=' + str(x_dist))<br><br>        # Check for terminal conditions<br>        if iters = maxiter:<br>            go = False<br>            print('Maximum iterations reached, terminating unsuccessfully.')<br>        if evals = maxeval:<br>            go = False<br>            print('Maximum evaluations reached, terminating unsuccessfully.')<br>        if f_dist < ftol:<br>            go = False<br>            print('Function tolerance reached, terminating successfully.')<br>        if x_dist < xtol:<br>            go = False<br>            print('Parameter tolerance reached, terminating successfully.')<br><br>        # Save the progress of the estimation if desired<br>        if savefreq is not None:<br>            if (iters % savefreq) == 0:<br>                 saveNelderMeadData(name, simplex, fvals, iters, evals)<br>                 if verbose  0:<br>                     print('Saved search progress in ' + name + '.txt')<br><br>    # Return the results<br>    xopt = simplex[0,:]<br>    return xopt, fmin

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_271615.txt
> def solve(self, solver_klass=None):<br>        """ Solves the combined unit decommitment / optimal power flow problem.<br>        """<br>        case = self.case<br>        generators = case.online_generators<br><br>        logger.info("Solving OPF with unit de-commitment [%s]." % case.name)<br><br>        t0 = time()<br><br>        # 1. Begin at stage zero (N = 0), assuming all generators are on-line<br>        # with all limits in place. At most one generator shutdown per stage.<br>        i_stage = 0<br><br>        # Check for sum(p_min)  total load, decommit as necessary.<br>        online = [g for g in generators if not g.is_load]<br>        online_vload = [g for g in generators if g.is_load]<br><br>        # Total dispatchable load capacity.<br>        vload_capacity = sum([g.p_min for g in online_vload])<br>        # Total load capacity.<br>        load_capacity = sum([b.p_demand for b in case.buses]) - vload_capacity<br><br>        # Minimum total online generation capacity.<br>        p_min_tot = sum([g.p_min for g in online])<br><br>        # Shutdown the most expensive units until the minimum generation<br>        # capacity is less than the total load capacity.<br>        while p_min_tot  load_capacity:<br>            i_stage += 1<br>            logger.debug("De-commitment stage %d." % i_stage)<br><br>            # Find generator with the maximum average cost at Pmin.<br>            avg_pmin_cost = [g.total_cost(g.p_min) / g.p_min for g in online]<br>            # Select at random from maximal generators with equal cost.<br>            g_idx, _ = fair_max(avg_pmin_cost)<br>            generator = online[g_idx]<br><br>            logger.info("Shutting down generator [%s] to satisfy all "<br>                        "p_min limits." % generator.name)<br><br>            # Shut down most expensive unit.<br>            generator.online = False<br><br>            # Update minimum generation capacity for while loop.<br>            online = [g for g in case.online_generators if not g.is_load]<br>            p_min_tot = sum([g.p_min for g in online])<br><br>        # 2. Solve a normal OPF and save the solution as the current best.<br>        solution = super(UDOPF, self).solve(solver_klass)<br><br>        logger.debug("Initial system cost: $%.3f" % solution["f"])<br><br>        if not solution["converged"] == True:<br>            logger.error("Non-convergent UDOPF [%s]." %<br>                         solution["output"]["message"])<br>            return solution<br><br>        # 3. Go to the next stage, N = N + 1. Using the best solution from the<br>        # previous stage as the base case for this stage, ...<br><br>        # Best case so far. A list of the on-line status of all generators.<br>        overall_online = [g.online for g in case.generators]<br>        # The objective function value is the total system cost.<br>        overall_cost = solution["f"]<br><br>        # Best case for this stage.<br>        stage_online = overall_online<br>        stage_cost = overall_cost<br><br>        # Shutdown at most one generator per stage.<br>        while True:<br>            # 4. Form a candidate list of generators with minimum<br>            # generation limits binding.<br><br>            # Activate generators according to the stage best.<br>            for i, generator in enumerate(case.generators):<br>                generator.online = stage_online[i]<br><br>            # Get candidates for shutdown. Lagrangian multipliers are often<br>            # very small so we round to four decimal places.<br>            candidates = [g for g in case.online_generators if \<br>                          (round(g.mu_pmin, 4)  0.0) and (g.p_min  0.0)]<br><br>            if len(candidates) == 0:<br>                break<br><br>            # Assume no improvement during this stage.<br>            done = True<br><br>            i_stage += 1<br>            logger.debug("De-commitment stage %d." % i_stage)<br><br>            for candidate in candidates:<br>                # 5. For each generator on the candidate list, solve an OPF to<br>                # find the total system cost with the generator shut down.<br><br>                # Activate generators according to the stage best.<br>                for i, generator in enumerate(case.generators):<br>                    generator.online = stage_online[i]<br><br>                # Shutdown candidate generator.<br>                candidate.online = False<br><br>                logger.debug("Solving OPF with generator '%s' shutdown." %<br>                    candidate.name)<br><br>                # Run OPF.<br>                solution = super(UDOPF, self).solve(solver_klass)<br><br>                # Compare total system costs for improvement.<br>                if solution["converged"] == True \<br>                    and (solution["f"] < overall_cost):<br>                    logger.debug("System cost improvement: $%.3f ($%.3f)" %<br>                                 (stage_cost - solution["f"], solution["f"]))<br>                    # 6. Replace the current best solution with this one if<br>                    # it has a lower cost.<br>                    overall_online = [g.online for g in case.generators]<br>                    overall_cost = solution["f"]<br>                    best_candidate = candidate<br>                    # Check for further decommitment.<br>                    done = False<br>                else:<br>                    logger.debug("Candidate OPF failed [%s]." %<br>                                 solution["output"]["message"])<br><br>                # Reactivate the candidate before deactivating the next.<br>#                candidate.online = True<br><br>            if done:<br>                # Decommits at this stage did not help.<br>                break<br>            else:<br>                # 7. If any of the candidate solutions produced an improvement,<br>                # return to step 3.<br><br>                # Shutting something else down helps, so let's keep going.<br>                logger.info("Shutting down generator '%s'.",<br>                            best_candidate.name)<br><br>                stage_online = overall_online<br>                stage_cost = overall_cost<br><br>        # 8. Use the best overall solution as the final solution.<br>        for i, generator in enumerate(case.generators):<br>            generator.online = overall_online[i]<br><br>        # One final solve using the best case to ensure all results are<br>        # up-to-date.<br>        solution = super(UDOPF, self).solve(solver_klass)<br><br>        logger.debug("UDOPF system cost: $%.3f" % solution["f"])<br><br>        # Compute elapsed time and log it.<br>        elapsed = time() - t0<br><br>        plural = "" if i_stage == 1 else "s"<br>        logger.info("Unit decommitment OPF solved in %.3fs (%d decommitment "<br>                    "stage%s)." % (elapsed, i_stage, plural))<br><br>        return solution

### DOC[3] (IRRELEVANT) leetcode/leetcode_2547.txt
> def minCost(nums: List[int], k: int) - int:<br>    """You are given an integer array `nums` and an integer `k`.<br><br>Split the array into some number of non-empty subarrays. The cost of a split is the sum of the importance value of each subarray in the split.<br><br>Let `trimmed(subarray)` be the version of the subarray where all numbers which appear only once are removed.<br><br>   For example, `trimmed([3,1,2,4,3,4]) = [3,4,3,4].`<br><br>The importance value of a subarray is `k + trimmed(subarray).length`.<br><br>   For example, if a subarray is `[1,2,3,3,3,4,4]`, then trimmed(`[1,2,3,3,3,4,4]) = [3,3,3,4,4].`The importance value of this subarray will be `k + 5`.<br><br>Return _the minimum possible cost of a split of_ `nums`.<br><br>A subarray is a contiguous non-empty sequence of elements within an array.<br><br>Example 1:<br><br>Input: nums = \[1,2,1,2,1,3,3\], k = 2<br>Output: 8<br>Explanation: We split nums to have two subarrays: \[1,2\], \[1,2,1,3,3\].<br>The importance value of \[1,2\] is 2 + (0) = 2.<br>The importance value of \[1,2,1,3,3\] is 2 + (2 + 2) = 6.<br>The cost of the split is 2 + 6 = 8. It can be shown that this is the minimum possible cost among all the possible splits.<br><br>Example 2:<br><br>Input: nums = \[1,2,1,2,1\], k = 2<br>Output: 6<br>Explanation: We split nums to have two subarrays: \[1,2\], \[1,2,1\].<br>The importance value of \[1,2\] is 2 + (0) = 2.<br>The importance value of \[1,2,1\] is 2 + (2) = 4.<br>The cost of the split is 2 + 4 = 6. It can be shown that this is the minimum possible cost among all the possible splits.<br><br>Example 3:<br><br>Input: nums = \[1,2,1,2,1\], k = 5<br>Output: 10<br>Explanation: We split nums to have one subarray: \[1,2,1,2,1\].<br>The importance value of \[1,2,1,2,1\] is 5 + (3 + 2) = 10.<br>The cost of the split is 10. It can be shown that this is the minimum possible cost among all the possible splits.<br><br>Constraints:<br><br>   `1 <= nums.length <= 1000`<br>   `0 <= nums[i] < nums.length`<br>   `1 <= k <= 109`<br><br>.spoilerbutton {display:block; border:dashed; padding: 0px 0px; margin:10px 0px; font-size:150%; font-weight: bold; color:#000000; background-color:cyan; outline:0;  } .spoiler {overflow:hidden;} .spoiler  div {-webkit-transition: all 0s ease;-moz-transition: margin 0s ease;-o-transition: all 0s ease;transition: margin 0s ease;} .spoilerbutton\[value="Show Message"\] + .spoiler  div {margin-top:-500%;} .spoilerbutton\[value="Hide Message"\] + .spoiler {padding:5px;}"""<br><br>    n = len(nums)<br>    count = [0]  1001<br>    dp = [0]  (n + 1)<br><br>    i = 0<br>    for j in range(n):<br>        count[nums[j]] += 1<br>        <br>        if count[nums[j]] == 2:<br>            dp[j + 1] = dp[i] + k + (j - i + 1)<br>        elif count[nums[j]]  2:<br>            dp[j + 1] = min(dp[j], dp[j - 1] + 1)<br>        else:<br>            dp[j + 1] = dp[j] + 1<br><br>        while i <= j and count[nums[i]] = 2:<br>            count[nums[i]] -= 1<br>            i += 1<br><br>    return dp[n] + n  k

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_321793.txt
> def evolution_strength_of_connection(A, B=None, epsilon=4.0, k=2,<br>                                     proj_type="l2", block_flag=False,<br>                                     symmetrize_measure=True):<br>    """Evolution Strength Measure.<br><br>    Construct strength of connection matrix using an Evolution-based measure<br><br>    Parameters<br>    ----------<br>    A : csr_matrix, bsr_matrix<br>        Sparse NxN matrix<br>    B : string, array<br>        If B=None, then the near nullspace vector used is all ones.  If B is<br>        an (NxK) array, then B is taken to be the near nullspace vectors.<br>    epsilon : scalar<br>        Drop tolerance<br>    k : integer<br>        ODE num time steps, step size is assumed to be 1/rho(DinvA)<br>    proj_type : {'l2','D_A'}<br>        Define norm for constrained min prob, i.e. define projection<br>    block_flag : boolean<br>        If True, use a block D inverse as preconditioner for A during<br>        weighted-Jacobi<br><br>    Returns<br>    -------<br>    Atilde : csr_matrix<br>        Sparse matrix of strength values<br><br>    See [2008OlScTu]_ for more details.<br><br>    References<br>    ----------<br>    .. [2008OlScTu] Olson, L. N., Schroder, J., Tuminaro, R. S.,<br>       "A New Perspective on Strength Measures in Algebraic Multigrid",<br>       submitted, June, 2008.<br><br>    Examples<br>    --------<br>     import numpy as np<br>     from pyamg.gallery import stencil_grid<br>     from pyamg.strength import evolution_strength_of_connection<br>     n=3<br>     stencil =  np.array([[-1.0,-1.0,-1.0],<br>    ...                        [-1.0, 8.0,-1.0],<br>    ...                        [-1.0,-1.0,-1.0]])<br>     A = stencil_grid(stencil, (n,n), format='csr')<br>     S = evolution_strength_of_connection(A,  np.ones((A.shape[0],1)))<br><br>    """<br>    # local imports for evolution_strength_of_connection<br>    from pyamg.util.utils import scale_rows, get_block_diag, scale_columns<br>    from pyamg.util.linalg import approximate_spectral_radius<br><br>    # ====================================================================<br>    # Check inputs<br>    if epsilon < 1.0:<br>        raise ValueError("expected epsilon  1.0")<br>    if k <= 0:<br>        raise ValueError("number of time steps must be  0")<br>    if proj_type not in ['l2', 'D_A']:<br>        raise ValueError("proj_type must be 'l2' or 'D_A'")<br>    if (not sparse.isspmatrix_csr(A)) and (not sparse.isspmatrix_bsr(A)):<br>        raise TypeError("expected csr_matrix or bsr_matrix")<br><br>    # ====================================================================<br>    # Format A and B correctly.<br>    # B must be in mat format, this isn't a deep copy<br>    if B is None:<br>        Bmat = np.mat(np.ones((A.shape[0], 1), dtype=A.dtype))<br>    else:<br>        Bmat = np.mat(B)<br><br>    # Pre-process A.  We need A in CSR, to be devoid of explicit 0's and have<br>    # sorted indices<br>    if (not sparse.isspmatrix_csr(A)):<br>        csrflag = False<br>        numPDEs = A.blocksize[0]<br>        D = A.diagonal()<br>        # Calculate DinvA<br>        if block_flag:<br>            Dinv = get_block_diag(A, blocksize=numPDEs, inv_flag=True)<br>            Dinv = sparse.bsr_matrix((Dinv, np.arange(Dinv.shape[0]),<br>                                      np.arange(Dinv.shape[0] + 1)),<br>                                     shape=A.shape)<br>            Dinv_A = (Dinv  A).tocsr()<br>        else:<br>            Dinv = np.zeros_like(D)<br>            mask = (D != 0.0)<br>            Dinv[mask] = 1.0 / D[mask]<br>            Dinv[D == 0] = 1.0<br>            Dinv_A = scale_rows(A, Dinv, copy=True)<br>        A = A.tocsr()<br>    else:<br>        csrflag = True<br>        numPDEs = 1<br>        D = A.diagonal()<br>        Dinv = np.zeros_like(D)<br>        mask = (D != 0.0)<br>        Dinv[mask] = 1.0 / D[mask]<br>        Dinv[D == 0] = 1.0<br>        Dinv_A = scale_rows(A, Dinv, copy=True)<br><br>    A.eliminate_zeros()<br>    A.sort_indices()<br><br>    # Handle preliminaries for the algorithm<br>    dimen = A.shape[1]<br>    NullDim = Bmat.shape[1]<br><br>    # Get spectral radius of DinvA, this will be used to scale the time step<br>    # size for the ODE<br>    rho_DinvA = approximate_spectral_radius(Dinv_A)<br><br>    # Calculate D_A for later use in the minimization problem<br>    if proj_type == "D_A":<br>        D_A = sparse.spdiags([D], [0], dimen, dimen, format='csr')<br>    else:<br>        D_A = sparse.eye(dimen, dimen, format="csr", dtype=A.dtype)<br><br>    # Calculate (I - delta_t Dinv A)^k<br>    #      In order to later access columns, we calculate the transpose in<br>    #      CSR format so that columns will be accessed efficiently<br>    # Calculate the number of time steps that can be done by squaring, and<br>    # the number of time steps that must be done incrementally<br>    nsquare = int(np.log2(k))<br>    ninc = k - 2nsquare<br><br>    # Calculate one time step<br>    Id = sparse.eye(dimen, dimen, format="csr", dtype=A.dtype)<br>    Atilde = (Id - (1.0 / rho_DinvA)  Dinv_A)<br>    Atilde = Atilde.T.tocsr()<br><br>    # Construct a sparsity mask for Atilde that will restrict Atilde^T to the<br>    # nonzero pattern of A, with the added constraint that row i of Atilde^T<br>    # retains only the nonzeros that are also in the same PDE as i.<br>    mask = A.copy()<br><br>    # Restrict to same PDE<br>    if numPDEs  1:<br>        row_length = np.diff(mask.indptr)<br>        my_pde = np.mod(np.arange(dimen), numPDEs)<br>        my_pde = np.repeat(my_pde, row_length)<br>        mask.data[np.mod(mask.indices, numPDEs) != my_pde] = 0.0<br>        del row_length, my_pde<br>        mask.eliminate_zeros()<br><br>    # If the total number of time steps is a power of two, then there is<br>    # a very efficient computational short-cut.  Otherwise, we support<br>    # other numbers of time steps, through an inefficient algorithm.<br>    if ninc  0:<br>        warn("The most efficient time stepping for the Evolution Strength\<br>             Method is done in powers of two.\nYou have chosen " + str(k) +<br>             " time steps.")<br><br>        # Calculate (Atilde^nsquare)^T = (Atilde^T)^nsquare<br>        for i in range(nsquare):<br>            Atilde = Atilde  Atilde<br><br>        JacobiStep = (Id - (1.0 / rho_DinvA)  Dinv_A).T.tocsr()<br>        for i in range(ninc):<br>            Atilde = Atilde  JacobiStep<br>        del JacobiStep<br><br>        # Apply mask to Atilde, zeros in mask have already been eliminated at<br>        # start of routine.<br>        mask.data[:] = 1.0<br>        Atilde = Atilde.multiply(mask)<br>        Atilde.eliminate_zeros()<br>        Atilde.sort_indices()<br><br>    elif nsquare == 0:<br>        if numPDEs  1:<br>            # Apply mask to Atilde, zeros in mask have already been eliminated<br>            # at start of routine.<br>            mask.data[:] = 1.0<br>            Atilde = Atilde.multiply(mask)<br>            Atilde.eliminate_zeros()<br>            Atilde.sort_indices()<br><br>    else:<br>        # Use computational short-cut for case (ninc == 0) and (nsquare  0)<br>        # Calculate Atilde^k only at the sparsity pattern of mask.<br>        for i in range(nsquare - 1):<br>            Atilde = Atilde  Atilde<br><br>        # Call incomplete mat-mat mult<br>        AtildeCSC = Atilde.tocsc()<br>        AtildeCSC.sort_indices()<br>        mask.sort_indices()<br>        Atilde.sort_indices()<br>        amg_core.incomplete_mat_mult_csr(Atilde.indptr, Atilde.indices,<br>                                         Atilde.data, AtildeCSC.indptr,<br>                                         AtildeCSC.indices, AtildeCSC.data,<br>                                         mask.indptr, mask.indices, mask.data,<br>                                         dimen)<br><br>        del AtildeCSC, Atilde<br>        Atilde = mask<br>        Atilde.eliminate_zeros()<br>        Atilde.sort_indices()<br><br>    del Dinv, Dinv_A, mask<br><br>    # Calculate strength based on constrained min problem of<br>    # min( z - Bx ), such that<br>    # (Bx)|_i = z|_i, i.e. they are equal at point i<br>    # z = (I - (t/k) Dinv A)^k delta_i<br>    #<br>    # Strength is defined as the relative point-wise approx. error between<br>    # Bx and z.  We don't use the full z in this problem, only that part of<br>    # z that is in the sparsity pattern of A.<br>    #<br>    # Can use either the D-norm, and inner product, or l2-norm and inner-prod<br>    # to solve the constrained min problem.  Using D gives scale invariance.<br>    #<br>    # This is a quadratic minimization problem with a linear constraint, so<br>    # we can build a linear system and solve it to find the critical point,<br>    # i.e. minimum.<br>    #<br>    # We exploit a known shortcut for the case of NullDim = 1.  The shortcut is<br>    # mathematically equivalent to the longer constrained min. problem<br><br>    if NullDim == 1:<br>        # Use shortcut to solve constrained min problem if B is only a vector<br>        # Strength(i,j) = | 1 - (z(i)/b(j))/(z(j)/b(i)) |<br>        # These ratios can be calculated by diagonal row and column scalings<br><br>        # Create necessary vectors for scaling Atilde<br>        #   Its not clear what to do where B == 0.  This is an<br>        #   an easy programming solution, that may make sense.<br>        Bmat_forscaling = np.ravel(Bmat)<br>        Bmat_forscaling[Bmat_forscaling == 0] = 1.0<br>        DAtilde = Atilde.diagonal()<br>        DAtildeDivB = np.ravel(DAtilde) / Bmat_forscaling<br><br>        # Calculate best approximation, z_tilde, in span(B)<br>        #   Importantly, scale_rows and scale_columns leave zero entries<br>        #   in the matrix.  For previous implementations this was useful<br>        #   because we assume data and Atilde.data are the same length below<br>        data = Atilde.data.copy()<br>        Atilde.data[:] = 1.0<br>        Atilde = scale_rows(Atilde, DAtildeDivB)<br>        Atilde = scale_columns(Atilde, np.ravel(Bmat_forscaling))<br><br>        # If angle in the complex plane between z and z_tilde is<br>        # greater than 90 degrees, then weak.  We can just look at the<br>        # dot product to determine if angle is greater than 90 degrees.<br>        angle = np.multiply(np.real(Atilde.data), np.real(data)) +\<br>            np.multiply(np.imag(Atilde.data), np.imag(data))<br>        angle = angle < 0.0<br>        angle = np.array(angle, dtype=bool)<br><br>        # Calculate Approximation ratio<br>        Atilde.data = Atilde.data / data<br><br>        # If approximation ratio is less than tol, then weak connection<br>        weak_ratio = (np.abs(Atilde.data) < 1e-4)<br><br>        # Calculate Approximation error<br>        Atilde.data = abs(1.0 - Atilde.data)<br><br>        # Set small ratios and large angles to weak<br>        Atilde.data[weak_ratio] = 0.0<br>        Atilde.data[angle] = 0.0<br><br>        # Set near perfect connections to 1e-4<br>        Atilde.eliminate_zeros()<br>        Atilde.data[Atilde.data < np.sqrt(np.finfo(float).eps)] = 1e-4<br><br>        del data, weak_ratio, angle<br><br>    else:<br>        # For use in computing local B_i^HB, precompute the element-wise<br>        # multiply of each column of B with each other column.  We also scale<br>        # by 2.0 to account for BDB's eventual use in a constrained<br>        # minimization problem<br>        BDBCols = int(np.sum(np.arange(NullDim + 1)))<br>        BDB = np.zeros((dimen, BDBCols), dtype=A.dtype)<br>        counter = 0<br>        for i in range(NullDim):<br>            for j in range(i, NullDim):<br>                BDB[:, counter] = 2.0 \<br>                    (np.conjugate(np.ravel(np.asarray(B[:, i]))) <br>                        np.ravel(np.asarray(D_A  B[:, j])))<br>                counter = counter + 1<br><br>        # Choose tolerance for dropping "numerically zero" values later<br>        t = Atilde.dtype.char<br>        eps = np.finfo(np.float).eps<br>        feps = np.finfo(np.single).eps<br>        geps = np.finfo(np.longfloat).eps<br>        _array_precision = {'f': 0, 'd': 1, 'g': 2, 'F': 0, 'D': 1, 'G': 2}<br>        tol = {0: feps  1e3, 1: eps  1e6, 2: geps  1e6}[_array_precision[t]]<br><br>        # Use constrained min problem to define strength<br>        amg_core.evolution_strength_helper(Atilde.data,<br>                                           Atilde.indptr,<br>                                           Atilde.indices,<br>                                           Atilde.shape[0],<br>                                           np.ravel(np.asarray(B)),<br>                                           np.ravel(np.asarray(<br>                                               (D_A  np.conjugate(B)).T)),<br>                                           np.ravel(np.asarray(BDB)),<br>                                           BDBCols, NullDim, tol)<br><br>        Atilde.eliminate_zeros()<br><br>    # All of the strength values are real by this point, so ditch the complex<br>    # part<br>    Atilde.data = np.array(np.real(Atilde.data), dtype=float)<br><br>    # Apply drop tolerance<br>    if epsilon != np.inf:<br>        amg_core.apply_distance_filter(dimen, epsilon, Atilde.indptr,<br>                                       Atilde.indices, Atilde.data)<br>        Atilde.eliminate_zeros()<br><br>    # Symmetrize<br>    if symmetrize_measure:<br>        Atilde = 0.5  (Atilde + Atilde.T)<br><br>    # Set diagonal to 1.0, as each point is strongly connected to itself.<br>    Id = sparse.eye(dimen, dimen, format="csr")<br>    Id.data -= Atilde.diagonal()<br>    Atilde = Atilde + Id<br><br>    # If converted BSR to CSR, convert back and return amalgamated matrix,<br>    #   i.e. the sparsity structure of the blocks of Atilde<br>    if not csrflag:<br>        Atilde = Atilde.tobsr(blocksize=(numPDEs, numPDEs))<br><br>        n_blocks = Atilde.indices.shape[0]<br>        blocksize = Atilde.blocksize[0]  Atilde.blocksize[1]<br>        CSRdata = np.zeros((n_blocks,))<br>        amg_core.min_blocks(n_blocks, blocksize,<br>                            np.ravel(np.asarray(Atilde.data)), CSRdata)<br>        # Atilde = sparse.csr_matrix((data, row, col), shape=(,))<br>        Atilde = sparse.csr_matrix((CSRdata, Atilde.indices, Atilde.indptr),<br>                                   shape=(int(Atilde.shape[0] / numPDEs),<br>                                          int(Atilde.shape[1] / numPDEs)))<br><br>    # Standardized strength values require small values be weak and large<br>    # values be strong.  So, we invert the algebraic distances computed here<br>    Atilde.data = 1.0 / Atilde.data<br><br>    # Scale C by the largest magnitude entry in each row<br>    Atilde = scale_rows_by_largest_entry(Atilde)<br><br>    return Atilde

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_287110.txt
> def mincost_diameter_augment(graph, max_cost, candidates=None, weight=None, cost=None):<br>    """<br>    PROBLEM: Bounded Cost Minimum Diameter Edge Addition (BCMD)<br><br>    Args:<br>        graph (nx.Graph): input graph<br>        max_cost (float): maximum weighted diamter of the graph<br>        weight (str): key of the edge weight attribute<br>        cost (str): key of the edge cost attribute<br>        candidates (list): set of non-edges, optional, defaults<br>            to the complement of the graph<br><br>    Returns:<br>        None: if no solution exists<br>        list: minimum cost edges if solution exists<br><br>    Notes:<br>        We are given a graph G = (V, E) with an edge weight function w, an edge<br>        cost function c, an a maximum cost B.<br><br>        The goal is to find a set of candidate non-edges F.<br><br>        Let x[e] in {0, 1} denote if a non-edge e is excluded or included.<br><br>        minimize sum(c(e)  x[e] for e in F)<br>        such that<br>        weighted_diamter(graph.union({e for e in F if x[e]})) <= B<br><br>    References:<br>        https://www.cse.unsw.edu.au/~sergeg/papers/FratiGGM13isaac.pdf<br>        http://www.cis.upenn.edu/~sanjeev/papers/diameter.pdf<br>        http://dl.acm.org/citation.cfm?id=2953882<br><br>    Notes:<br>        There is a 4-Approximation of the BCMD problem<br>        Running time is O((3  B  B  3 + n + log(B  n))  B  n  2)<br><br>        This algorithm usexs a clustering approach to find a set C, of B + 1<br>        cluster centers.  Then we create a minimum height rooted tree, T = (U<br>        \subseteq V, D) so that C \subseteq U.  This tree T approximates an<br>        optimal B-augmentation.<br><br>    Example:<br>         # ENABLE_DOCTEST<br>         from utool.util_graph import   # NOQA<br>         import utool as ut<br>         graph = nx.Graph()<br>         if nx.__version__.startswith('1'):<br>             nx.add_path = nx.Graph.add_path<br>         nx.add_path(graph, range(6))<br>         #cost_func   = lambda e: e[0] + e[1]<br>         cost_func   = lambda e: 1<br>         weight_func = lambda e: (e[0]) / e[1]<br>         comp_graph = nx.complement(graph)<br>         nx.set_edge_attributes(graph, name='cost', values={e: cost_func(e) for e in graph.edges()})<br>         nx.set_edge_attributes(graph, name='weight', values={e: weight_func(e) for e in graph.edges()})<br>         nx.set_edge_attributes(comp_graph, name='cost', values={e: cost_func(e) for e in comp_graph.edges()})<br>         nx.set_edge_attributes(comp_graph, name='weight', values={e: weight_func(e) for e in comp_graph.edges()})<br>         candidates = list(comp_graph.edges(data=True))<br>         max_cost = 2<br>         cost = 'cost'<br>         weight = 'weight'<br>         best_edges = mincost_diameter_augment(graph, max_cost, candidates, weight, cost)<br>         print('best_edges = %r' % (best_edges,))<br>         soln_edges = greedy_mincost_diameter_augment(graph, max_cost, candidates, weight, cost)<br>         print('soln_edges = %r' % (soln_edges,))<br>    """<br>    import utool as ut<br>    import operator as op<br><br>    if candidates is None:<br>        candidates = list(graph.complement().edges(data=True))<br><br>    def augment_add(graph, edges):<br>        aug_graph = graph.copy()<br>        aug_graph.add_edges_from(edges)<br>        return aug_graph<br><br>    def solution_energy(chosen_edges):<br>        if weight is None:<br>            return len(chosen_edges)<br>        else:<br>            return sum(d[weight] for (u, v, d) in chosen_edges)<br><br>    variable_basis = [(0, 1) for _ in candidates]<br>    best_energy = np.inf<br>    best_soln = None<br><br>    soln_generator = ut.product(variable_basis)<br>    length = reduce(op.mul, map(len, variable_basis), 1)<br>    if length  3000:<br>        # Let the user know that it might take some time to find a solution<br>        soln_generator = ut.ProgIter(soln_generator, label='BruteForce BCMD',<br>                                     length=length)<br>    # Brute force solution<br>    for x in soln_generator:<br>        chosen_edges = ut.compress(candidates, x)<br>        aug_graph = augment_add(graph, chosen_edges)<br>        total_cost = weighted_diamter(aug_graph, weight=cost)<br>        energy = solution_energy(chosen_edges)<br>        if total_cost <= max_cost:<br>            if energy < best_energy:<br>                best_energy = energy<br>                best_soln = x<br><br>    best_edges = ut.compress(candidates, best_soln)<br>    return best_edges


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_374.txt
> def guessNumber(n):<br>    """We are playing the Guess Game. The game is as follows:<br><br>I pick a number from `1` to `n`. You have to guess which number I picked.<br><br>Every time you guess wrong, I will tell you whether the number I picked is higher or lower than your guess.<br><br>You call a pre-defined API `int guess(int num)`, which returns three possible results:<br><br>   `-1`: Your guess is higher than the number I picked (i.e. `num  pick`).<br>   `1`: Your guess is lower than the number I picked (i.e. `num < pick`).<br>   `0`: your guess is equal to the number I picked (i.e. `num == pick`).<br><br>Return _the number that I picked_.<br><br>Example 1:<br><br>Input: n = 10, pick = 6<br>Output: 6<br><br>Example 2:<br><br>Input: n = 1, pick = 1<br>Output: 1<br><br>Example 3:<br><br>Input: n = 2, pick = 1<br>Output: 1<br><br>Constraints:<br><br>   `1 <= n <= 231 - 1`<br>   `1 <= pick <= n`"""<br><br>    low, high = 1, n<br>    while low <= high:<br>        mid = low + (high - low) // 2<br>        result = guess(mid)<br>        if result == 0:<br>            return mid<br>        elif result < 0:<br>            high = mid - 1<br>        else:<br>            low = mid + 1<br>    return -1

### GROUND TRUTH 1, ranked not in top 100, leetcode/leetcode_464.txt
> def canIWin(maxChoosableInteger, desiredTotal):<br>    """In the "100 game " two players take turns adding, to a running total, any integer from `1` to `10`. The player who first causes the running total to reach or exceed 100 wins.<br><br>What if we change the game so that players cannot re-use integers?<br><br>For example, two players might take turns drawing from a common pool of numbers from 1 to 15 without replacement until they reach a total = 100.<br><br>Given two integers `maxChoosableInteger` and `desiredTotal`, return `true` if the first player to move can force a win, otherwise, return `false`. Assume both players play optimally.<br><br>Example 1:<br><br>Input: maxChoosableInteger = 10, desiredTotal = 11<br>Output: false<br>Explanation:<br>No matter which integer the first player choose, the first player will lose.<br>The first player can choose an integer from 1 up to 10.<br>If the first player choose 1, the second player can only choose integers from 2 up to 10.<br>The second player will win by choosing 10 and get a total = 11, which is = desiredTotal.<br>Same with other integers chosen by the first player, the second player will always win.<br><br>Example 2:<br><br>Input: maxChoosableInteger = 10, desiredTotal = 0<br>Output: true<br><br>Example 3:<br><br>Input: maxChoosableInteger = 10, desiredTotal = 1<br>Output: true<br><br>Constraints:<br><br>   `1 <= maxChoosableInteger <= 20`<br>   `0 <= desiredTotal <= 300`"""<br><br>    if maxChoosableInteger = desiredTotal: return True<br>    if (maxChoosableInteger  (maxChoosableInteger + 1)) // 2 < desiredTotal: return False<br><br>    memo = {}<br>        <br>    def canWin(state):<br>        if state in memo:<br>            return memo[state]<br>        for i in range(1, maxChoosableInteger + 1):<br>            bitCheck = 1 << i<br>            if (state & bitCheck) == 0:<br>                if not canWin(state | bitCheck):<br>                    memo[state] = True<br>                    return True<br>    <br>        memo[state] = False<br>        return False<br><br>    return canWin(0)

### GROUND TRUTH 2, ranked not in top 100, leetcode/leetcode_658.txt
> from typing import List<br>    """Given a sorted integer array `arr`, two integers `k` and `x`, return the `k` closest integers to `x` in the array. The result should also be sorted in ascending order.<br><br>An integer `a` is closer to `x` than an integer `b` if:<br><br>   `|a - x| < |b - x|`, or<br>   `|a - x| == |b - x|` and `a < b`<br><br>Example 1:<br><br>Input: arr = \[1,2,3,4,5\], k = 4, x = 3<br>Output: \[1,2,3,4\]<br><br>Example 2:<br><br>Input: arr = \[1,2,3,4,5\], k = 4, x = -1<br>Output: \[1,2,3,4\]<br><br>Constraints:<br><br>   `1 <= k <= arr.length`<br>   `1 <= arr.length <= 104`<br>   `arr` is sorted in ascending order.<br>   `-104 <= arr[i], x <= 104`"""<br><br><br>def findClosestElements(arr: List[int], k: int, x: int) - List[int]:<br>    left = 0<br>    right = len(arr) - k<br><br>    while left < right:<br>        mid = left + (right - left) // 2<br>        if x - arr[mid]  arr[mid + k] - x:<br>            left = mid + 1<br>        else:<br>            right = mid<br><br>    return arr[left:left + k]
