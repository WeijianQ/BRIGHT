# Query `75`

## Original Question

*You have a bomb to defuse, and your time is running out! Your informer will provide you with a circular array `code` of length of `n` and a key `k`.

To decrypt the code, you must replace every number. All the numbers are replaced simultaneously.

   If `k > 0`, replace the `ith` number with the sum of the next `k` numbers.
   If `k < 0`, replace the `ith` number with the sum of the previous `k` numbers.
   If `k == 0`, replace the `ith` number with `0`.

As `code` is circular, the next element of `code[n-1]` is `code[0]`, and the previous element of `code[0]` is `code[n-1]`.

Given the circular array `code` and an integer key `k`, return _the decrypted code to defuse the bomb_!

Example 1:

Input: code = \[5,7,1,4\], k = 3
Output: \[12,10,16,13\]
Explanation: Each number is replaced by the sum of the next 3 numbers. The decrypted code is \[7+1+4, 1+4+5, 4+5+7, 5+7+1\]. Notice that the numbers wrap around.

Example 2:

Input: code = \[1,2,3,4\], k = 0
Output: \[0,0,0,0\]
Explanation: When k is zero, the numbers are replaced by 0. 

Example 3:

Input: code = \[2,4,9,3\], k = -2
Output: \[12,5,6,13\]
Explanation: The decrypted code is \[3+9, 2+3, 4+2, 9+4\]. Notice that the numbers wrap around again. If k is negative, the sum is of the previous numbers.

Constraints:

   `n == code.length`
   `1 <= n <= 100`
   `1 <= code[i] <= 100`
   `-(n - 1) <= k <= n - 1`*

## gpt4 Reasoning

```markdown
The essential problem here is to decrypt a circular array based on a given key, `k`, which determines the direction (next for positive `k`, previous for negative `k`) and the range (the absolute value of `k`) for summing the elements to replace each number in the array. The operation must be applied to all elements simultaneously, and special handling is required when `k` is zero, which simplifies the problem as all elements will be replaced with zero. 

To solve this, we can follow these steps:

1. **Understand the Circular Nature of the Array**: Since the array is circular, we need to ensure that our algorithm wraps around the array correctly. This means if we're at the last element and need to sum the next `k` elements, we start again from the beginning of the array. Similarly, if we're at the beginning and need to sum the previous `k` elements, we wrap around to the end of the array.

2. **Handling Different Values of `k`**:
   - If `k > 0`, we sum the next `k` numbers for each element in the array. Since the array is circular, if we reach the end, we continue counting from the start.
   - If `k < 0`, we sum the previous `k` numbers for each element. For circular wrapping, if we're at the start of the array, we continue from the end to get the previous numbers.
   - If `k == 0`, we simply replace every element in the array with `0`.

3. **Implementing the Solution**:
   - Initialize a new array of the same length as `code` to store the decrypted code. This ensures that all elements are replaced simultaneously.
   - For each element in `code`, calculate the sum based on the value of `k`:
     - Use modulo operations to correctly wrap around the array's start and end points.
     - For positive `k`, iterate through the next `k` elements, summing them up. Make sure to wrap around if necessary.
     - For negative `k`, iterate through the previous `k` elements (in reverse), summing them up. Again, ensure wrapping around the array correctly.
   - For `k == 0`, set all elements in the new array to `0`.
   - Return the new array as the decrypted code.

4. **Optimization Considerations**:
   - To avoid repeatedly calculating sums for elements that are within the range of each other, we can utilize a sliding window approach for positive and negative `k` values, adjusting the sum as we move the window across the array.
   - For very large arrays or extreme values of `k`, ensuring efficient wrapping and sum calculation is crucial to avoid performance bottlenecks.

5. **Code Implementation** (Pseudocode):
```python
def decrypt_code(code, k):
    n = len(code)
    decrypted = [0] * n  # Initialize decrypted array

    if k == 0:
        return decrypted  # All elements are 0

    for i in range(n):
        sum = 0
        if k > 0:
            # Sum next k elements
            for j in range(1, k + 1):
                sum += code[(i + j) % n]
        else:
            # Sum previous k elements
            for j in range(1, abs(k) + 1):
                sum += code[(i - j + n) % n]  # +n ensures positive index
        decrypted[i] = sum

    return decrypted
```

This approach ensures that we can decrypt the code based on the rules provided, taking into account the circular nature of the array and the specific requirements based on the value of `k`.
```

### Metrics

```
recip_rank: 0.1250
P_1: 0.0000
P_5: 0.0000
P_10: 0.1000
P_25: 0.0400
P_50: 0.0200
P_100: 0.0100
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.3333
recall_25: 0.3333
recall_50: 0.3333
recall_100: 0.3333
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.1480
ndcg_cut_25: 0.1480
ndcg_cut_50: 0.1480
ndcg_cut_100: 0.1480
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0417
map_cut_25: 0.0417
map_cut_50: 0.0417
map_cut_100: 0.0417
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_226306.txt
> def _berlekamp_massey_fast(self, s, k=None, erasures_loc=None, erasures_eval=None, erasures_count=0):<br>        '''Faster implementation of errata (errors-and-erasures) Berlekamp-Massey.<br>        Returns the error locator polynomial (sigma) and the<br>        error evaluator polynomial (omega) with a faster implementation.<br>        '''<br>        n = self.n<br>        if not k: k = self.k<br><br>        # Initialize, depending on if we include erasures or not:<br>        if erasures_loc:<br>            sigma = Polynomial(erasures_loc.coefficients) # copy erasures_loc by creating a new Polynomial, so that we initialize the errata locator polynomial with the erasures locator polynomial.<br>            sigmaprev = Polynomial(sigma.coefficients)<br>            B = Polynomial(sigma.coefficients)<br>            omega = Polynomial(erasures_eval.coefficients) # to compute omega (the evaluator polynomial) at the same time, we also need to initialize it with the partial erasures evaluator polynomial<br>            omegaprev = Polynomial(omega.coefficients)<br>            A = Polynomial(omega.coefficients) # TODO: fix the initial value of the evaluator support polynomial, because currently the final omega is not correct (it contains higher order terms that should be removed by the end of BM)<br>        else:<br>            sigma = sigmaprev = Polynomial([GF2int(1)]) # error locator polynomial. Also called Lambda in other notations.<br>            sigmaprev = Polynomial([GF2int(1)]) # we need the previous iteration to compute the next value of the support polynomials<br>            B = Polynomial([GF2int(1)]) # this is the error locator support/secondary polynomial, which is a funky way to say that it's just a temporary variable that will help us construct sigma, the error locator polynomial<br>            omega = omegaprev = Polynomial([GF2int(1)]) # error evaluator polynomial. We don't need to initialize it with erasures_loc, it will still work, because Delta is computed using sigma, which itself is correctly initialized with erasures if needed.<br>            omegaprev = Polynomial([GF2int(1)])<br>            A = Polynomial([GF2int(0)]) # this is the error evaluator support/secondary polynomial, to help us construct omega<br>        L = 0 # update flag: necessary variable to check when updating is necessary and to check bounds (to avoid wrongly eliminating the higher order terms). For more infos, see https://www.cs.duke.edu/courses/spring11/cps296.3/decoding_rs.pdf<br>        #M = 0 # optional variable to check bounds (so that we do not mistakenly overwrite the higher order terms). This is not necessary, it's only an additional safe check. For more infos, see the presentation decoding_rs.pdf by Andrew Brown in the doc folder.<br><br>        # Fix the syndrome shifting: when computing the syndrome, some implementations may prepend a 0 coefficient for the lowest degree term (the constant). This is a case of syndrome shifting, thus the syndrome will be bigger than the number of ecc symbols (I don't know what purpose serves this shifting). If that's the case, then we need to account for the syndrome shifting when we use the syndrome such as inside BM, by skipping those prepended coefficients.<br>        # Another way to detect the shifting is to detect the 0 coefficients: by definition, a syndrome does not contain any 0 coefficient (except if there are no errors/erasures, in this case they are all 0). This however doesn't work with the modified Forney syndrome (that we do not use in this lib but it may be implemented in the future), which set to 0 the coefficients corresponding to erasures, leaving only the coefficients corresponding to errors.<br>        synd_shift = 0<br>        if len(s)  (n-k): synd_shift = len(s) - (n-k)<br><br>        # Polynomial constants:<br>        ONE = Polynomial([GF2int(1)])<br>        ZERO = GF2int(0)<br>        Z = Polynomial([GF2int(1), GF2int(0)]) # used to shift polynomials, simply multiply your poly  Z to shift<br><br>        # Precaching<br>        s2 = ONE+s<br><br>        # Iteratively compute the polynomials n-k-erasures_count times. The last ones will be correct (since the algorithm refines the error/errata locator polynomial iteratively depending on the discrepancy, which is kind of a difference-from-correctness measure).<br>        for l in _range(n-k-erasures_count): # skip the first erasures_count iterations because we already computed the partial errata locator polynomial (by initializing with the erasures locator polynomial)<br>            K = erasures_count+l+synd_shift # skip the FIRST erasures_count iterations (not the last iterations, that's very important!)<br><br>            # Goal for each iteration: Compute sigma[l+1] and omega[l+1] such that<br>            # (1 + s)sigma[l] == omega[l] in mod z^(K)<br><br>            # For this particular loop iteration, we have sigma[l] and omega[l],<br>            # and are computing sigma[l+1] and omega[l+1]<br><br>            # First find Delta, the non-zero coefficient of z^(K) in<br>            # (1 + s)  sigma[l]<br>            # Note that adding 1 to the syndrome s is not really necessary, you can do as well without.<br>            # This delta is valid for l (this iteration) only<br>            Delta = s2.mul_at(sigma, K) # Delta is also known as the Discrepancy, and is always a scalar (not a polynomial). We just need one coefficient at a specific degree, so we can optimize by computing only the polynomial multiplication at this term, and skip the others.<br><br>            # Can now compute sigma[l+1] and omega[l+1] from<br>            # sigma[l], omega[l], B[l], A[l], and Delta<br>            sigmaprev = sigma<br>            omegaprev = omega<br>            sigma = sigma - (Z  B).scale(Delta)<br>            omega = omega - (Z  A).scale(Delta)<br><br>            # Now compute the next support polynomials B and A<br>            # There are two ways to do this<br>            # This is based on a messy case analysis on the degrees of the four polynomials sigma, omega, A and B in order to minimize the degrees of A and B. For more infos, see https://www.cs.duke.edu/courses/spring10/cps296.3/decoding_rs_scribe.pdf<br>            # In fact it ensures that the degree of the final polynomials aren't too large.<br>            if Delta == ZERO or 2L  K+erasures_count:<br>                #or (2L == K+erasures_count and M == 0):<br>            #if Delta == ZERO or len(sigma) <= len(sigmaprev): # another way to compute when to update, and it doesn't require to maintain the update flag L<br>                # Rule A<br>                B = Z  B<br>                A = Z  A<br>                #L = L<br>                #M = M<br><br>            else:<br>            #elif (Delta != ZERO and 2L < K+erasures_count) \<br>            #    or (2L == K+erasures_count and M != 0):<br>            # elif Delta != ZERO and len(sigma)  len(sigmaprev): # another way to compute when to update, and it doesn't require to maintain the update flag L<br>                # Rule B<br>                B = sigmaprev.scale(Delta.inverse())<br>                A = omegaprev.scale(Delta.inverse())<br>                L = K - L # the update flag L is tricky: in Blahut's schema, it's mandatory to use `L = K - L - erasures_count` (and indeed in a previous draft of this function, if you forgot to do `- erasures_count` it would lead to correcting only 2(errors+erasures) <= (n-k) instead of 2errors+erasures <= (n-k)), but in this latest draft, this will lead to a wrong decoding in some cases where it should correctly decode! Thus you should try with and without `- erasures_count` to update L on your own implementation and see which one works OK without producing wrong decoding failures.<br>                #M = 1 - M<br><br>            #else:<br>            #    raise Exception("Code shouldn't have gotten here")<br><br>        # Hack to fix the simultaneous computation of omega, the errata evaluator polynomial: because A (the errata evaluator support polynomial) is not correctly initialized (I could not find any info in academic papers). So at the end, we get the correct errata evaluator polynomial omega + some higher order terms that should not be present, but since we know that sigma is always correct and the maximum degree should be the same as omega, we can fix omega by truncating too high order terms.<br>        if omega.degree  sigma.degree: omega = Polynomial(omega.coefficients[-(sigma.degree+1):])<br><br>        # Return the last result of the iterations (since BM compute iteratively, the last iteration being correct - it may already be before, but we're not sure)<br>        return sigma, omega

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_326876.txt
> def hurst_rs(data, nvals=None, fit="RANSAC", debug_plot=False,<br>             debug_data=False, plot_file=None, corrected=True, unbiased=True):<br>  """<br>  Calculates the Hurst exponent by a standard rescaled range (R/S) approach.<br><br>  Explanation of Hurst exponent:<br>    The Hurst exponent is a measure for the "long-term memory" of a<br>    time series, meaning the long statistical dependencies in the data that do<br>    not originate from cycles.<br><br>    It originates from H.E. Hursts observations of the problem of long-term<br>    storage in water reservoirs. If x_i is the discharge of a river in year i<br>    and we observe this discharge for N years, we can calculate the storage<br>    capacity that would be required to keep the discharge steady at its mean<br>    value.<br><br>    To do so, we first substract the mean over all x_i from the individual<br>    x_i to obtain the departures x'_i from the mean for each year i. As the<br>    excess or deficit in discharge always carrys over from year i to year i+1,<br>    we need to examine the cumulative sum of x'_i, denoted by y_i. This<br>    cumulative sum represents the filling of our hypothetical storage. If the<br>    sum is above 0, we are storing excess discharge from the river, if it is<br>    below zero we have compensated a deficit in discharge by releasing<br>    water from the storage. The range (maximum - minimum) R of y_i therefore<br>    represents the total capacity required for the storage.<br><br>    Hurst showed that this value follows a steady trend for varying N if it<br>    is normalized by the standard deviation sigma over the x_i. Namely he<br>    obtained the following formula:<br><br>    R/sigma = (N/2)^K<br><br>    In this equation, K is called the Hurst exponent. Its value is 0.5 for<br>    white noise, but becomes greater for time series that exhibit some positive<br>    dependency on previous values. For negative dependencies it becomes less<br>    than 0.5.<br><br>  Explanation of the algorithm:<br>    The rescaled range (R/S) approach is directly derived from Hurst's<br>    definition. The time series of length N is split into non-overlapping<br>    subseries of length n. Then, R and S (S = sigma) are calculated for each<br>    subseries and the mean is taken over all subseries yielding (R/S)_n. This<br>    process is repeated for several lengths n. Finally, the exponent K is<br>    obtained by fitting a straight line to the plot of log((R/S)_n) vs log(n).<br><br>    There seems to be no consensus how to chose the subseries lenghts n.<br>    This function therefore leaves the choice to the user. The module provides<br>    some utility functions for "typical" values:<br><br>       binary_n: N/2, N/4, N/8, ...<br>       logarithmic_n: min_n, min_n  f, min_n  f^2, ...<br><br>  References:<br>    .. [h_1] H. E. Hurst, “The problem of long-term storage in reservoirs,”<br>       International Association of Scientific Hydrology. Bulletin, vol. 1,<br>       no. 3, pp. 13–27, 1956.<br>    .. [h_2] H. E. Hurst, “A suggested statistical model of some time series<br>       which occur in nature,” Nature, vol. 180, p. 494, 1957.<br>    .. [h_3] R. Weron, “Estimating long-range dependence: finite sample<br>       properties and confidence intervals,” Physica A: Statistical Mechanics<br>       and its Applications, vol. 312, no. 1, pp. 285–299, 2002.<br><br>  Reference Code:<br>    .. [h_a] "hurst" function in R-package "pracma",<br>             url: https://cran.r-project.org/web/packages/pracma/pracma.pdf<br><br>             Note: Pracma yields several estimates of the Hurst exponent, which<br>             are listed below. Unless otherwise stated they use the divisors<br>             of the length of the sequence as n. The length is reduced by at<br>             most 1% to find the value that has the most divisors.<br><br>              The "Simple R/S" estimate is just log((R/S)_n) / log(n) for <br>               n = N.<br>              The "theoretical Hurst exponent" is the value that would be<br>               expected of an uncorrected rescaled range approach for random<br>               noise of the size of the input data.<br>              The "empirical Hurst exponent" is the uncorrected Hurst exponent<br>               obtained by the rescaled range approach.<br>              The "corrected empirical Hurst exponent" is the Anis-Lloyd-Peters<br>               corrected Hurst exponent, but with sqrt(1/2  pi  n) added to<br>               the (R/S)_n before the log.<br>              The "corrected R over S Hurst exponent" uses the R-function "lm"<br>               instead of pracmas own "polyfit" and uses n = N/2, N/4, N/8, ...<br>               by successively halving the subsequences (which means that some<br>               subsequences may be one element longer than others). In contrast<br>               to its name it does not use the Anis-Lloyd-Peters correction<br>               factor.<br><br>             If you want to compare the output of pracma to the output of<br>             nolds, the "empirical hurst exponent" is the only measure that<br>             exactly corresponds to the Hurst measure implemented in nolds<br>             (by choosing corrected=False, fit="poly" and employing the same<br>             strategy for choosing n as the divisors of the (reduced)<br>             sequence length).<br>    .. [h_b] Rafael Weron, "HURST: MATLAB function to compute the Hurst<br>             exponent using R/S Analysis",<br>             url: https://ideas.repec.org/c/wuu/hscode/m11003.html<br><br>             Note: When the same values for nvals are used and fit is set to<br>             "poly", nolds yields exactly the same results as this<br>             implementation.<br>    .. [h_c] Bill Davidson, "Hurst exponent",<br>             url: http://www.mathworks.com/matlabcentral/fileexchange/9842-hurst-exponent<br>    .. [h_d] Tomaso Aste, "Generalized Hurst exponent",<br>             url: http://de.mathworks.com/matlabcentral/fileexchange/30076-generalized-hurst-exponent<br><br>  Args:<br>    data (array-like of float):<br>      time series<br>  Kwargs:<br>    nvals (iterable of int):<br>      sizes of subseries to use<br>      (default: logmid_n(total_N, ratio=1/4.0, nsteps=15) , that is 15<br>      logarithmically spaced values in the medium 25% of the logarithmic range)<br><br>      Generally, the choice for n is a trade-off between the length and the<br>      number of the subsequences that are used for the calculation of the<br>      (R/S)_n. Very low values of n lead to high variance in the ``r`` and ``s``<br>      while very high values may leave too few subsequences that the mean along<br>      them is still meaningful. Logarithmic spacing makes sense, because it <br>      translates to even spacing in the log-log-plot.<br>    fit (str):<br>      the fitting method to use for the line fit, either 'poly' for normal<br>      least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which<br>      is more robust to outliers<br>    debug_plot (boolean):<br>      if True, a simple plot of the final line-fitting step will be shown<br>    debug_data (boolean):<br>      if True, debugging data will be returned alongside the result<br>    plot_file (str):<br>      if debug_plot is True and plot_file is not None, the plot will be saved<br>      under the given file name instead of directly showing it through<br>      ``plt.show()``<br>    corrected (boolean):<br>      if True, the Anis-Lloyd-Peters correction factor will be applied to the<br>      output according to the expected value for the individual (R/S)_n<br>      (see [h_3]_)<br>    unbiased (boolean):<br>      if True, the standard deviation based on the unbiased variance<br>      (1/(N-1) instead of 1/N) will be used. This should be the default choice,<br>      since the true mean of the sequences is not known. This parameter should<br>      only be changed to recreate results of other implementations.<br><br>  Returns:<br>    float:<br>      estimated Hurst exponent K using a rescaled range approach (if K = 0.5<br>      there are no long-range correlations in the data, if K < 0.5 there are<br>      negative long-range correlations, if K  0.5 there are positive<br>      long-range correlations)<br>    (1d-vector, 1d-vector, list):<br>      only present if debug_data is True: debug data of the form<br>      ``(nvals, rsvals, poly)`` where ``nvals`` are the values used for log(n), <br>      ``rsvals`` are the corresponding log((R/S)_n) and ``poly`` are the line <br>      coefficients (``[slope, intercept]``)<br>  """<br>  data = np.asarray(data)<br>  total_N = len(data)<br>  if nvals is None:<br>    # chooses a default value for nvals that will give 15 logarithmically<br>    # spaced datapoints leaning towards the middle of the logarithmic range<br>    # (since both too small and too large n introduce too much variance)<br>    nvals = logmid_n(total_N, ratio=1/4.0, nsteps=15)<br>  # get individual values for (R/S)_n<br>  rsvals = np.array([rs(data, n, unbiased=unbiased) for n in nvals])<br>  # filter NaNs (zeros should not be possible, because if R is 0 then<br>  # S is also zero)<br>  not_nan = np.logical_not(np.isnan(rsvals))<br>  rsvals = rsvals[not_nan]<br>  nvals = np.asarray(nvals)[not_nan]<br>  # it may happen that no rsvals are left (if all values of data are the same)<br>  if len(rsvals) == 0:<br>    poly = [np.nan, np.nan]<br>    if debug_plot:<br>      warnings.warn("Cannot display debug plot, all (R/S)_n are NaN")<br>  else:<br>    # fit a line to the logarithm of the obtained (R/S)_n<br>    xvals = np.log(nvals)<br>    yvals = np.log(rsvals)<br>    if corrected:<br>      yvals -= np.log([expected_rs(n) for n in nvals])<br>    poly = poly_fit(xvals, yvals, 1, fit=fit)<br>    if debug_plot:<br>      plot_reg(xvals, yvals, poly, "log(n)", "log((R/S)_n)",<br>               fname=plot_file)<br>  # account for correction if necessary<br>  h = poly[0] + 0.5 if corrected else poly[0]<br>  # return line slope (+ correction) as hurst exponent<br>  if debug_data:<br>    return (h, (np.log(nvals), np.log(rsvals), poly))<br>  else:<br>    return h

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_226305.txt
> def _berlekamp_massey(self, s, k=None, erasures_loc=None, erasures_eval=None, erasures_count=0):<br>        '''Computes and returns the errata (errors+erasures) locator polynomial (sigma) and the<br>        error evaluator polynomial (omega) at the same time.<br>        If the erasures locator is specified, we will return an errors-and-erasures locator polynomial and an errors-and-erasures evaluator polynomial, else it will compute only errors. With erasures in addition to errors, it can simultaneously decode up to v+2e <= (n-k) where v is the number of erasures and e the number of errors.<br>        Mathematically speaking, this is equivalent to a spectral analysis (see Blahut, "Algebraic Codes for Data Transmission", 2003, chapter 7.6 Decoding in Time Domain).<br>        The parameter s is the syndrome polynomial (syndromes encoded in a<br>        generator function) as returned by _syndromes.<br><br>        Notes:<br>        The error polynomial:<br>        E(x) = E_0 + E_1 x + ... + E_(n-1) x^(n-1)<br><br>        j_1, j_2, ..., j_s are the error positions. (There are at most s<br>        errors)<br><br>        Error location X_i is defined: X_i = α^(j_i)<br>        that is, the power of α (alpha) corresponding to the error location<br><br>        Error magnitude Y_i is defined: E_(j_i)<br>        that is, the coefficient in the error polynomial at position j_i<br><br>        Error locator polynomial:<br>        sigma(z) = Product( 1 - X_i  z, i=1..s )<br>        roots are the reciprocals of the error locations<br>        ( 1/X_1, 1/X_2, ...)<br><br>        Error evaluator polynomial omega(z) is here computed at the same time as sigma, but it can also be constructed afterwards using the syndrome and sigma (see _find_error_evaluator() method).<br><br>        It can be seen that the algorithm tries to iteratively solve for the error locator polynomial by<br>        solving one equation after another and updating the error locator polynomial. If it turns out that it<br>        cannot solve the equation at some step, then it computes the error and weights it by the last<br>        non-zero discriminant found, and delays the weighted result to increase the polynomial degree<br>        by 1. Ref: "Reed Solomon Decoder: TMS320C64x Implementation" by Jagadeesh Sankaran, December 2000, Application Report SPRA686<br><br>        The best paper I found describing the BM algorithm for errata (errors-and-erasures) evaluator computation is in "Algebraic Codes for Data Transmission", Richard E. Blahut, 2003.<br>        '''<br>        # For errors-and-erasures decoding, see: "Algebraic Codes for Data Transmission", Richard E. Blahut, 2003 and (but it's less complete): Blahut, Richard E. "Transform techniques for error control codes." IBM Journal of Research and development 23.3 (1979): 299-315. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.600&rep=rep1&type=pdf and also a MatLab implementation here: http://www.mathworks.com/matlabcentral/fileexchange/23567-reed-solomon-errors-and-erasures-decoder/content//RS_E_E_DEC.m<br>        # also see: Blahut, Richard E. "A universal Reed-Solomon decoder." IBM Journal of Research and Development 28.2 (1984): 150-158. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.2084&rep=rep1&type=pdf<br>        # and another good alternative book with concrete programming examples: Jiang, Yuan. A practical guide to error-control coding using Matlab. Artech House, 2010.<br>        n = self.n<br>        if not k: k = self.k<br><br>        # Initialize, depending on if we include erasures or not:<br>        if erasures_loc:<br>            sigma = [ Polynomial(erasures_loc.coefficients) ] # copy erasures_loc by creating a new Polynomial, so that we initialize the errata locator polynomial with the erasures locator polynomial.<br>            B = [ Polynomial(erasures_loc.coefficients) ]<br>            omega = [ Polynomial(erasures_eval.coefficients) ] # to compute omega (the evaluator polynomial) at the same time, we also need to initialize it with the partial erasures evaluator polynomial<br>            A = [ Polynomial(erasures_eval.coefficients) ] # TODO: fix the initial value of the evaluator support polynomial, because currently the final omega is not correct (it contains higher order terms that should be removed by the end of BM)<br>        else:<br>            sigma = [ Polynomial([GF2int(1)]) ] # error locator polynomial. Also called Lambda in other notations.<br>            B = [ Polynomial([GF2int(1)]) ] # this is the error locator support/secondary polynomial, which is a funky way to say that it's just a temporary variable that will help us construct sigma, the error locator polynomial<br>            omega = [ Polynomial([GF2int(1)]) ] # error evaluator polynomial. We don't need to initialize it with erasures_loc, it will still work, because Delta is computed using sigma, which itself is correctly initialized with erasures if needed.<br>            A = [ Polynomial([GF2int(0)]) ] # this is the error evaluator support/secondary polynomial, to help us construct omega<br>        L = [ 0 ] # update flag: necessary variable to check when updating is necessary and to check bounds (to avoid wrongly eliminating the higher order terms). For more infos, see https://www.cs.duke.edu/courses/spring11/cps296.3/decoding_rs.pdf<br>        M = [ 0 ] # optional variable to check bounds (so that we do not mistakenly overwrite the higher order terms). This is not necessary, it's only an additional safe check. For more infos, see the presentation decoding_rs.pdf by Andrew Brown in the doc folder.<br><br>        # Fix the syndrome shifting: when computing the syndrome, some implementations may prepend a 0 coefficient for the lowest degree term (the constant). This is a case of syndrome shifting, thus the syndrome will be bigger than the number of ecc symbols (I don't know what purpose serves this shifting). If that's the case, then we need to account for the syndrome shifting when we use the syndrome such as inside BM, by skipping those prepended coefficients.<br>        # Another way to detect the shifting is to detect the 0 coefficients: by definition, a syndrome does not contain any 0 coefficient (except if there are no errors/erasures, in this case they are all 0). This however doesn't work with the modified Forney syndrome (that we do not use in this lib but it may be implemented in the future), which set to 0 the coefficients corresponding to erasures, leaving only the coefficients corresponding to errors.<br>        synd_shift = 0<br>        if len(s)  (n-k): synd_shift = len(s) - (n-k)<br><br>        # Polynomial constants:<br>        ONE = Polynomial(z0=GF2int(1))<br>        ZERO = Polynomial(z0=GF2int(0))<br>        Z = Polynomial(z1=GF2int(1)) # used to shift polynomials, simply multiply your poly  Z to shift<br><br>        # Precaching<br>        s2 = ONE + s<br><br>        # Iteratively compute the polynomials n-k-erasures_count times. The last ones will be correct (since the algorithm refines the error/errata locator polynomial iteratively depending on the discrepancy, which is kind of a difference-from-correctness measure).<br>        for l in _range(0, n-k-erasures_count): # skip the first erasures_count iterations because we already computed the partial errata locator polynomial (by initializing with the erasures locator polynomial)<br>            K = erasures_count+l+synd_shift # skip the FIRST erasures_count iterations (not the last iterations, that's very important!)<br><br>            # Goal for each iteration: Compute sigma[l+1] and omega[l+1] such that<br>            # (1 + s)sigma[l] == omega[l] in mod z^(K)<br><br>            # For this particular loop iteration, we have sigma[l] and omega[l],<br>            # and are computing sigma[l+1] and omega[l+1]<br><br>            # First find Delta, the non-zero coefficient of z^(K) in<br>            # (1 + s)  sigma[l]<br>            # Note that adding 1 to the syndrome s is not really necessary, you can do as well without.<br>            # This delta is valid for l (this iteration) only<br>            Delta = ( s2  sigma[l] ).get_coefficient(K) # Delta is also known as the Discrepancy, and is always a scalar (not a polynomial).<br>            # Make it a polynomial of degree 0, just for ease of computation with polynomials sigma and omega.<br>            Delta = Polynomial(x0=Delta)<br><br>            # Can now compute sigma[l+1] and omega[l+1] from<br>            # sigma[l], omega[l], B[l], A[l], and Delta<br>            sigma.append( sigma[l] - Delta  Z  B[l] )<br>            omega.append( omega[l] - Delta  Z  A[l] )<br><br>            # Now compute the next support polynomials B and A<br>            # There are two ways to do this<br>            # This is based on a messy case analysis on the degrees of the four polynomials sigma, omega, A and B in order to minimize the degrees of A and B. For more infos, see https://www.cs.duke.edu/courses/spring10/cps296.3/decoding_rs_scribe.pdf<br>            # In fact it ensures that the degree of the final polynomials aren't too large.<br>            if Delta == ZERO or 2L[l]  K+erasures_count \<br>                or (2L[l] == K+erasures_count and M[l] == 0):<br>            #if Delta == ZERO or len(sigma[l+1]) <= len(sigma[l]): # another way to compute when to update, and it doesn't require to maintain the update flag L<br>                # Rule A<br>                B.append( Z  B[l] )<br>                A.append( Z  A[l] )<br>                L.append( L[l] )<br>                M.append( M[l] )<br><br>            elif (Delta != ZERO and 2L[l] < K+erasures_count) \<br>                or (2L[l] == K+erasures_count and M[l] != 0):<br>            # elif Delta != ZERO and len(sigma[l+1])  len(sigma[l]): # another way to compute when to update, and it doesn't require to maintain the update flag L<br>                # Rule B<br>                B.append( sigma[l] // Delta )<br>                A.append( omega[l] // Delta )<br>                L.append( K - L[l] ) # the update flag L is tricky: in Blahut's schema, it's mandatory to use `L = K - L - erasures_count` (and indeed in a previous draft of this function, if you forgot to do `- erasures_count` it would lead to correcting only 2(errors+erasures) <= (n-k) instead of 2errors+erasures <= (n-k)), but in this latest draft, this will lead to a wrong decoding in some cases where it should correctly decode! Thus you should try with and without `- erasures_count` to update L on your own implementation and see which one works OK without producing wrong decoding failures.<br>                M.append( 1 - M[l] )<br><br>            else:<br>                raise Exception("Code shouldn't have gotten here")<br><br>        # Hack to fix the simultaneous computation of omega, the errata evaluator polynomial: because A (the errata evaluator support polynomial) is not correctly initialized (I could not find any info in academic papers). So at the end, we get the correct errata evaluator polynomial omega + some higher order terms that should not be present, but since we know that sigma is always correct and the maximum degree should be the same as omega, we can fix omega by truncating too high order terms.<br>        if omega[-1].degree  sigma[-1].degree: omega[-1] = Polynomial(omega[-1].coefficients[-(sigma[-1].degree+1):])<br><br>        # Debuglines, uncomment to show the result of every iterations<br>        #print "SIGMA BM"<br>        #for i,x in enumerate(sigma):<br>            #print i, ":", x<br><br>        # Return the last result of the iterations (since BM compute iteratively, the last iteration being correct - it may already be before, but we're not sure)<br>        return sigma[-1], omega[-1]

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_309526.txt
> def solveConsAggMarkov(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,MrkvArray,<br>                       PermGroFac,PermGroFacAgg,aXtraGrid,BoroCnstArt,Mgrid,<br>                       AFunc,Rfunc,wFunc,DeprFac):<br>    '''<br>    Solve one period of a consumption-saving problem with idiosyncratic and<br>    aggregate shocks (transitory and permanent).  Moreover, the macroeconomic<br>    state follows a Markov process that determines the income distribution and<br>    aggregate permanent growth factor. This is a basic solver that can't handle<br>    cubic splines, nor can it calculate a value function.<br><br>    Parameters<br>    ----------<br>    solution_next : ConsumerSolution<br>        The solution to the succeeding one period problem.<br>    IncomeDstn : [[np.array]]<br>        A list of lists, each containing five arrays of floats, representing a<br>        discrete approximation to the income process between the period being<br>        solved and the one immediately following (in solution_next). Order: event<br>        probabilities, idisyncratic permanent shocks, idiosyncratic transitory<br>        shocks, aggregate permanent shocks, aggregate transitory shocks.<br>    LivPrb : float<br>        Survival probability; likelihood of being alive at the beginning of<br>        the succeeding period.<br>    DiscFac : float<br>        Intertemporal discount factor for future utility.<br>    CRRA : float<br>        Coefficient of relative risk aversion.<br>    MrkvArray : np.array<br>        Markov transition matrix between discrete macroeconomic states.<br>        MrkvArray[i,j] is probability of being in state j next period conditional<br>        on being in state i this period.<br>    PermGroFac : float<br>        Expected permanent income growth factor at the end of this period,<br>        for the individual's productivity.<br>    PermGroFacAgg : [float]<br>        Expected aggregate productivity growth in each Markov macro state.<br>    aXtraGrid : np.array<br>        Array of "extra" end-of-period asset values-- assets above the<br>        absolute minimum acceptable level.<br>    BoroCnstArt : float<br>        Artificial borrowing constraint; minimum allowable end-of-period asset-to-<br>        permanent-income ratio.  Unlike other models, this can't be None.<br>    Mgrid : np.array<br>        A grid of aggregate market resourses to permanent income in the economy.<br>    AFunc : [function]<br>        Aggregate savings as a function of aggregate market resources, for each<br>        Markov macro state.<br>    Rfunc : function<br>        The net interest factor on assets as a function of capital ratio k.<br>    wFunc : function<br>        The wage rate for labor as a function of capital-to-labor ratio k.<br>    DeprFac : float<br>        Capital Depreciation Rate<br><br>    Returns<br>    -------<br>    solution_now : ConsumerSolution<br>        The solution to the single period consumption-saving problem.  Includes<br>        a consumption function cFunc (linear interpolation over linear interpola-<br>        tions) and marginal value function vPfunc.<br>    '''<br>    # Get sizes of grids<br>    aCount = aXtraGrid.size<br>    Mcount = Mgrid.size<br>    StateCount = MrkvArray.shape[0]<br><br>    # Loop through next period's states, assuming we reach each one at a time.<br>    # Construct EndOfPrdvP_cond functions for each state.<br>    EndOfPrdvPfunc_cond = []<br>    BoroCnstNat_cond = []<br>    for j in range(StateCount):<br>        # Unpack next period's solution<br>        vPfuncNext = solution_next.vPfunc[j]<br>        mNrmMinNext = solution_next.mNrmMin[j]<br><br>        # Unpack the income shocks<br>        ShkPrbsNext  = IncomeDstn[j][0]<br>        PermShkValsNext = IncomeDstn[j][1]<br>        TranShkValsNext = IncomeDstn[j][2]<br>        PermShkAggValsNext = IncomeDstn[j][3]<br>        TranShkAggValsNext = IncomeDstn[j][4]<br>        ShkCount = ShkPrbsNext.size<br>        aXtra_tiled = np.tile(np.reshape(aXtraGrid,(1,aCount,1)),(Mcount,1,ShkCount))<br><br>        # Make tiled versions of the income shocks<br>        # Dimension order: Mnow, aNow, Shk<br>        ShkPrbsNext_tiled = np.tile(np.reshape(ShkPrbsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>        PermShkValsNext_tiled = np.tile(np.reshape(PermShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>        TranShkValsNext_tiled = np.tile(np.reshape(TranShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>        PermShkAggValsNext_tiled = np.tile(np.reshape(PermShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>        TranShkAggValsNext_tiled = np.tile(np.reshape(TranShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br><br>        # Make a tiled grid of end-of-period aggregate assets.  These lines use<br>        # next prd state j's aggregate saving rule to get a relevant set of Aagg,<br>        # which will be used to make an interpolated EndOfPrdvP_cond function.<br>        # After constructing these functions, we will use the aggregate saving<br>        # rule for current state i to get values of Aagg at which to evaluate<br>        # these conditional marginal value functions.  In the strange, maybe even<br>        # impossible case where the aggregate saving rules differ wildly across<br>        # macro states and there is "anti-persistence", so that the macro state<br>        # is very likely to change each period, then this procedure will lead to<br>        # an inaccurate solution because the grid of Aagg values on which the<br>        # conditional marginal value functions are constructed is not relevant<br>        # to the values at which it will actually be evaluated.<br>        AaggGrid = AFunc[j](Mgrid)<br>        AaggNow_tiled = np.tile(np.reshape(AaggGrid,(Mcount,1,1)),(1,aCount,ShkCount))<br><br>        # Calculate returns to capital and labor in the next period<br>        kNext_array = AaggNow_tiled/(PermGroFacAgg[j]PermShkAggValsNext_tiled) # Next period's aggregate capital to labor ratio<br>        kNextEff_array = kNext_array/TranShkAggValsNext_tiled # Same thing, but account for transitory shock<br>        R_array = Rfunc(kNextEff_array) # Interest factor on aggregate assets<br>        Reff_array = R_array/LivPrb # Effective interest factor on individual assets for survivors<br>        wEff_array = wFunc(kNextEff_array)TranShkAggValsNext_tiled # Effective wage rate (accounts for labor supply)<br>        PermShkTotal_array = PermGroFacPermGroFacAgg[j]PermShkValsNext_tiledPermShkAggValsNext_tiled # total / combined permanent shock<br>        Mnext_array = kNext_arrayR_array + wEff_array # next period's aggregate market resources<br><br>        # Find the natural borrowing constraint for each value of M in the Mgrid.<br>        # There is likely a faster way to do this, but someone needs to do the math:<br>        # is aNrmMin determined by getting the worst shock of all four types?<br>        aNrmMin_candidates = PermGroFacPermGroFacAgg[j]PermShkValsNext_tiled[:,0,:]PermShkAggValsNext_tiled[:,0,:]/Reff_array[:,0,:]\<br>                             (mNrmMinNext(Mnext_array[:,0,:]) - wEff_array[:,0,:]TranShkValsNext_tiled[:,0,:])<br>        aNrmMin_vec = np.max(aNrmMin_candidates,axis=1)<br>        BoroCnstNat_vec = aNrmMin_vec<br>        aNrmMin_tiled = np.tile(np.reshape(aNrmMin_vec,(Mcount,1,1)),(1,aCount,ShkCount))<br>        aNrmNow_tiled = aNrmMin_tiled + aXtra_tiled<br><br>        # Calculate market resources next period (and a constant array of capital-to-labor ratio)<br>        mNrmNext_array = Reff_arrayaNrmNow_tiled/PermShkTotal_array + TranShkValsNext_tiledwEff_array<br><br>        # Find marginal value next period at every income shock realization and every aggregate market resource gridpoint<br>        vPnext_array = Reff_arrayPermShkTotal_array(-CRRA)vPfuncNext(mNrmNext_array,Mnext_array)<br><br>        # Calculate expectated marginal value at the end of the period at every asset gridpoint<br>        EndOfPrdvP = DiscFacLivPrbnp.sum(vPnext_arrayShkPrbsNext_tiled,axis=2)<br><br>        # Make the conditional end-of-period marginal value function<br>        BoroCnstNat = LinearInterp(np.insert(AaggGrid,0,0.0),np.insert(BoroCnstNat_vec,0,0.0))<br>        EndOfPrdvPnvrs = np.concatenate((np.zeros((Mcount,1)),EndOfPrdvP(-1./CRRA)),axis=1)<br>        EndOfPrdvPnvrsFunc_base = BilinearInterp(np.transpose(EndOfPrdvPnvrs),np.insert(aXtraGrid,0,0.0),AaggGrid)<br>        EndOfPrdvPnvrsFunc = VariableLowerBoundFunc2D(EndOfPrdvPnvrsFunc_base,BoroCnstNat)<br>        EndOfPrdvPfunc_cond.append(MargValueFunc2D(EndOfPrdvPnvrsFunc,CRRA))<br>        BoroCnstNat_cond.append(BoroCnstNat)<br><br>    # Prepare some objects that are the same across all current states<br>    aXtra_tiled = np.tile(np.reshape(aXtraGrid,(1,aCount)),(Mcount,1))<br>    cFuncCnst = BilinearInterp(np.array([[0.0,0.0],[1.0,1.0]]),np.array([BoroCnstArt,BoroCnstArt+1.0]),np.array([0.0,1.0]))<br><br>    # Now loop through this period's discrete states, calculating end-of-period<br>    # marginal value (weighting across state transitions), then construct consumption<br>    # and marginal value function for each state.<br>    cFuncNow = []<br>    vPfuncNow = []<br>    mNrmMinNow = []<br>    for i in range(StateCount):<br>        # Find natural borrowing constraint for this state by Aagg<br>        AaggNow = AFunc[i](Mgrid)<br>        aNrmMin_candidates = np.zeros((StateCount,Mcount)) + np.nan<br>        for j in range(StateCount):<br>            if MrkvArray[i,j]  0.: # Irrelevant if transition is impossible<br>                aNrmMin_candidates[j,:] = BoroCnstNat_cond[j](AaggNow)<br>        aNrmMin_vec = np.nanmax(aNrmMin_candidates,axis=0)<br>        BoroCnstNat_vec = aNrmMin_vec<br><br>        # Make tiled grids of aNrm and Aagg<br>        aNrmMin_tiled = np.tile(np.reshape(aNrmMin_vec,(Mcount,1)),(1,aCount))<br>        aNrmNow_tiled = aNrmMin_tiled + aXtra_tiled<br>        AaggNow_tiled = np.tile(np.reshape(AaggNow,(Mcount,1)),(1,aCount))<br><br>        # Loop through feasible transitions and calculate end-of-period marginal value<br>        EndOfPrdvP = np.zeros((Mcount,aCount))<br>        for j in range(StateCount):<br>            if MrkvArray[i,j]  0.:<br>                temp = EndOfPrdvPfunc_cond[j](aNrmNow_tiled,AaggNow_tiled)<br>                EndOfPrdvP += MrkvArray[i,j]temp<br><br>        # Calculate consumption and the endogenous mNrm gridpoints for this state<br>        cNrmNow = EndOfPrdvP(-1./CRRA)<br>        mNrmNow = aNrmNow_tiled + cNrmNow<br><br>        # Loop through the values in Mgrid and make a piecewise linear consumption function for each<br>        cFuncBaseByM_list = []<br>        for n in range(Mcount):<br>            c_temp = np.insert(cNrmNow[n,:],0,0.0) # Add point at bottom<br>            m_temp = np.insert(mNrmNow[n,:] - BoroCnstNat_vec[n],0,0.0)<br>            cFuncBaseByM_list.append(LinearInterp(m_temp,c_temp))<br>            # Add the M-specific consumption function to the list<br><br>        # Construct the unconstrained consumption function by combining the M-specific functions<br>        BoroCnstNat = LinearInterp(np.insert(Mgrid,0,0.0),np.insert(BoroCnstNat_vec,0,0.0))<br>        cFuncBase = LinearInterpOnInterp1D(cFuncBaseByM_list,Mgrid)<br>        cFuncUnc  = VariableLowerBoundFunc2D(cFuncBase,BoroCnstNat)<br><br>        # Combine the constrained consumption function with unconstrained component<br>        cFuncNow.append(LowerEnvelope2D(cFuncUnc,cFuncCnst))<br><br>        # Make the minimum m function as the greater of the natural and artificial constraints<br>        mNrmMinNow.append(UpperEnvelope(BoroCnstNat,ConstantFunction(BoroCnstArt)))<br><br>        # Construct the marginal value function using the envelope condition<br>        vPfuncNow.append(MargValueFunc2D(cFuncNow[-1],CRRA))<br><br>    # Pack up and return the solution<br>    solution_now = ConsumerSolution(cFunc=cFuncNow,vPfunc=vPfuncNow,mNrmMin=mNrmMinNow)<br>    return solution_now

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_234279.txt
> def operator_oropt(self, graph, solution, op_diff_round_digits, anim=None):<br>        # TODO: check docstring<br>        """Applies Or-Opt intra-route operator to solution<br>        <br>        Takes chains of nodes (length=3..1 consecutive nodes) from a given<br>        route and calculates savings when inserted into another position on the<br>        same route (all possible positions). Performes best move (max. saving)<br>        and starts over again with new route until no improvement is found.<br>        <br>        Args<br>        ----<br>        graph: :networkx:`NetworkX Graph Obj< `<br>            A NetworkX graaph is used.<br>        solution: BaseSolution<br>            BaseSolution instance<br>        op_diff_round_digits: float<br>            Precision (floating point digits) for rounding route length differences.<br>            <br>            Details: In some cases when an exchange is performed on two routes with one node each,<br>            the difference between the both solutions (before and after the exchange) is not zero.<br>            This is due to internal rounding errors of float type. So the loop won't break<br>            (alternating between these two solutions), we need an additional criterion to avoid<br>            this behaviour: A threshold to handle values very close to zero as if they were zero<br>            (for a more detailed description of the matter see http://floating-point-gui.de or<br>            https://docs.python.org/3.5/tutorial/floatingpoint.html)<br>        anim: AnimationDing0<br>            AnimationDing0 object<br>        <br>        Returns<br>        -------<br>        LocalSearchSolution<br>           A solution (LocalSearchSolution class)<br>        <br>        Notes<br>        -----<br>        Since Or-Opt is an intra-route operator, it has not to be checked if route can allocate (Route's method<br>        can_allocate()) nodes during relocation regarding max. peak load/current because the line/cable type is the<br>        same along the entire route. However, node order within a route has an impact on the voltage stability<br>        so the check would be actually required. Due to large line capacity (load factor of lines/cables ~60 %)<br>        the voltage stability issues are neglected.<br><br>        (Inner) Loop variables:<br>        <br>         s: length (count of consecutive nodes) of the chain that is moved. Values: 3..1<br>         i: node that precedes the chain before moving (position in the route `tour`, not node name)<br>         j: node that precedes the chain after moving (position in the route `tour`, not node name)<br>        <br>        Todo<br>        ----<br>         insert literature reference for Or-algorithm here<br>         Remove ugly nested loops, convert to more efficient matrix operations<br>        """<br>        no_ctr = 100<br>        # shorter var names for loop<br>        dm = graph._matrix<br>        dn = graph._nodes<br>        <br>        for route in solution.routes():<br><br>            # exclude routes with single high-demand nodes (Load Areas)<br>            if len(route._nodes) == 1:<br>                if solution._problem._is_aggregated[str(route._nodes[0])]:<br>                    continue<br><br>            n = len(route._nodes)+1<br><br>            # create tour by adding depot at start and end<br>            tour = [graph._depot] + route._nodes + [graph._depot]<br>            <br>            # Or-Opt: Search better solutions by checking possible chain moves<br>            while True:<br>                length = route.length()<br>                length_best = length<br>                <br>                for s in range(3,0,-1):<br>                    for i in range(1,n-s):<br>                        length_diff = (length -<br>                                       dm[dn[tour[i-1].name()]][dn[tour[i].name()]] -<br>                                       dm[dn[tour[i+s-1].name()]][dn[tour[i+s].name()]] +<br>                                       dm[dn[tour[i-1].name()]][dn[tour[i+s].name()]])<br>                        for j in range(i+s+1,n+1):<br>                            if j == n:<br>                                j2 = 1<br>                            else:<br>                                j2 = j+1<br>                            length_new = (length_diff +<br>                                          dm[dn[tour[j-1].name()]][dn[tour[i].name()]] +<br>                                          dm[dn[tour[i+s-1].name()]][dn[tour[j2-1].name()]] -<br>                                          dm[dn[tour[j-1].name()]][dn[tour[j2-1].name()]])<br>                            if length_new < length_best:<br>                                length_best = length_new<br>                                s_best, i_best, j_best = s, i, j<br>                if length_best < length:<br>                    tour = tour[0:i_best] + tour[i_best+s_best:j_best] + tour[i_best:i_best+s_best] + tour[j_best:n+1]<br><br>                    if anim is not None:<br>                        solution.draw_network(anim)<br><br>                # no improvement found<br>                if length_best == length:<br>                    # replace old route by new (same arg for allocation and deallocation since node order is considered at allocation)<br>                    solution._routes[solution._routes.index(route)].deallocate(tour[1:-1])<br>                    solution._routes[solution._routes.index(route)].allocate(tour[1:-1])<br>                    <br>                    break<br>        <br>        #solution = LocalSearchSolution(solution, graph, new_routes)<br>        return solution


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_2490.txt
> def is_circular(sentence: str) - bool:<br>    """A sentence is a list of words that are separated by a single space with no leading or trailing spaces.<br><br>   For example, `"Hello World "`, `"HELLO "`, `"hello world hello world "` are all sentences.<br><br>Words consist of only uppercase and lowercase English letters. Uppercase and lowercase English letters are considered different.<br><br>A sentence is circular if:<br><br>   The last character of a word is equal to the first character of the next word.<br>   The last character of the last word is equal to the first character of the first word.<br><br>For example, `"leetcode exercises sound delightful "`, `"eetcode "`, `"leetcode eats soul "` are all circular sentences. However, `"Leetcode is cool "`, `"happy Leetcode "`, `"Leetcode "` and `"I like Leetcode "` are not circular sentences.<br><br>Given a string `sentence`, return `true` _if it is circular_. Otherwise, return `false`.<br><br>Example 1:<br><br>Input: sentence =  "leetcode exercises sound delightful "<br>Output: true<br>Explanation: The words in sentence are \[ "leetcode ",  "exercises ",  "sound ",  "delightful "\].<br>- leetcode's last character is equal to exercises's first character.<br>- exercises's last character is equal to sound's first character.<br>- sound's last character is equal to delightful's first character.<br>- delightful's last character is equal to leetcode's first character.<br>The sentence is circular.<br><br>Example 2:<br><br>Input: sentence =  "eetcode "<br>Output: true<br>Explanation: The words in sentence are \[ "eetcode "\].<br>- eetcode's last character is equal to eetcode's first character.<br>The sentence is circular.<br><br>Example 3:<br><br>Input: sentence =  "Leetcode is cool "<br>Output: false<br>Explanation: The words in sentence are \[ "Leetcode ",  "is ",  "cool "\].<br>- Leetcode's last character is not equal to is's first character.<br>The sentence is not circular.<br><br>Constraints:<br><br>   `1 <= sentence.length <= 500`<br>   `sentence` consist of only lowercase and uppercase English letters and spaces.<br>   The words in `sentence` are separated by a single space.<br>   There are no leading or trailing spaces."""<br><br>    words = sentence.split()<br>    for i in range(1, len(words)):<br>        if words[i - 1][-1] != words[i][0]:<br>            return False<br>    return words[-1][-1] == words[0][0]

### GROUND TRUTH 1, ranked 7, leetcode/leetcode_2515.txt
> def shortest_distance(words, target, startIndex):<br>    """You are given a 0-indexed circular string array `words` and a string `target`. A circular array means that the array's end connects to the array's beginning.<br><br>   Formally, the next element of `words[i]` is `words[(i + 1) % n]` and the previous element of `words[i]` is `words[(i - 1 + n) % n]`, where `n` is the length of `words`.<br><br>Starting from `startIndex`, you can move to either the next word or the previous word with `1` step at a time.<br><br>Return _the shortest distance needed to reach the string_ `target`. If the string `target` does not exist in `words`, return `-1`.<br><br>Example 1:<br><br>Input: words = \[ "hello ", "i ", "am ", "leetcode ", "hello "\], target =  "hello ", startIndex = 1<br>Output: 1<br>Explanation: We start from index 1 and can reach  "hello " by<br>- moving 3 units to the right to reach index 4.<br>- moving 2 units to the left to reach index 4.<br>- moving 4 units to the right to reach index 0.<br>- moving 1 unit to the left to reach index 0.<br>The shortest distance to reach  "hello " is 1.<br><br>Example 2:<br><br>Input: words = \[ "a ", "b ", "leetcode "\], target =  "leetcode ", startIndex = 0<br>Output: 1<br>Explanation: We start from index 0 and can reach  "leetcode " by<br>- moving 2 units to the right to reach index 3.<br>- moving 1 unit to the left to reach index 3.<br>The shortest distance to reach  "leetcode " is 1.<br><br>Example 3:<br><br>Input: words = \[ "i ", "eat ", "leetcode "\], target =  "ate ", startIndex = 0<br>Output: -1<br>Explanation: Since  "ate " does not exist in `words`, we return -1.<br><br>Constraints:<br><br>   `1 <= words.length <= 100`<br>   `1 <= words[i].length <= 100`<br>   `words[i]` and `target` consist of only lowercase English letters.<br>   `0 <= startIndex < words.length`"""<br><br>    n = len(words)<br>    left, right, i = 0, 0, startIndex<br>    while True:<br>        if words[i] == target:<br>            return min(left, right)<br>        left += 1<br>        right += 1<br>        i = (i + 1) % n<br>        if left == n:<br>            break<br>    return -1

### GROUND TRUTH 2, ranked not in top 100, leetcode/leetcode_2516.txt
> def min_minutes(s: str, k: int) - int:<br>    """You are given a string `s` consisting of the characters `'a'`, `'b'`, and `'c'` and a non-negative integer `k`. Each minute, you may take either the leftmost character of `s`, or the rightmost character of `s`.<br><br>Return _the minimum number of minutes needed for you to take at least_ `k` _of each character, or return_ `-1` _if it is not possible to take_ `k` _of each character._<br><br>Example 1:<br><br>Input: s =  "aabaaaacaabc ", k = 2<br>Output: 8<br>Explanation: <br>Take three characters from the left of s. You now have two 'a' characters, and one 'b' character.<br>Take five characters from the right of s. You now have four 'a' characters, two 'b' characters, and two 'c' characters.<br>A total of 3 + 5 = 8 minutes is needed.<br>It can be proven that 8 is the minimum number of minutes needed.<br><br>Example 2:<br><br>Input: s =  "a ", k = 1<br>Output: -1<br>Explanation: It is not possible to take one 'b' or 'c' so return -1.<br><br>Constraints:<br><br>   `1 <= s.length <= 105`<br>   `s` consists of only the letters `'a'`, `'b'`, and `'c'`.<br>   `0 <= k <= s.length`"""<br><br>    n = len(s)<br>    left_counts = [0]  3<br>    right_counts = [0]  3<br>    for c in s:<br>        right_counts[ord(c) - ord('a')] += 1<br><br>    ans, total = n, 0<br>    l, r = 0, n - 1<br>    while l < r:<br>        while l < r and left_counts[ord(s[l]) - ord('a')] < k and right_counts[ord(s[r]) - ord('a')] < k:<br>            need_left = max(0, k - left_counts[ord(s[l]) - ord('a')])<br>            need_right = max(0, k - right_counts[ord(s[r]) - ord('a')])<br>            if need_left <= need_right:<br>                total += need_left<br>                l += 1<br>                left_counts[ord(s[l]) - ord('a')] += 1<br>                right_counts[ord(s[l]) - ord('a')] -= 1<br>            else:<br>                total += need_right<br>                r -= 1<br>                right_counts[ord(s[r]) - ord('a')] -= 1<br>                left_counts[ord(s[r]) - ord('a')] += 1<br>        ans = min(ans, total)<br>        if l + 1 < n:<br>            total -= left_counts[ord(s[l]) - ord('a')]<br>            left_counts[ord(s[l]) - ord('a')] -= 1<br>            right_counts[ord(s[l + 1]) - ord('a')] += 1<br>            l += 1<br>        else:<br>            break<br>    if all(cnt = k for cnt in left_counts): <br>        return ans<br>    return -1
