# Query `120`

## Original Question

*You are given a 2D integer array `tiles` where `tiles[i] = [li, ri]` represents that every tile `j` in the range `li <= j <= ri` is colored white.

You are also given an integer `carpetLen`, the length of a single carpet that can be placed anywhere.

Return _the maximum number of white tiles that can be covered by the carpet_.

Example 1:

Input: tiles = \[\[1,5\],\[10,11\],\[12,18\],\[20,25\],\[30,32\]\], carpetLen = 10
Output: 9
Explanation: Place the carpet starting on tile 10. 
It covers 9 white tiles, so we return 9.
Note that there may be other places where the carpet covers 9 white tiles.
It can be shown that the carpet cannot cover more than 9 white tiles.

Example 2:

Input: tiles = \[\[10,11\],\[1,1\]\], carpetLen = 2
Output: 2
Explanation: Place the carpet starting on tile 10. 
It covers 2 white tiles, so we return 2.

Constraints:

   `1 <= tiles.length <= 5  104`
   `tiles[i].length == 2`
   `1 <= li <= ri <= 109`
   `1 <= carpetLen <= 109`
   The `tiles` are non-overlapping.*

## gpt4 Reasoning

```markdown
To solve this problem, we need to determine how to maximize the number of white tiles covered by a carpet of a fixed length. This involves several steps and considerations:

### 1. Sort the Tiles

First, it's crucial to sort the tiles based on their starting positions (`li`). Sorting ensures that we process the tiles in a sequence, which is essential for determining the optimal placement of the carpet.

### 2. Merge Adjacent Tiles

Although the problem states that the tiles are non-overlapping, it does not specify whether they can be adjacent. If two tiles are adjacent, they can be treated as a single continuous tile for the purpose of carpet placement. Thus, merging adjacent tiles can simplify the problem.

### 3. Sliding Window Approach

The essence of the problem is to find the maximum number of consecutive white tiles that can be covered by a carpet of length `carpetLen`. This can be approached as a sliding window problem, where the window represents the carpet covering the tiles. The goal is to slide this window across the tiles to find the position where it covers the maximum number of white tiles.

#### Key Points for the Sliding Window Approach:

- **Window Size:** The size of the window is fixed, equivalent to `carpetLen`.
- **Window Movement:** The window moves across the sorted and merged tiles. At each step, it shifts rightwards, attempting to cover more white tiles.
- **Counting Tiles:** As the window moves, calculate the number of white tiles within the window. This involves summing the lengths of the tiles (or portions thereof) that fall within the window.
- **Updating Maximum:** Keep track of the maximum number of tiles covered at any point during the sliding window's movement. This is the potential answer to the problem.

### 4. Handling Gaps

There might be gaps between tiles that the carpet spans but does not cover any white tiles. It's essential to account for these gaps in the sliding window's movement, as they do not contribute to the count of covered white tiles but do consume part of the carpet's length.

### 5. Optimization Techniques

To optimize, we can use two pointers or indices to represent the start and end of the carpet (window). As we move the end pointer to the right, we adjust the start pointer to ensure the window size does not exceed `carpetLen`. This adjustment accounts for both the length of tiles covered and the gaps between them.

### Conclusion

By applying these steps, especially the sliding window technique with attention to gaps and the optimization of the window's placement, we can efficiently find the maximum number of white tiles that can be covered by the carpet. This approach balances the need to cover as many white tiles as possible while adjusting for gaps and ensuring the carpet is used optimally.
```

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/leetcode_2209.txt
> def minVisibleWhiteTiles(floor: str, numCarpets: int, carpetLen: int) - int:<br>    """You are given a 0-indexed binary string `floor`, which represents the colors of tiles on a floor:<br><br>   `floor[i] = '0'` denotes that the `ith` tile of the floor is colored black.<br>   On the other hand, `floor[i] = '1'` denotes that the `ith` tile of the floor is colored white.<br><br>You are also given `numCarpets` and `carpetLen`. You have `numCarpets` black carpets, each of length `carpetLen` tiles. Cover the tiles with the given carpets such that the number of white tiles still visible is minimum. Carpets may overlap one another.<br><br>Return _the minimum number of white tiles still visible._<br><br>Example 1:<br><br>Input: floor =  "10110101 ", numCarpets = 2, carpetLen = 2<br>Output: 2<br>Explanation: <br>The figure above shows one way of covering the tiles with the carpets such that only 2 white tiles are visible.<br>No other way of covering the tiles with the carpets can leave less than 2 white tiles visible.<br><br>Example 2:<br><br>Input: floor =  "11111 ", numCarpets = 2, carpetLen = 3<br>Output: 0<br>Explanation: <br>The figure above shows one way of covering the tiles with the carpets such that no white tiles are visible.<br>Note that the carpets are able to overlap one another.<br><br>Constraints:<br><br>   `1 <= carpetLen <= floor.length <= 1000`<br>   `floor[i]` is either `'0'` or `'1'`.<br>   `1 <= numCarpets <= 1000`"""<br><br>    n, res = len(floor), len(floor)<br>    for i in range(n - carpetLen + 1):<br>        count = sum(1 for j in range(carpetLen) if floor[i + j] == '1')<br>        for k in range(1, numCarpets + 1):<br>            if k  carpetLen - 1 <= i + carpetLen:<br>                res = min(res, n - count  k)<br>                if i + carpetLen < n:<br>                    count -= floor[i + carpetLen - k  carpetLen] == '1'<br>                    count += floor[i + carpetLen] == '1'<br>    return res

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_144360.txt
> def place(vertices_resources, nets, machine, constraints,<br>          effort=1.0, random=default_random, on_temperature_change=None,<br>          kernel=default_kernel, kernel_kwargs={}):<br>    """A flat Simulated Annealing based placement algorithm.<br><br>    This placement algorithm uses simulated annealing directly on the supplied<br>    problem graph with the objective of reducing wire lengths (and thus,<br>    indirectly, the potential for congestion). Though computationally<br>    expensive, this placer produces relatively good placement solutions.<br><br>    The annealing temperature schedule used by this algorithm is taken from<br>    "VPR: A New Packing, Placement and Routing Tool for FPGA Research" by<br>    Vaughn Betz and Jonathan Rose from the "1997 International Workshop on<br>    Field Programmable Logic and Applications".<br><br>    Two implementations of the algorithm's kernel are available:<br><br>     :py:class:`~rig.place_and_route.place.sa.python_kernel.PythonKernel` A<br>      pure Python implementation which is available on all platforms supported<br>      by Rig.<br>     :py:class:`~rig.place_and_route.place.sa.c_kernel.CKernel` A C<br>      implementation which is typically 50-150x faster than the basic Python<br>      kernel. Since this implementation requires a C compiler during<br>      installation, it is an optional feature of Rig. See the<br>      :py:class:`CKernel's documentation<br>      <rig.place_and_route.place.sa.c_kernel.CKernel` for details.<br><br>    The fastest kernel installed is used by default and can be manually chosen<br>    using the ``kernel`` argument.<br><br>    This algorithm produces INFO level logging information describing the<br>    progress made by the algorithm.<br><br>    .. warning:<br>        This algorithm does not attempt to produce good solutions to the<br>        bin-packing problem of optimally fitting vertices into chips and it may<br>        fail if a good placement requires good bin packing.<br><br>    Parameters<br>    ----------<br>    effort : float<br>        A scaling factor for the number of iterations the algorithm should run<br>        for. 1.0 is probably about as low as you'll want to go in practice and<br>        runtime increases linearly as you increase this parameter.<br>    random : :py:class:`random.Random`<br>        A Python random number generator. Defaults to ``import random`` but can<br>        be set to your own instance of :py:class:`random.Random` to allow you<br>        to control the seed and produce deterministic results. For results to<br>        be deterministic, vertices_resources must be supplied as an<br>        :py:class:`collections.OrderedDict`.<br>    on_temperature_change : callback_function or None<br>        An (optional) callback function which is called every time the<br>        temperature is changed. This callback can be used to provide status<br>        updates<br><br>        The callback function is passed the following arguments:<br><br>         ``iteration_count``: the number of iterations the placer has<br>          attempted (integer)<br>         ``placements``: The current placement solution.<br>         ``cost``: the weighted sum over all nets of bounding-box size.<br>          (float)<br>         ``acceptance_rate``: the proportion of iterations which have resulted<br>          in an accepted change since the last callback call. (float between<br>          0.0 and 1.0)<br>         ``temperature``: The current annealing temperature. (float)<br>         ``distance_limit``: The maximum distance any swap may be made over.<br>          (integer)<br><br>        If the callback returns False, the anneal is terminated immediately and<br>        the current solution is returned.<br>    kernel : :py:class:`~rig.place_and_route.place.sa.kernel.Kernel`<br>        A simulated annealing placement kernel. A sensible default will be<br>        chosen based on the available kernels on this machine. The kernel may<br>        not be used if the placement problem has a trivial solution.<br>    kernel_kwargs : dict<br>        Optional kernel-specific keyword arguments to pass to the kernel<br>        constructor.<br>    """<br>    # Special case: just return immediately when there's nothing to place<br>    if len(vertices_resources) == 0:<br>        return {}<br><br>    # Within the algorithm we modify the resource availability values in the<br>    # machine to account for the effects of the current placement. As a result,<br>    # an internal copy of the structure must be made.<br>    machine = machine.copy()<br><br>    # {vertex: (x, y), ...} gives the location of all vertices whose position<br>    # is fixed by a LocationConstraint.<br>    fixed_vertices = {}<br><br>    # Handle constraints<br>    vertices_resources, nets, constraints, substitutions = \<br>        apply_same_chip_constraints(vertices_resources, nets, constraints)<br>    for constraint in constraints:<br>        if isinstance(constraint, LocationConstraint):<br>            # Location constraints are handled by recording the set of fixed<br>            # vertex locations and subtracting their resources from the chips<br>            # they're allocated to. These vertices will then not be added to<br>            # the internal placement data structure to prevent annealing from<br>            # moving them. They will be re-introduced at the last possible<br>            # moment.<br>            location = constraint.location<br>            if location not in machine:<br>                raise InvalidConstraintError(<br>                    "Chip requested by {} unavailable".format(machine))<br>            vertex = constraint.vertex<br><br>            # Record the constrained vertex's location<br>            fixed_vertices[vertex] = location<br><br>            # Make sure the vertex fits at the requested location (updating the<br>            # resource availability after placement)<br>            resources = vertices_resources[vertex]<br>            machine[location] = subtract_resources(machine[location],<br>                                                   resources)<br>            if overallocated(machine[location]):<br>                raise InsufficientResourceError(<br>                    "Cannot meet {}".format(constraint))<br>        elif isinstance(constraint,  # pragma: no branch<br>                        ReserveResourceConstraint):<br>            apply_reserve_resource_constraint(machine, constraint)<br><br>    # Initially randomly place the movable vertices<br>    movable_vertices = {v for v in vertices_resources<br>                        if v not in fixed_vertices}<br>    initial_placements = _initial_placement(movable_vertices,<br>                                            vertices_resources,<br>                                            machine, random)<br><br>    # Include the fixed vertices in initial placement<br>    initial_placements.update(fixed_vertices)<br><br>    # Filter out empty or singleton nets and those weighted as zero since they<br>    # cannot influence placement.<br>    nets = [n for n in nets if len(set(n))  1 and n.weight  0.0]<br><br>    # Special cases where no placement effort is required:<br>    #  There is only one chip<br>    #  There are no resource types to be consumed<br>    #  No effort is to be made<br>    #  No movable vertices<br>    #  There are no nets (and moving things has no effect)<br>    trivial = ((machine.width, machine.height) == (1, 1) or<br>               len(machine.chip_resources) == 0 or<br>               effort == 0.0 or<br>               len(movable_vertices) == 0 or<br>               len(nets) == 0)<br>    if trivial:<br>        logger.info("Placement has trivial solution. SA not used.")<br>        finalise_same_chip_constraints(substitutions, initial_placements)<br>        return initial_placements<br><br>    # Intialise the algorithm kernel<br>    k = kernel(vertices_resources, movable_vertices, set(fixed_vertices),<br>               initial_placements, nets, machine, random, kernel_kwargs)<br><br>    logger.info("SA placement kernel: %s", kernel.__name__)<br><br>    # Specifies the maximum distance any swap can span. Initially consider<br>    # swaps that span the entire machine.<br>    distance_limit = max(machine.width, machine.height)<br><br>    # Determine initial temperature according to the heuristic used by VPR: 20<br>    # times the standard deviation of len(movable_vertices) random swap costs.<br>    # The arbitrary very-high temperature is used to cause "all" swaps to be<br>    # accepted.<br>    _0, _1, cost_delta_sd = k.run_steps(len(movable_vertices),<br>                                        distance_limit,<br>                                        1e100)<br>    temperature = 20.0  cost_delta_sd<br><br>    # The number of swap-attempts between temperature changes is selected by<br>    # the heuristic used by VPR. This value is scaled linearly by the effort<br>    # parameter.<br>    num_steps = max(1, int(effort  len(vertices_resources)1.33))<br><br>    logger.info("Initial placement temperature: %0.1f", temperature)<br><br>    # Counter for the number of swap attempts made (used for diagnostic<br>    # purposes)<br>    iteration_count = 0<br><br>    # Holds the total cost of the current placement. This default value chosen<br>    # to ensure the loop below iterates at least once.<br>    current_cost = 0.0<br><br>    # The annealing algorithm runs until a heuristic termination condition<br>    # (taken from VPR) is hit. The heuristic waits until the temperature falls<br>    # below a small fraction of the average net cost.<br>    while temperature  (0.005  current_cost) / len(nets):<br>        # Run an iteration at the current temperature<br>        num_accepted, current_cost, _ = k.run_steps(<br>            num_steps, int(math.ceil(distance_limit)), temperature)<br><br>        # The ratio of accepted-to-not-accepted changes<br>        r_accept = num_accepted / float(num_steps)<br><br>        # Special case: Can't do better than 0 cost! This is a special case<br>        # since the normal termination condition will not terminate if the cost<br>        # doesn't drop below 0.<br>        if current_cost == 0:<br>            break<br><br>        # The temperature is reduced by a factor heuristically based on the<br>        # acceptance rate. The schedule below attempts to maximise the time<br>        # spent at temperatures where a large portion (but not all) of changes<br>        # are being accepted. If lots of changes are being accepted (e.g.<br>        # during high-temperature periods) then most of them are likely not to<br>        # be beneficial. If few changes are being accepted, we're probably<br>        # pretty close to the optimal placement.<br>        if r_accept  0.96:<br>            alpha = 0.5<br>        elif r_accept  0.8:<br>            alpha = 0.9<br>        elif r_accept  0.15:<br>            alpha = 0.95<br>        else:<br>            alpha = 0.8<br>        temperature = alpha  temperature<br><br>        # According to:<br>        #  M. Huang, F. Romeo, and A. Sangiovanni-Vincentelli, "An Efficient<br>        #   General Cooling Schedule for Simulated Annealing" ICCAD, 1986, pp.<br>        #   381 - 384 and J. Lam<br>        #  J. Delosme, "Performance of a New Annealing Schedule" DAC, 1988,<br>        #   pp. 306 - 311.<br>        # It is desirable to keep the acceptance ratio as close to 0.44 for as<br>        # long as possible. As a result, when r_accept falls below this we can<br>        # help increase the acceptance rate by reducing the set of possible<br>        # swap candidates based on the observation that near the end of<br>        # placement, most things are near their optimal location and thus long<br>        # distance swaps are unlikely to be useful.<br>        distance_limit = 1.0 - 0.44 + r_accept<br>        distance_limit = min(max(distance_limit, 1.0),<br>                             max(machine.width, machine.height))<br><br>        iteration_count += num_steps<br>        logger.debug("Iteration: %d, "<br>                     "Cost: %0.1f, "<br>                     "Kept: %0.1f%%, "<br>                     "Temp: %0.3f, "<br>                     "Dist: %d.",<br>                     iteration_count, current_cost,<br>                     r_accept100, temperature, math.ceil(distance_limit))<br><br>        # Call the user callback before the next iteration, terminating if<br>        # requested.<br>        if on_temperature_change is not None:<br>            placements = k.get_placements().copy()<br>            finalise_same_chip_constraints(substitutions, placements)<br>            ret_val = on_temperature_change(iteration_count,<br>                                            placements,<br>                                            current_cost,<br>                                            r_accept,<br>                                            temperature,<br>                                            distance_limit)<br>            if ret_val is False:<br>                break<br><br>    logger.info("Anneal terminated after %d iterations.", iteration_count)<br><br>    placements = k.get_placements()<br>    finalise_same_chip_constraints(substitutions, placements)<br><br>    return placements

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_381694.txt
> def time_multi_coincidence(times, slide_step=0, slop=.003,<br>                           pivot='H1', fixed='L1'):<br>    """ Find multi detector concidences.<br><br>    Parameters<br>    ----------<br>    times: dict of numpy.ndarrays<br>        Dictionary keyed by ifo of the times of each single detector trigger.<br>    slide_step: float<br>        The interval between time slides<br>    slop: float<br>        The amount of time to add to the TOF between detectors for coincidence<br>    pivot: str<br>        ifo used to test coincidence against in first stage<br>    fixed: str<br>        the other ifo used in the first stage coincidence which we'll use<br>        as a fixed time reference for coincident triggers. All other detectors<br>        are time slid by being fixed to this detector.<br>    """<br>    # pivots are used to determine standard coincidence triggers, we then<br>    # pair off additional detectors to those.<br>    def win(ifo1, ifo2):<br>        d1 = Detector(ifo1)<br>        d2 = Detector(ifo2)<br>        return d1.light_travel_time_to_detector(d2) + slop<br><br>    # Find coincs first between the two fully time-slid detectors<br>    pivot_id, fix_id, slide = time_coincidence(times[pivot], times[fixed],<br>                                               win(pivot, fixed),<br>                                               slide_step=slide_step)<br><br>    # additional detectors do not slide independently of the fixed one<br>    # Each trigger in an additional detector must be concident with an<br>    # existing coincident one. All times moved to 'fixed' relative time<br>    fixed_time = times[fixed][fix_id]<br>    pivot_time = times[pivot][pivot_id] - slide_step  slide<br><br>    ctimes = {fixed: fixed_time, pivot:pivot_time}<br>    ids = {fixed:fix_id, pivot:pivot_id}<br><br>    dep_ifos = [ifo for ifo in times.keys() if ifo != fixed and ifo != pivot]<br>    for ifo1 in dep_ifos:<br>        otime = times[ifo1]<br>        sort = times[ifo1].argsort()<br>        time = otime[sort]<br><br>        # Find coincidences between dependent ifo triggers and existing coinc.<br>        for ifo2 in ids.keys():<br>            # Currently assumes that additional detectors do not slide<br>            # independently of the 'fixed one'<br>            #<br>            # To modify that assumption, the code here would be modified<br>            # by adding a function that remaps the coinc time frame and unmaps<br>            # it and the end of this loop.<br>            # This remapping must ensure<br>            #     function of the standard slide number<br>            #     ensure all times remain within coincident segment<br>            #     unbiased distribution of triggers after mapping.<br><br>            w = win(ifo1, ifo2)<br>            left = numpy.searchsorted(time, ctimes[ifo2] - w)<br>            right = numpy.searchsorted(time, ctimes[ifo2] + w)<br><br>            # remove elements that will not form a coinc<br>            # There is only at most one trigger for an existing coinc<br>            # (assumes triggers spaced  slide step)<br>            nz = (right - left).nonzero()<br>            dep_ids = left[nz]<br><br>            # The property that only one trigger can be within the window is ensured<br>            # by the peak finding algorithm we use for each template.<br>            # If that is modifed, this function may need to be<br>            # extended.<br>            if len(left)  0 and (right - left).max()  1:<br>                raise ValueError('Somehow triggers are closer than time-delay window')<br><br>            slide = slide[nz]<br>            for ifo in ctimes:<br>                ctimes[ifo] = ctimes[ifo][nz]<br>                ids[ifo] = ids[ifo][nz]<br><br>        # Add this detector now to the cumulative set and proceed to the next<br>        # ifo coincidence test<br>        ids[ifo1] = sort[dep_ids]<br>        ctimes[ifo1] = otime[ids[ifo1]]<br><br>    return ids, slide

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_5201.txt
> def basic_params1():<br>  """A set of basic hyperparameters."""<br>  return hparam.HParams(<br>      # If the problem consists of variable-length sequences<br>      # (see problem.batch_size_means_tokens()), then this is the number<br>      # of tokens per batch per GPU or per TPU core.  Otherwise, this is<br>      # the number of examples per GPU or per TPU core.<br>      batch_size=4096,<br>      batch_shuffle_size=512,<br>      # If True, then if the features are of variable length, the batch_size is<br>      # used as the actual batch size (and not tokens per batch).<br>      use_fixed_batch_size=False,<br>      num_hidden_layers=4,<br>      kernel_height=3,<br>      kernel_width=1,<br>      hidden_size=64,<br>      compress_steps=0,<br>      # All hyperparameters ending in "dropout" are automatically set to 0.0<br>      # when not in training mode.<br>      dropout=0.2,<br>      clip_grad_norm=2.0,<br>      grad_noise_scale=0.0,<br>      summarize_grads=False,<br>      # Flag for whether mlperf mode is on<br>      mlperf_mode=False,<br>      # Whether to log the name and size of every variable<br>      summarize_vars=False,<br>      initializer="orthogonal",<br>      initializer_gain=1.5,<br>      label_smoothing=0.1,<br>      optimizer="adam",<br>      optimizer_adam_epsilon=1e-6,<br>      optimizer_adam_beta1=0.85,<br>      optimizer_adam_beta2=0.997,<br>      optimizer_momentum_momentum=0.9,<br>      optimizer_momentum_nesterov=False,<br>      optimizer_adafactor_beta1=0.0,<br>      optimizer_adafactor_beta2=0.999,<br>      optimizer_adafactor_factored=True,<br>      optimizer_adafactor_decay_type="pow",<br>      optimizer_adafactor_memory_exponent=0.8,<br>      optimizer_adafactor_clipping_threshold=1.0,<br>      optimizer_adafactor_multiply_by_parameter_scale=True,<br>      # Number of accumulating steps for multi step optimizers.<br>      optimizer_multistep_accumulate_steps=0,<br>      # Loss scaling used.<br>      # Generally only necessary with mixed precision training.<br>      # Mixed precision training only supports exponential scaling currently<br>      # To disable the scaler, see to 0/False<br>      mixed_precision_optimizer_loss_scaler="exponential",<br>      # Determines the initial loss scaling value for mixed precision<br>      mixed_precision_optimizer_init_loss_scale=215,<br>      # Whether to zero gradients that were not computed, so that the<br>      # appropriate slots are created. Useful for sharing checkpoints between<br>      # models with different sets of heads.<br>      optimizer_zero_grads=False,<br>      weight_decay=1e-6,<br>      weight_noise=0.0,<br>      # Defines the learning rate as a product of named functions.<br>      # Available functions are listed in learning_rate._LEARNING_RATE_FUNCTIONS<br>      # e.g. "constantlinear_warmuprsqrt_decayrsqrt_hidden_size"<br>      learning_rate_schedule="legacy",<br>      learning_rate_constant=1.0,<br>      # If learning_rate_schedule=="legacy",<br>      # then we specify decay scheme here.  Warmup is always exponential,<br>      # except with "noam" learning rate decay scheme.<br>      # see optimize.legacy_learning_rate_schedule()<br>      # TODO(noam): migrate everyone away from this.<br>      learning_rate_decay_scheme="none",<br>      # decay_steps and decay_staircase for learning_rate_decay_scheme=="exp"<br>      learning_rate_decay_steps=5000,<br>      learning_rate_decay_staircase=False,<br>      learning_rate_minimum=None,<br>      learning_rate_decay_rate=1.0,<br>      learning_rate_warmup_steps=100,<br>      learning_rate_cosine_cycle_steps=250000,<br>      learning_rate=0.1,<br>      sampling_method="argmax",  # "argmax" or "random"<br>      sampling_temp=1.0,  # temperature for sampling<br>      sampling_keep_top_k=-1,  # If 0, ignore all but the top k logits<br>      # expand the logits a piece at a time - saves memory.<br>      factored_logits=False,<br>      multiply_embedding_mode="sqrt_depth",<br>      # Parameters related to mixtures of experts.<br>      moe_hidden_sizes="2048",  # hidden layer sizes (comma-separated)<br>      moe_num_experts=64,  # number of experts per layer<br>      moe_k=2,  # how many experts to use for each batch element<br>      moe_loss_coef=1e-2,<br>      # Sequences of operations to perform on layer input and layer output.<br>      # Used by common_layers.layer_preprocess, common_layers.layer_postprocess<br>      # Each character represents an operation:<br>      # none: no preprocessing<br>      #    d: apply dropout<br>      #    n: apply normalization (see norm_type and norm_epsilon)<br>      #    a: add layer input (residual connection - only during postprocess)<br>      # The special string "none" is used instead of the empty string<br>      # to indicate no pre/postprocessing, since the empty string causes<br>      # trouble for hyperparameter tuning.<br>      # TODO(noam): The current settings ("", "dan") are the published version<br>      # of the transformer.  ("n", "da") seems better for harder-to-learn<br>      # models, so it should probably be the default.<br>      layer_preprocess_sequence="none",<br>      layer_postprocess_sequence="dan",<br>      # dropout rate to use during layer_preprocess and layer_postprocess<br>      layer_prepostprocess_dropout=0.1,<br>      # broadcast dimensions for layer_prepostprocess_dropout<br>      # a comma-separated list of integers.<br>      # see common_layers.dropout_with_broadcast_dims()<br>      # Change this to "1" to save memory.<br>      layer_prepostprocess_dropout_broadcast_dims="",<br>      # dropout some symbols (set them to 0) before embedding.<br>      symbol_dropout=0.0,<br>      # What type of normalization to use<br>      norm_type="layer",  # "batch", layer", "noam", "none".<br>      # epsilon parameter to normalization function<br>      norm_epsilon=1e-6,<br>      # pad vocabularies so that this value divides the vocabulary size.<br>      vocab_divisor=1,<br>      # During training, we drop sequences whose inputs and targets are shorter<br>      # than min_length<br>      min_length=0,<br>      # During training, we drop sequences whose inputs or targets are longer<br>      # than max_length.<br>      # If max_length==0, we use hparams.batch_size instead.<br>      max_length=0,<br>      # Pack examples on the fly.<br>      pack_dataset=False,<br>      # Use custom ops not included in standard tensorflow.<br>      use_custom_ops=True,<br>      # Split targets on the first axis into chunks of this length.<br>      split_targets_chunk_length=0,<br>      split_targets_max_chunks=100,<br>      split_targets_strided_training=False,<br>      # Maximum length in the smallest length bucket.  Setting this<br>      # flag too high will result in wasteful padding of short<br>      # sequences.  Due to some (hopefully) temporary hacks in the<br>      # data reading and batching code, setting this flag too low<br>      # results in a very long batch-shuffling queue.<br>      # TODO(noam): change this once the Datasets API changes.<br>      min_length_bucket=8,<br>      # This flag controls the number of length buckets in the data<br>      # reader.  The buckets have maximum lengths from<br>      # min_bucket_length to (max_length or batch_size), increasing<br>      # (approximately) by factors of length_bucket_step.<br>      length_bucket_step=1.1,<br>      # If set to True, drop sequences longer than max_length during eval.<br>      # This affects the validity of the evaluation metrics.<br>      eval_drop_long_sequences=False,<br>      # If True, run the model autoregressively instead of teacher-forcing<br>      # during eval<br>      eval_run_autoregressive=False,<br>      # (For features with symbol modality) If True, share all of the<br>      # input embeddings, target embeddings, and softmax weights.<br>      shared_embedding_and_softmax_weights=False,<br>      # (For features with symbol modality) If True, share the input embeddings<br>      # and target embeddings.<br>      shared_embedding=False,<br>      # (For features with symbol modality) Number to shard embeddings by.<br>      symbol_modality_num_shards=1,<br>      # Feature transformations are optional dictionaries comprising key-value<br>      # pairs of a feature name (str) and its transformation (function). If not<br>      # specified, T2TModel applies a default transformation according to the<br>      # feature's modality. Bottom is applicable to all features; loss, top, and<br>      # weights_fn are only applicable to target features.<br>      # TODO(trandustin): `name` is an optional hparam for legacy reasons,<br>      # defining variable scope names. Remove this hparam in the future.<br>      bottom={},<br>      loss={},<br>      name={},<br>      top={},<br>      weights_fn={},<br>      # The maximum length of "input" sequence.<br>      # Sequences longer than this value will be truncated. 0 or negative values<br>      # mean there is no maximum or truncation.<br>      # You can change this behavior by overriding preprocess_example() method<br>      # in your problem class.<br>      max_input_seq_length=0,<br>      # The maximum length of "target" sequence.<br>      # Sequences longer than this value will be truncated. 0 or negative values<br>      # mean there is no maximum or truncation.<br>      # You can change this behavior by overriding preprocess_example() method<br>      # in your problem class.<br>      max_target_seq_length=0,<br>      # if nonzero, we split the target sequences on example read.<br>      # This is for use with language modeling problems with fixed length<br>      # examples.  e.g.  The examples may be written with length 65536, but we<br>      # want to split each example into 64 examples of length 1024.<br>      split_to_length=0,<br>      # Video settings: how many frames to batch on input and targets.<br>      video_num_input_frames=1,<br>      video_num_target_frames=1,<br>      # This flag allows us to optionally treat a seq-to-seq problem<br>      # as a language model.  Legal values are:<br>      #<br>      # "none" - Do not prepend the inputs to the targets.<br>      # "prepend_inputs_masked_attention"<br>      #     replace "targets" in preprocessing with<br>      #     tf.concat([inputs, [0], targets], axis=1)<br>      #     i.e. we prepend the inputs to the targets with a single<br>      #     padding token in between.  Use masked self-attention on the<br>      #     entire resulting sequence.  During training, we compute losses on<br>      #     the combined sequence.  During eval, we compute the metrics<br>      #     on only the targets portion.<br>      # "prepend_inputs_full_attention"<br>      #     similar to the previous option except that each<br>      #     position in the inputs portion can see the<br>      #     entire inputs portion.  This removes the challenge of<br>      #     autoregressively predicting the inputs portion.<br>      prepend_mode="none",<br>      # Scheduled sampling is interesting for auto-regressive models.<br>      # It runs an additional step using the generated output as autoregressive<br>      # targets, which can improve the models inference results later. The<br>      # parameter scheduled_sampling_prob determines with what probability<br>      # will such additional step be run. It's turned off (0.0) by default.<br>      # This probability will exponentially warm up for the number of<br>      # steps determined by scheduled_sampling_warmup_steps.<br>      # The tensor used for the n-th pass will consist of outputs from<br>      # the (n-1)-th pass mixed with gold truth, with the proportion of gold<br>      # determined by scheduled_sampling_gold_mixin_prob. Control the number<br>      # of passes with scheduled_sampling_num_passes.<br>      scheduled_sampling_prob=0.0,<br>      scheduled_sampling_warmup_steps=50000,<br>      scheduled_sampling_gold_mixin_prob=0.5,<br>      # TODO(duckworthd): Uncomment when we can ascertain why adding an<br>      # extra field to HParam causes test failures.<br>      # scheduled_sampling_num_passes=1,<br><br>      # This setting controls whether to copy variables around in a daisy chain<br>      # (if true) or leave their placement to TensorFlow. It only affects multi<br>      # device training and mostly should be turned on for performance. One<br>      # exception are recurrent models: with dynamic loops it must be off.<br>      daisy_chain_variables=True,<br>      # If True in PREDICT mode, then last-position-only optimizations are not<br>      # used.<br>      force_full_predict=False,<br>      # Set this for pure model parallelism.  There is only one data shard.<br>      no_data_parallelism=False,<br>      # dtype used for activations. - "float32" or "bfloat16"<br>      # activation_dtype="bfloat16" currently only works on TPU.<br>      #    It lowers activation-memory usage<br>      #    and does not appear to affect quality.<br>      #    You can train on TPU with activation_dtype="bfloat16" and evaluate<br>      #    on CPU/GPU with activation_dtype="float32"<br>      activation_dtype="float32",<br>      # dtype used for parameters: "float32" or "bfloat16"<br>      # bfloat16 currently only works with optimizer="adafactor".<br>      #   The savings in memory allow for training larger models.<br>      #   Weights are encoded as (w128)^8, using pseudostochastic<br>      #   roundoff.  Initial experiments show that model quality is similar<br>      #   to baseline for about 3M training steps, but worse thereafter.<br>      weight_dtype="float32",<br>      # Directory containing a checkpoint for a pretrained model. This will only<br>      # be used if a new run is being started. Parameters not found in the<br>      # pretrained model will be randomly initialized. Superfluous parameters in<br>      # the pretrained model will be ignored.<br>      pretrained_model_dir="",<br>      # Threshold used for two cases: the primary task probability for the<br>      # constant mixing schedule, and the exponential schedule limit for when<br>      # mixing should stop (eg: 0.5 means stop at 50-50 mixing, 0.8 means stop<br>      # at 20-80 mixing for the primary-others mixing case.)<br>      multiproblem_schedule_threshold=0.5,<br>      # For more than 2 tasks, we may want to specify per-task thresholds here.<br>      # In that case, this needs to be a string with as many floating point<br>      # numbers as the number of tasks in the multi-problem. These numbers<br>      # are later normalized to add up to 1 and taken as probabilities for<br>      # each task. This enforces a constant mixing schedule and if this is<br>      # empty then the threshold from above is used for the first task and<br>      # the other tasks get the remaining probability split uniformly.<br>      multiproblem_per_task_threshold="",<br>      # The number of examples at which the proportion of the mixed in datasets<br>      # is multiproblem_schedule_threshold<br>      multiproblem_schedule_max_examples=1e7,<br>      # When training multiproblems, we can mix the data according to different<br>      # schedules. Example: a constant schedule mixing 20-80 between the primary<br>      # and other tasks.<br>      # A list of supported schedules can be found in<br>      # `data_generators.multi_problem.py`.<br>      multiproblem_mixing_schedule="constant",<br>      # A boolean that decides whether input sequence losses and target label<br>      # losses in classification problems should be reweighted.<br>      multiproblem_reweight_label_loss=False,<br>      # How much weight the targets in classification problems receive. Inputs<br>      # receive 1 minus this weight.<br>      multiproblem_label_weight=0.5,<br>      # Hyperparameters for relative attention.<br>      # The maximum relative positional distance to learn an embedding for.<br>      max_relative_position=0,<br>      # If heads share the same relative embedding.<br>      heads_share_relative_embedding=False,<br>      # If relative embedding terms are added to values too.<br>      add_relative_to_values=False,<br>      # If enable the host_call which is executed every training step.<br>      # There could be a performance drop if host_call function is slow and<br>      # cannot keep up with the TPU-side computation.<br>      tpu_enable_host_call=False,<br>      # Pad batch dim of inputs to nearest multiple of batch multiple.<br>      pad_batch=False,<br>      # When true, do not evaluate on the language model data when running the<br>      # multiproblem since it can take a while. If False, set eval_steps to<br>      # something large like 6000 or 10000.<br>      multiproblem_target_eval_only=False,<br>      # Max out the vocab size to a power of 2 for efficiency and to reserve<br>      # extra space in the vocabulary for new task ids and label classes.<br>      multiproblem_vocab_size=-1,<br>      # When using multiproblem with generation tasks, need to truncate the<br>      # inputs and targets manually before concatenating them.<br>      multiproblem_max_input_length=-1,<br>      multiproblem_max_target_length=-1,<br>      # If positive, makes training targets fixed-length in MultiProblem.<br>      multiproblem_fixed_train_length=-1,<br>      # Load weights from a second model. For instance, when using<br>      # pre-trained weights, you might want to initialize the encoder<br>      # and decoder by loading different models.<br>      warm_start_from_second="",<br>      # Area attention hyper parameters<br>      area_value_mode="none",<br>      area_key_mode="none",<br>      # Using area attention for the number of layers from the bottom<br>      num_area_layers=0,<br>      max_area_width=1,<br>      max_area_height=1,<br>      memory_height=1<br>  )

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_312182.txt
> def detect(self, stream, threshold, threshold_type, trig_int, plotvar,<br>               daylong=False, parallel_process=True, xcorr_func=None,<br>               concurrency=None, cores=None, ignore_length=False,<br>               group_size=None, overlap="calculate", debug=0,<br>               full_peaks=False, save_progress=False,<br>               process_cores=None, kwargs):<br>        """<br>        Detect using a Tribe of templates within a continuous stream.<br><br>        :type stream: `obspy.core.stream.Stream`<br>        :param stream: Continuous data to detect within using the Template.<br>        :type threshold: float<br>        :param threshold:<br>            Threshold level, if using `threshold_type='MAD'` then this will be<br>            the multiple of the median absolute deviation.<br>        :type threshold_type: str<br>        :param threshold_type:<br>            The type of threshold to be used, can be MAD, absolute or<br>            av_chan_corr.  See Note on thresholding below.<br>        :type trig_int: float<br>        :param trig_int:<br>            Minimum gap between detections in seconds. If multiple detections<br>            occur within trig_int of one-another, the one with the highest<br>            cross-correlation sum will be selected.<br>        :type plotvar: bool<br>        :param plotvar:<br>            Turn plotting on or off, see warning about plotting below<br>        :type daylong: bool<br>        :param daylong:<br>            Set to True to use the<br>            :func:`eqcorrscan.utils.pre_processing.dayproc` routine, which<br>            preforms additional checks and is more efficient for day-long data<br>            over other methods.<br>        :type parallel_process: bool<br>        :param parallel_process:<br>        :type xcorr_func: str or callable<br>        :param xcorr_func:<br>            A str of a registered xcorr function or a callable for implementing<br>            a custom xcorr function. For more information see:<br>            :func:`eqcorrscan.utils.correlate.register_array_xcorr`<br>        :type concurrency: str<br>        :param concurrency:<br>            The type of concurrency to apply to the xcorr function. Options are<br>            'multithread', 'multiprocess', 'concurrent'. For more details see<br>            :func:`eqcorrscan.utils.correlate.get_stream_xcorr`<br>        :type cores: int<br>        :param cores: Number of workers for procesisng and detection.<br>        :type ignore_length: bool<br>        :param ignore_length:<br>            If using daylong=True, then dayproc will try check that the data<br>            are there for at least 80% of the day, if you don't want this check<br>            (which will raise an error if too much data are missing) then set<br>            ignore_length=True.  This is not recommended!<br>        :type group_size: int<br>        :param group_size:<br>            Maximum number of templates to run at once, use to reduce memory<br>            consumption, if unset will use all templates.<br>        :type overlap: float<br>        :param overlap:<br>            Either None, "calculate" or a float of number of seconds to<br>            overlap detection streams by.  This is to counter the effects of<br>            the delay-and-stack in calculating cross-correlation sums. Setting<br>            overlap = "calculate" will work out the appropriate overlap based<br>            on the maximum lags within templates.<br>        :type debug: int<br>        :param debug:<br>            Debug level from 0-5 where five is more output, for debug levels<br>            4 and 5, detections will not be computed in parallel.<br>        :type full_peaks: bool<br>        :param full_peaks: See `eqcorrscan.utils.findpeak.find_peaks2_short`<br>        :type save_progress: bool<br>        :param save_progress:<br>            Whether to save the resulting party at every data step or not.<br>            Useful for long-running processes.<br>        :type process_cores: int<br>        :param process_cores:<br>            Number of processes to use for pre-processing (if different to<br>            `cores`).<br><br>        :return:<br>            :class:`eqcorrscan.core.match_filter.Party` of Families of<br>            detections.<br><br>        .. Note::<br>            `stream` must not be pre-processed. If your data contain gaps<br>            you should NOT fill those gaps before using this method.<br>            The pre-process functions (called within) will fill the gaps<br>            internally prior to processing, process the data, then re-fill<br>            the gaps with zeros to ensure correlations are not incorrectly<br>            calculated within gaps. If your data have gaps you should pass a<br>            merged stream without the `fill_value` argument<br>            (e.g.: `stream = stream.merge()`).<br><br>        .. Note::<br>            Detections are not corrected for `pre-pick`, the<br>            detection.detect_time corresponds to the beginning of the earliest<br>            template channel at detection.<br><br>        .. warning::<br>            Picks included in the output Party.get_catalog() will not be<br>            corrected for pre-picks in the template.<br><br>        .. note::<br>            Data overlap:<br><br>            Internally this routine shifts and trims the data according to the<br>            offsets in the template (e.g. if trace 2 starts 2 seconds after<br>            trace 1 in the template then the continuous data will be shifted<br>            by 2 seconds to align peak correlations prior to summing).<br>            Because of this, detections at the start and end of continuous<br>            data streams may be missed.  The maximum time-period that<br>            might be missing detections is the maximum offset in the template.<br><br>            To work around this, if you are conducting matched-filter<br>            detections through long-duration continuous data, we suggest<br>            using some overlap (a few seconds, on the order of the maximum<br>            offset in the templates) in the continuous data.  You will then<br>            need to post-process the detections (which should be done anyway<br>            to remove duplicates).  See below note for how `overlap` argument<br>            affects data internally if `stream` is longer than the processing<br>            length.<br><br>        .. Note::<br>            If `stream` is longer than processing length, this routine will<br>            ensure that data overlap between loops, which will lead to no<br>            missed detections at data start-stop points (see above note).<br>            This will result in end-time not being strictly<br>            honoured, so detections may occur after the end-time set.  This is<br>            because data must be run in the correct process-length.<br><br>        .. note::<br>            Thresholding:<br><br>            MAD threshold is calculated as the:<br><br>            .. math::<br><br>                threshold {\\times} (median(abs(cccsum)))<br><br>            where :math:`cccsum` is the cross-correlation sum for a given<br>            template.<br><br>            absolute threshold is a true absolute threshold based on the<br>            cccsum value.<br><br>            av_chan_corr is based on the mean values of single-channel<br>            cross-correlations assuming all data are present as required for<br>            the template, e.g:<br><br>            .. math::<br><br>                av\_chan\_corr\_thresh=threshold \\times (cccsum /<br>                len(template))<br><br>            where :math:`template` is a single template from the input and the<br>            length is the number of channels within this template.<br>        """<br>        party = Party()<br>        template_groups = []<br>        for master in self.templates:<br>            for group in template_groups:<br>                if master in group:<br>                    break<br>            else:<br>                new_group = [master]<br>                for slave in self.templates:<br>                    if master.same_processing(slave) and master != slave:<br>                        new_group.append(slave)<br>                template_groups.append(new_group)<br>        # template_groups will contain an empty first list<br>        for group in template_groups:<br>            if len(group) == 0:<br>                template_groups.remove(group)<br>        # now we can compute the detections for each group<br>        for group in template_groups:<br>            group_party = _group_detect(<br>                templates=group, stream=stream.copy(), threshold=threshold,<br>                threshold_type=threshold_type, trig_int=trig_int,<br>                plotvar=plotvar, group_size=group_size, pre_processed=False,<br>                daylong=daylong, parallel_process=parallel_process,<br>                xcorr_func=xcorr_func, concurrency=concurrency, cores=cores,<br>                ignore_length=ignore_length, overlap=overlap, debug=debug,<br>                full_peaks=full_peaks, process_cores=process_cores, kwargs)<br>            party += group_party<br>            if save_progress:<br>                party.write("eqcorrscan_temporary_party")<br>        if len(party)  0:<br>            for family in party:<br>                if family is not None:<br>                    family.detections = family._uniq().detections<br>        return party


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_1456.txt
> def findTheCity(n, edges, distanceThreshold):<br>    """Given a string `s` and an integer `k`, return _the maximum number of vowel letters in any substring of_ `s` _with length_ `k`.<br><br>Vowel letters in English are `'a'`, `'e'`, `'i'`, `'o'`, and `'u'`.<br><br>Example 1:<br><br>Input: s =  "abciiidef ", k = 3<br>Output: 3<br>Explanation: The substring  "iii " contains 3 vowel letters.<br><br>Example 2:<br><br>Input: s =  "aeiou ", k = 2<br>Output: 2<br>Explanation: Any substring of length 2 contains 2 vowels.<br><br>Example 3:<br><br>Input: s =  "leetcode ", k = 3<br>Output: 2<br>Explanation:  "lee ",  "eet " and  "ode " contain 2 vowels.<br><br>Constraints:<br><br>   `1 <= s.length <= 105`<br>   `s` consists of lowercase English letters.<br>   `1 <= k <= s.length`"""<br><br>    INF = float("inf")<br>    dist = [[INF]  n for _ in range(n)]<br><br>    for i in range(n):<br>        dist[i][i] = 0<br><br>    for fr, to, w in edges:<br>        dist[fr][to] = dist[to][fr] = w<br><br>    for k in range(n):<br>        for i in range(n):<br>            for j in range(n):<br>                dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])<br><br>    res, minReachable = -1, n + 1<br>    for i in range(n):<br>        cnt = sum(1 for d in dist[i] if d <= distanceThreshold)<br>        if cnt <= minReachable:<br>            minReachable = cnt<br>            res = i<br><br>    return res
