# Query `32`

## Original Question

*You are keeping the scores for a baseball game with strange rules. At the beginning of the game, you start with an empty record.

You are given a list of strings `operations`, where `operations[i]` is the `ith` operation you must apply to the record and is one of the following:

   An integer `x`.
       Record a new score of `x`.
   `'+'`.
       Record a new score that is the sum of the previous two scores.
   `'D'`.
       Record a new score that is the double of the previous score.
   `'C'`.
       Invalidate the previous score, removing it from the record.

Return _the sum of all the scores on the record after applying all the operations_.

The test cases are generated such that the answer and all intermediate calculations fit in a 32-bit integer and that all operations are valid.

Example 1:

Input: ops = \[ "5 ", "2 ", "C ", "D ", "+ "\]
Output: 30
Explanation:
 "5 " - Add 5 to the record, record is now \[5\].
 "2 " - Add 2 to the record, record is now \[5, 2\].
 "C " - Invalidate and remove the previous score, record is now \[5\].
 "D " - Add 2 \ 5 = 10 to the record, record is now \[5, 10\].
 "+ " - Add 5 + 10 = 15 to the record, record is now \[5, 10, 15\].
The total sum is 5 + 10 + 15 = 30.

Example 2:

Input: ops = \[ "5 ", "-2 ", "4 ", "C ", "D ", "9 ", "+ ", "+ "\]
Output: 27
Explanation:
 "5 " - Add 5 to the record, record is now \[5\].
 "-2 " - Add -2 to the record, record is now \[5, -2\].
 "4 " - Add 4 to the record, record is now \[5, -2, 4\].
 "C " - Invalidate and remove the previous score, record is now \[5, -2\].
 "D " - Add 2 \ -2 = -4 to the record, record is now \[5, -2, -4\].
 "9 " - Add 9 to the record, record is now \[5, -2, -4, 9\].
 "+ " - Add -4 + 9 = 5 to the record, record is now \[5, -2, -4, 9, 5\].
 "+ " - Add 9 + 5 = 14 to the record, record is now \[5, -2, -4, 9, 5, 14\].
The total sum is 5 + -2 + -4 + 9 + 5 + 14 = 27.

Example 3:

Input: ops = \[ "1 ", "C "\]
Output: 0
Explanation:
 "1 " - Add 1 to the record, record is now \[1\].
 "C " - Invalidate and remove the previous score, record is now \[\].
Since the record is empty, the total sum is 0.

Constraints:

   `1 <= operations.length <= 1000`
   `operations[i]` is `"C "`, `"D "`, `"+ "`, or a string representing an integer in the range `[-3  104, 3  104]`.
   For operation `"+ "`, there will always be at least two previous scores on the record.
   For operations `"C "` and `"D "`, there will always be at least one previous score on the record.*

## gpt4 Reasoning

```markdown
The essential problem here is to manage and manipulate a list (or record) of integer scores based on a series of operations. These operations can add new scores, modify existing ones, or remove scores from the record. The goal is to compute the sum of all scores in the record after all operations have been applied.

To tackle this problem, we need to follow these steps for each operation in the given list:

1. **Integer `x`**: When the operation is a numeric string (which represents an integer), we need to convert this string to an integer and add it to the record. This operation increases the size of the record by one.

2. **`'+'`**: This operation requires us to look at the last two scores in the record, sum them up, and add this sum as a new score to the record. It's crucial to ensure that there are at least two scores in the record before performing this operation. This also increases the size of the record by one.

3. **`'D'`**: For the 'D' operation, we double the last score in the record and add this new score to the record. We must ensure there is at least one score in the record to double. This operation also increases the size of the record by one.

4. **`'C'`**: This operation removes the most recent score from the record. It's essential to check that the record isn't empty before attempting to remove a score. This operation decreases the size of the record by one.

After applying all the operations, we need to sum all the scores in the record to get the final result.

Here's a step-by-step approach to implementing the solution:

- Initialize an empty list or stack to serve as the record.
- Iterate through each operation in the given list of operations.
    - If the operation is an integer (after attempting to convert it), append it to the record.
    - If the operation is `'+'`, calculate the sum of the last two scores in the record and append this sum to the record.
    - If the operation is `'D'`, double the last score in the record and append this new score.
    - If the operation is `'C'`, remove the last score from the record.
- After processing all operations, sum all elements in the record to get the total score.

This approach ensures that we dynamically adjust the record based on each operation's requirements, ultimately allowing us to compute the total score efficiently. It's also worth noting that we need to handle potential errors or edge cases, such as attempting to perform operations without enough scores in the record (though the problem statement guarantees valid operations, it's a good practice in software development).
```

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_144360.txt
> def place(vertices_resources, nets, machine, constraints,<br>          effort=1.0, random=default_random, on_temperature_change=None,<br>          kernel=default_kernel, kernel_kwargs={}):<br>    """A flat Simulated Annealing based placement algorithm.<br><br>    This placement algorithm uses simulated annealing directly on the supplied<br>    problem graph with the objective of reducing wire lengths (and thus,<br>    indirectly, the potential for congestion). Though computationally<br>    expensive, this placer produces relatively good placement solutions.<br><br>    The annealing temperature schedule used by this algorithm is taken from<br>    "VPR: A New Packing, Placement and Routing Tool for FPGA Research" by<br>    Vaughn Betz and Jonathan Rose from the "1997 International Workshop on<br>    Field Programmable Logic and Applications".<br><br>    Two implementations of the algorithm's kernel are available:<br><br>     :py:class:`~rig.place_and_route.place.sa.python_kernel.PythonKernel` A<br>      pure Python implementation which is available on all platforms supported<br>      by Rig.<br>     :py:class:`~rig.place_and_route.place.sa.c_kernel.CKernel` A C<br>      implementation which is typically 50-150x faster than the basic Python<br>      kernel. Since this implementation requires a C compiler during<br>      installation, it is an optional feature of Rig. See the<br>      :py:class:`CKernel's documentation<br>      <rig.place_and_route.place.sa.c_kernel.CKernel` for details.<br><br>    The fastest kernel installed is used by default and can be manually chosen<br>    using the ``kernel`` argument.<br><br>    This algorithm produces INFO level logging information describing the<br>    progress made by the algorithm.<br><br>    .. warning:<br>        This algorithm does not attempt to produce good solutions to the<br>        bin-packing problem of optimally fitting vertices into chips and it may<br>        fail if a good placement requires good bin packing.<br><br>    Parameters<br>    ----------<br>    effort : float<br>        A scaling factor for the number of iterations the algorithm should run<br>        for. 1.0 is probably about as low as you'll want to go in practice and<br>        runtime increases linearly as you increase this parameter.<br>    random : :py:class:`random.Random`<br>        A Python random number generator. Defaults to ``import random`` but can<br>        be set to your own instance of :py:class:`random.Random` to allow you<br>        to control the seed and produce deterministic results. For results to<br>        be deterministic, vertices_resources must be supplied as an<br>        :py:class:`collections.OrderedDict`.<br>    on_temperature_change : callback_function or None<br>        An (optional) callback function which is called every time the<br>        temperature is changed. This callback can be used to provide status<br>        updates<br><br>        The callback function is passed the following arguments:<br><br>         ``iteration_count``: the number of iterations the placer has<br>          attempted (integer)<br>         ``placements``: The current placement solution.<br>         ``cost``: the weighted sum over all nets of bounding-box size.<br>          (float)<br>         ``acceptance_rate``: the proportion of iterations which have resulted<br>          in an accepted change since the last callback call. (float between<br>          0.0 and 1.0)<br>         ``temperature``: The current annealing temperature. (float)<br>         ``distance_limit``: The maximum distance any swap may be made over.<br>          (integer)<br><br>        If the callback returns False, the anneal is terminated immediately and<br>        the current solution is returned.<br>    kernel : :py:class:`~rig.place_and_route.place.sa.kernel.Kernel`<br>        A simulated annealing placement kernel. A sensible default will be<br>        chosen based on the available kernels on this machine. The kernel may<br>        not be used if the placement problem has a trivial solution.<br>    kernel_kwargs : dict<br>        Optional kernel-specific keyword arguments to pass to the kernel<br>        constructor.<br>    """<br>    # Special case: just return immediately when there's nothing to place<br>    if len(vertices_resources) == 0:<br>        return {}<br><br>    # Within the algorithm we modify the resource availability values in the<br>    # machine to account for the effects of the current placement. As a result,<br>    # an internal copy of the structure must be made.<br>    machine = machine.copy()<br><br>    # {vertex: (x, y), ...} gives the location of all vertices whose position<br>    # is fixed by a LocationConstraint.<br>    fixed_vertices = {}<br><br>    # Handle constraints<br>    vertices_resources, nets, constraints, substitutions = \<br>        apply_same_chip_constraints(vertices_resources, nets, constraints)<br>    for constraint in constraints:<br>        if isinstance(constraint, LocationConstraint):<br>            # Location constraints are handled by recording the set of fixed<br>            # vertex locations and subtracting their resources from the chips<br>            # they're allocated to. These vertices will then not be added to<br>            # the internal placement data structure to prevent annealing from<br>            # moving them. They will be re-introduced at the last possible<br>            # moment.<br>            location = constraint.location<br>            if location not in machine:<br>                raise InvalidConstraintError(<br>                    "Chip requested by {} unavailable".format(machine))<br>            vertex = constraint.vertex<br><br>            # Record the constrained vertex's location<br>            fixed_vertices[vertex] = location<br><br>            # Make sure the vertex fits at the requested location (updating the<br>            # resource availability after placement)<br>            resources = vertices_resources[vertex]<br>            machine[location] = subtract_resources(machine[location],<br>                                                   resources)<br>            if overallocated(machine[location]):<br>                raise InsufficientResourceError(<br>                    "Cannot meet {}".format(constraint))<br>        elif isinstance(constraint,  # pragma: no branch<br>                        ReserveResourceConstraint):<br>            apply_reserve_resource_constraint(machine, constraint)<br><br>    # Initially randomly place the movable vertices<br>    movable_vertices = {v for v in vertices_resources<br>                        if v not in fixed_vertices}<br>    initial_placements = _initial_placement(movable_vertices,<br>                                            vertices_resources,<br>                                            machine, random)<br><br>    # Include the fixed vertices in initial placement<br>    initial_placements.update(fixed_vertices)<br><br>    # Filter out empty or singleton nets and those weighted as zero since they<br>    # cannot influence placement.<br>    nets = [n for n in nets if len(set(n))  1 and n.weight  0.0]<br><br>    # Special cases where no placement effort is required:<br>    #  There is only one chip<br>    #  There are no resource types to be consumed<br>    #  No effort is to be made<br>    #  No movable vertices<br>    #  There are no nets (and moving things has no effect)<br>    trivial = ((machine.width, machine.height) == (1, 1) or<br>               len(machine.chip_resources) == 0 or<br>               effort == 0.0 or<br>               len(movable_vertices) == 0 or<br>               len(nets) == 0)<br>    if trivial:<br>        logger.info("Placement has trivial solution. SA not used.")<br>        finalise_same_chip_constraints(substitutions, initial_placements)<br>        return initial_placements<br><br>    # Intialise the algorithm kernel<br>    k = kernel(vertices_resources, movable_vertices, set(fixed_vertices),<br>               initial_placements, nets, machine, random, kernel_kwargs)<br><br>    logger.info("SA placement kernel: %s", kernel.__name__)<br><br>    # Specifies the maximum distance any swap can span. Initially consider<br>    # swaps that span the entire machine.<br>    distance_limit = max(machine.width, machine.height)<br><br>    # Determine initial temperature according to the heuristic used by VPR: 20<br>    # times the standard deviation of len(movable_vertices) random swap costs.<br>    # The arbitrary very-high temperature is used to cause "all" swaps to be<br>    # accepted.<br>    _0, _1, cost_delta_sd = k.run_steps(len(movable_vertices),<br>                                        distance_limit,<br>                                        1e100)<br>    temperature = 20.0  cost_delta_sd<br><br>    # The number of swap-attempts between temperature changes is selected by<br>    # the heuristic used by VPR. This value is scaled linearly by the effort<br>    # parameter.<br>    num_steps = max(1, int(effort  len(vertices_resources)1.33))<br><br>    logger.info("Initial placement temperature: %0.1f", temperature)<br><br>    # Counter for the number of swap attempts made (used for diagnostic<br>    # purposes)<br>    iteration_count = 0<br><br>    # Holds the total cost of the current placement. This default value chosen<br>    # to ensure the loop below iterates at least once.<br>    current_cost = 0.0<br><br>    # The annealing algorithm runs until a heuristic termination condition<br>    # (taken from VPR) is hit. The heuristic waits until the temperature falls<br>    # below a small fraction of the average net cost.<br>    while temperature  (0.005  current_cost) / len(nets):<br>        # Run an iteration at the current temperature<br>        num_accepted, current_cost, _ = k.run_steps(<br>            num_steps, int(math.ceil(distance_limit)), temperature)<br><br>        # The ratio of accepted-to-not-accepted changes<br>        r_accept = num_accepted / float(num_steps)<br><br>        # Special case: Can't do better than 0 cost! This is a special case<br>        # since the normal termination condition will not terminate if the cost<br>        # doesn't drop below 0.<br>        if current_cost == 0:<br>            break<br><br>        # The temperature is reduced by a factor heuristically based on the<br>        # acceptance rate. The schedule below attempts to maximise the time<br>        # spent at temperatures where a large portion (but not all) of changes<br>        # are being accepted. If lots of changes are being accepted (e.g.<br>        # during high-temperature periods) then most of them are likely not to<br>        # be beneficial. If few changes are being accepted, we're probably<br>        # pretty close to the optimal placement.<br>        if r_accept  0.96:<br>            alpha = 0.5<br>        elif r_accept  0.8:<br>            alpha = 0.9<br>        elif r_accept  0.15:<br>            alpha = 0.95<br>        else:<br>            alpha = 0.8<br>        temperature = alpha  temperature<br><br>        # According to:<br>        #  M. Huang, F. Romeo, and A. Sangiovanni-Vincentelli, "An Efficient<br>        #   General Cooling Schedule for Simulated Annealing" ICCAD, 1986, pp.<br>        #   381 - 384 and J. Lam<br>        #  J. Delosme, "Performance of a New Annealing Schedule" DAC, 1988,<br>        #   pp. 306 - 311.<br>        # It is desirable to keep the acceptance ratio as close to 0.44 for as<br>        # long as possible. As a result, when r_accept falls below this we can<br>        # help increase the acceptance rate by reducing the set of possible<br>        # swap candidates based on the observation that near the end of<br>        # placement, most things are near their optimal location and thus long<br>        # distance swaps are unlikely to be useful.<br>        distance_limit = 1.0 - 0.44 + r_accept<br>        distance_limit = min(max(distance_limit, 1.0),<br>                             max(machine.width, machine.height))<br><br>        iteration_count += num_steps<br>        logger.debug("Iteration: %d, "<br>                     "Cost: %0.1f, "<br>                     "Kept: %0.1f%%, "<br>                     "Temp: %0.3f, "<br>                     "Dist: %d.",<br>                     iteration_count, current_cost,<br>                     r_accept100, temperature, math.ceil(distance_limit))<br><br>        # Call the user callback before the next iteration, terminating if<br>        # requested.<br>        if on_temperature_change is not None:<br>            placements = k.get_placements().copy()<br>            finalise_same_chip_constraints(substitutions, placements)<br>            ret_val = on_temperature_change(iteration_count,<br>                                            placements,<br>                                            current_cost,<br>                                            r_accept,<br>                                            temperature,<br>                                            distance_limit)<br>            if ret_val is False:<br>                break<br><br>    logger.info("Anneal terminated after %d iterations.", iteration_count)<br><br>    placements = k.get_placements()<br>    finalise_same_chip_constraints(substitutions, placements)<br><br>    return placements

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_64048.txt
> def step_until_intersect(pos, field_line, sign, time,  direction=None,<br>                        step_size_goal=5., <br>                        field_step_size=None):   <br>    """Starting at pos, method steps along magnetic unit vector direction <br>    towards the supplied field line trace. Determines the distance of <br>    closest approach to field line.<br>    <br>    Routine is used when calculting the mapping of electric fields along <br>    magnetic field lines. Voltage remains constant along the field but the <br>    distance between field lines does not.This routine may be used to form the <br>    last leg when trying to trace out a closed field line loop.<br>    <br>    Routine will create a high resolution field line trace (.01 km step size) <br>    near the location of closest approach to better determine where the <br>    intersection occurs. <br>    <br>    Parameters<br>    ----------<br>    pos : array-like<br>        X, Y, and Z ECEF locations to start from<br>    field_line : array-like (:,3)<br>        X, Y, and Z ECEF locations of field line trace, produced by the<br>        field_line_trace method.<br>    sign : int<br>        if 1, move along positive unit vector. Negwtive direction for -1.<br>    time : datetime or float<br>        Date to perform tracing on (year + day/365 + hours/24. + etc.)<br>        Accounts for leap year if datetime provided.<br>    direction : string ('meridional', 'zonal', or 'aligned')<br>        Which unit vector direction to move slong when trying to intersect<br>        with supplied field line trace. See step_along_mag_unit_vector method<br>        for more.<br>    step_size_goal : float<br>        step size goal that method will try to match when stepping towards field line. <br>    <br>    Returns<br>    -------<br>    (float, array, float)<br>        Total distance taken along vector direction; the position after taking <br>        the step [x, y, z] in ECEF; distance of closest approach from input pos <br>        towards the input field line trace.<br>         <br>    """ <br>                                                         <br>    # work on a copy, probably not needed<br>    field_copy = field_line<br>    # set a high last minimum distance to ensure first loop does better than this<br>    last_min_dist = 2500000.<br>    # scalar is the distance along unit vector line that we are taking<br>    scalar = 0.<br>    # repeat boolean<br>    repeat=True<br>    # first run boolean<br>    first=True<br>    # factor is a divisor applied to the remaining distance between point and field line<br>    # I slowly take steps towards the field line and I don't want to overshoot<br>    # each time my minimum distance increases, I step back, increase factor, reducing<br>    # my next step size, then I try again<br>    factor = 1<br>    while repeat:<br>        # take a total step along magnetic unit vector<br>        # try to take steps near user provided step_size_goal<br>        unit_steps = np.abs(scalar//step_size_goal)<br>        if unit_steps == 0:<br>            unit_steps = 1<br>        # print (unit_steps, scalar/unit_steps)<br>        pos_step = step_along_mag_unit_vector(pos[0], pos[1], pos[2], time, <br>                                              direction=direction,<br>                                              num_steps=unit_steps, <br>                                              step_size=np.abs(scalar)/unit_steps,<br>                                              scalar=sign) <br>        # find closest point along field line trace<br>        diff = field_copy - pos_step<br>        diff_mag = np.sqrt((diff  2).sum(axis=1))<br>        min_idx = np.argmin(diff_mag)<br>        if first:<br>            # first time in while loop, create some information<br>            # make a high resolution field line trace around closest distance<br>            # want to take a field step size in each direction<br>            # maintain accuracy of high res trace below to be .01 km<br>            init = field_copy[min_idx,:]<br>            field_copy = full_field_line(init, time, 0.,<br>                                         step_size=0.01, <br>                                         max_steps=int(field_step_size/.01),<br>                                         recurse=False)<br>            # difference with position<br>            diff = field_copy - pos_step<br>            diff_mag = np.sqrt((diff  2).sum(axis=1))<br>            # find closest one<br>            min_idx = np.argmin(diff_mag)<br>            # # reduce number of elements we really need to check<br>            # field_copy = field_copy[min_idx-100:min_idx+100]<br>            # # difference with position<br>            # diff = field_copy - pos_step<br>            # diff_mag = np.sqrt((diff  2).sum(axis=1))<br>            # # find closest one<br>            # min_idx = np.argmin(diff_mag)<br>            first = False<br>            <br>        # pull out distance of closest point <br>        min_dist = diff_mag[min_idx]<br>        <br>        # check how the solution is doing<br>        # if well, add more distance to the total step and recheck if closer<br>        # if worse, step back and try a smaller step<br>        if min_dist  last_min_dist:<br>            # last step we took made the solution worse<br>            if factor  4:<br>                # we've tried enough, stop looping<br>                repeat = False<br>                # undo increment to last total distance<br>                scalar = scalar - last_min_dist/(2factor)<br>                # calculate latest position<br>                pos_step = step_along_mag_unit_vector(pos[0], pos[1], pos[2], <br>                                        time, <br>                                        direction=direction,<br>                                        num_steps=unit_steps, <br>                                        step_size=np.abs(scalar)/unit_steps,<br>                                        scalar=sign) <br>            else:<br>                # undo increment to last total distance<br>                scalar = scalar - last_min_dist/(2factor)<br>                # increase the divisor used to reduce the distance <br>                # actually stepped per increment<br>                factor = factor + 1.<br>                # try a new increment to total distance<br>                scalar = scalar + last_min_dist/(2factor)<br>        else:<br>            # we did better, move even closer, a fraction of remaining distance<br>            # increment scalar, but only by a fraction<br>            scalar = scalar + min_dist/(2factor)<br>            # we have a new standard to judge against, set it<br>            last_min_dist = min_dist.copy()<br><br>    # return magnitude of step<br>    return scalar, pos_step, min_dist

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_178623.txt
> def _get_efron_values_batch(self, X, T, E, weights, beta):  # pylint: disable=too-many-locals<br>        """<br>        Assumes sorted on ascending on T<br>        Calculates the first and second order vector differentials, with respect to beta.<br><br>        A good explanation for how Efron handles ties. Consider three of five subjects who fail at the time.<br>        As it is not known a priori that who is the first to fail, so one-third of<br>        (φ1 + φ2 + φ3) is adjusted from sum_j^{5} φj after one fails. Similarly two-third<br>        of (φ1 + φ2 + φ3) is adjusted after first two individuals fail, etc.<br><br>        Returns<br>        -------<br>        hessian: (d, d) numpy array,<br>        gradient: (1, d) numpy array<br>        log_likelihood: float<br>        """<br><br>        n, d = X.shape<br>        hessian = np.zeros((d, d))<br>        gradient = np.zeros((d,))<br>        log_lik = 0<br>        # weights = weights[:, None]<br><br>        # Init risk and tie sums to zero<br>        risk_phi, tie_phi = 0, 0<br>        risk_phi_x, tie_phi_x = np.zeros((d,)), np.zeros((d,))<br>        risk_phi_x_x, tie_phi_x_x = np.zeros((d, d)), np.zeros((d, d))<br><br>        # counts are sorted by -T<br>        _, counts = np.unique(-T, return_counts=True)<br>        scores = weights  np.exp(np.dot(X, beta))<br>        pos = n<br><br>        for count_of_removals in counts:<br><br>            slice_ = slice(pos - count_of_removals, pos)<br><br>            X_at_t = X[slice_]<br>            weights_at_t = weights[slice_]<br><br>            phi_i = scores[slice_, None]<br>            phi_x_i = phi_i  X_at_t<br>            phi_x_x_i = np.dot(X_at_t.T, phi_x_i)<br><br>            # Calculate sums of Risk set<br>            risk_phi = risk_phi + array_sum_to_scalar(phi_i)<br>            risk_phi_x = risk_phi_x + matrix_axis_0_sum_to_array(phi_x_i)<br>            risk_phi_x_x = risk_phi_x_x + phi_x_x_i<br><br>            # Calculate the sums of Tie set<br>            deaths = E[slice_]<br><br>            tied_death_counts = array_sum_to_scalar(deaths.astype(int))<br>            if tied_death_counts == 0:<br>                # no deaths, can continue<br>                pos -= count_of_removals<br>                continue<br><br>            xi_deaths = X_at_t[deaths]<br>            weights_deaths = weights_at_t[deaths]<br><br>            x_death_sum = matrix_axis_0_sum_to_array(weights_deaths[:, None]  xi_deaths)<br><br>            weight_count = array_sum_to_scalar(weights_deaths)<br>            weighted_average = weight_count / tied_death_counts<br><br>            if tied_death_counts  1:<br><br>                # a lot of this is now in Einstein notation for performance, but see original "expanded" code here<br>                # https://github.com/CamDavidsonPilon/lifelines/blob/e7056e7817272eb5dff5983556954f56c33301b1/lifelines/fitters/coxph_fitter.py#L755-L789<br><br>                # it's faster if we can skip computing these when we don't need to.<br>                tie_phi = array_sum_to_scalar(phi_i[deaths])<br>                tie_phi_x = matrix_axis_0_sum_to_array(phi_x_i[deaths])<br>                tie_phi_x_x = np.dot(xi_deaths.T, phi_i[deaths]  xi_deaths)<br><br>                increasing_proportion = np.arange(tied_death_counts) / tied_death_counts<br>                denom = 1.0 / (risk_phi - increasing_proportion  tie_phi)<br>                numer = risk_phi_x - np.outer(increasing_proportion, tie_phi_x)<br><br>                # computes outer products and sums them together.<br>                # Naive approach is to<br>                # 1) broadcast tie_phi_x_x and increasing_proportion into a (tied_death_counts, d, d) matrix<br>                # 2) broadcast risk_phi_x_x and denom into a (tied_death_counts, d, d) matrix<br>                # 3) subtract them, and then sum to (d, d)<br>                # Alternatively, we can sum earlier without having to explicitly create (_, d, d) matrices. This is used here.<br>                #<br>                a1 = np.einsum("ab,i-ab", risk_phi_x_x, denom) - np.einsum(<br>                    "ab,i-ab", tie_phi_x_x, increasing_proportion  denom<br>                )<br>            else:<br>                # no tensors here, but do some casting to make it easier in the converging step next.<br>                denom = 1.0 / np.array([risk_phi])<br>                numer = risk_phi_x<br>                a1 = risk_phi_x_x  denom<br><br>            summand = numer  denom[:, None]<br>            # This is a batch outer product.<br>            # given a matrix t, for each row, m, compute it's outer product: m.dot(m.T), and stack these new matrices together.<br>            # which would be: np.einsum("Bi, Bj-Bij", t, t)<br>            a2 = summand.T.dot(summand)<br><br>            gradient = gradient + x_death_sum - weighted_average  summand.sum(0)<br>            log_lik = log_lik + np.dot(x_death_sum, beta) + weighted_average  np.log(denom).sum()<br>            hessian = hessian + weighted_average  (a2 - a1)<br>            pos -= count_of_removals<br><br>        return hessian, gradient, log_lik

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_343871.txt
> def search(cursor=None, expr=None, facet=None, filterQuery=None, highlight=None, partial=None, query=None, queryOptions=None, queryParser=None, returnFields=None, size=None, sort=None, start=None, stats=None):<br>    """<br>    Retrieves a list of documents that match the specified search criteria. How you specify the search criteria depends on which query parser you use. Amazon CloudSearch supports four query parsers:<br>    For more information, see Searching Your Data in the Amazon CloudSearch Developer Guide .<br>    The endpoint for submitting Search requests is domain-specific. You submit search requests to a domain's search endpoint. To get the search endpoint for your domain, use the Amazon CloudSearch configuration service DescribeDomains action. A domain's endpoints are also displayed on the domain dashboard in the Amazon CloudSearch console.<br>    See also: AWS API Documentation<br>    <br>    <br>    :example: response = client.search(<br>        cursor='string',<br>        expr='string',<br>        facet='string',<br>        filterQuery='string',<br>        highlight='string',<br>        partial=True|False,<br>        query='string',<br>        queryOptions='string',<br>        queryParser='simple'|'structured'|'lucene'|'dismax',<br>        returnFields='string',<br>        size=123,<br>        sort='string',<br>        start=123,<br>        stats='string'<br>    )<br>    <br>    <br>    :type cursor: string<br>    :param cursor: Retrieves a cursor value you can use to page through large result sets. Use the size parameter to control the number of hits to include in each response. You can specify either the cursor or start parameter in a request; they are mutually exclusive. To get the first cursor, set the cursor value to initial . In subsequent requests, specify the cursor value returned in the hits section of the response.<br>            For more information, see Paginating Results in the Amazon CloudSearch Developer Guide .<br>            <br><br>    :type expr: string<br>    :param expr: Defines one or more numeric expressions that can be used to sort results or specify search or filter criteria. You can also specify expressions as return fields.<br>            You specify the expressions in JSON using the form {'EXPRESSIONNAME':'EXPRESSION'} . You can define and use multiple expressions in a search request. For example:<br>            {'expression1':'_scorerating', 'expression2':'(1/rank)year'}<br>            For information about the variables, operators, and functions you can use in expressions, see Writing Expressions in the Amazon CloudSearch Developer Guide .<br>            <br><br>    :type facet: string<br>    :param facet: Specifies one or more fields for which to get facet information, and options that control how the facet information is returned. Each specified field must be facet-enabled in the domain configuration. The fields and options are specified in JSON using the form {'FIELD':{'OPTION':VALUE,'OPTION:'STRING'},'FIELD':{'OPTION':VALUE,'OPTION':'STRING'}} .<br>            You can specify the following faceting options:<br>            buckets specifies an array of the facet values or ranges to count. Ranges are specified using the same syntax that you use to search for a range of values. For more information, see Searching for a Range of Values in the Amazon CloudSearch Developer Guide . Buckets are returned in the order they are specified in the request. The sort and size options are not valid if you specify buckets .<br>            size specifies the maximum number of facets to include in the results. By default, Amazon CloudSearch returns counts for the top 10. The size parameter is only valid when you specify the sort option; it cannot be used in conjunction with buckets .<br>            sort specifies how you want to sort the facets in the results: bucket or count . Specify bucket to sort alphabetically or numerically by facet value (in ascending order). Specify count to sort by the facet counts computed for each facet value (in descending order). To retrieve facet counts for particular values or ranges of values, use the buckets option instead of sort .<br>            If no facet options are specified, facet counts are computed for all field values, the facets are sorted by facet count, and the top 10 facets are returned in the results.<br>            To count particular buckets of values, use the buckets option. For example, the following request uses the buckets option to calculate and return facet counts by decade.<br>            {'year':{'buckets':['[1970,1979]','[1980,1989]','[1990,1999]','[2000,2009]','[2010,}']}}<br>            To sort facets by facet count, use the count option. For example, the following request sets the sort option to count to sort the facet values by facet count, with the facet values that have the most matching documents listed first. Setting the size option to 3 returns only the top three facet values.<br>            {'year':{'sort':'count','size':3}}<br>            To sort the facets by value, use the bucket option. For example, the following request sets the sort option to bucket to sort the facet values numerically by year, with earliest year listed first.<br>            {'year':{'sort':'bucket'}}<br>            For more information, see Getting and Using Facet Information in the Amazon CloudSearch Developer Guide .<br>            <br><br>    :type filterQuery: string<br>    :param filterQuery: Specifies a structured query that filters the results of a search without affecting how the results are scored and sorted. You use filterQuery in conjunction with the query parameter to filter the documents that match the constraints specified in the query parameter. Specifying a filter controls only which matching documents are included in the results, it has no effect on how they are scored and sorted. The filterQuery parameter supports the full structured query syntax.<br>            For more information about using filters, see Filtering Matching Documents in the Amazon CloudSearch Developer Guide .<br>            <br><br>    :type highlight: string<br>    :param highlight: Retrieves highlights for matches in the specified text or text-array fields. Each specified field must be highlight enabled in the domain configuration. The fields and options are specified in JSON using the form {'FIELD':{'OPTION':VALUE,'OPTION:'STRING'},'FIELD':{'OPTION':VALUE,'OPTION':'STRING'}} .<br>            You can specify the following highlight options:<br>            format : specifies the format of the data in the text field: text or html . When data is returned as HTML, all non-alphanumeric characters are encoded. The default is html .<br>            max_phrases : specifies the maximum number of occurrences of the search term(s) you want to highlight. By default, the first occurrence is highlighted.<br>            pre_tag : specifies the string to prepend to an occurrence of a search term. The default for HTML highlights is lt;emgt; . The default for text highlights is  .<br>            post_tag : specifies the string to append to an occurrence of a search term. The default for HTML highlights is lt;/emgt; . The default for text highlights is  .<br>            If no highlight options are specified for a field, the returned field text is treated as HTML and the first match is highlighted with emphasis tags: lt;emsearch-termlt;/emgt; .<br>            For example, the following request retrieves highlights for the actors and title fields.<br>            { 'actors': {}, 'title': {'format': 'text','max_phrases': 2,'pre_tag': '','post_tag': ' '} }<br>            <br><br>    :type partial: boolean<br>    :param partial: Enables partial results to be returned if one or more index partitions are unavailable. When your search index is partitioned across multiple search instances, by default Amazon CloudSearch only returns results if every partition can be queried. This means that the failure of a single search instance can result in 5xx (internal server) errors. When you enable partial results, Amazon CloudSearch returns whatever results are available and includes the percentage of documents searched in the search results (percent-searched). This enables you to more gracefully degrade your users' search experience. For example, rather than displaying no results, you could display the partial results and a message indicating that the results might be incomplete due to a temporary system outage.<br><br>    :type query: string<br>    :param query: [REQUIRED]<br>            Specifies the search criteria for the request. How you specify the search criteria depends on the query parser used for the request and the parser options specified in the queryOptions parameter. By default, the simple query parser is used to process requests. To use the structured , lucene , or dismax query parser, you must also specify the queryParser parameter.<br>            For more information about specifying search criteria, see Searching Your Data in the Amazon CloudSearch Developer Guide .<br>            <br><br>    :type queryOptions: string<br>    :param queryOptions: Configures options for the query parser specified in the queryParser parameter. You specify the options in JSON using the following form {'OPTION1':'VALUE1','OPTION2':VALUE2'...'OPTIONN':'VALUEN'}.<br>            The options you can configure vary according to which parser you use:<br>            defaultOperator : The default operator used to combine individual terms in the search string. For example: defaultOperator: 'or' . For the dismax parser, you specify a percentage that represents the percentage of terms in the search string (rounded down) that must match, rather than a default operator. A value of 0% is the equivalent to OR, and a value of 100% is equivalent to AND. The percentage must be specified as a value in the range 0-100 followed by the percent (%) symbol. For example, defaultOperator: 50% . Valid values: and , or , a percentage in the range 0%-100% (dismax ). Default: and (simple , structured , lucene ) or 100 (dismax ). Valid for: simple , structured , lucene , and dismax .<br>            fields : An array of the fields to search when no fields are specified in a search. If no fields are specified in a search and this option is not specified, all text and text-array fields are searched. You can specify a weight for each field to control the relative importance of each field when Amazon CloudSearch calculates relevance scores. To specify a field weight, append a caret (^ ) symbol and the weight to the field name. For example, to boost the importance of the title field over the description field you could specify: 'fields':['title^5','description'] . Valid values: The name of any configured field and an optional numeric value greater than zero. Default: All text and text-array fields. Valid for: simple , structured , lucene , and dismax .<br>            operators : An array of the operators or special characters you want to disable for the simple query parser. If you disable the and , or , or not operators, the corresponding operators (+ , | , - ) have no special meaning and are dropped from the search string. Similarly, disabling prefix disables the wildcard operator ( ) and disabling phrase disables the ability to search for phrases by enclosing phrases in double quotes. Disabling precedence disables the ability to control order of precedence using parentheses. Disabling near disables the ability to use the ~ operator to perform a sloppy phrase search. Disabling the fuzzy operator disables the ability to use the ~ operator to perform a fuzzy search. escape disables the ability to use a backslash (\ ) to escape special characters within the search string. Disabling whitespace is an advanced option that prevents the parser from tokenizing on whitespace, which can be useful for Vietnamese. (It prevents Vietnamese words from being split incorrectly.) For example, you could disable all operators other than the phrase operator to support just simple term and phrase queries: 'operators':['and','not','or', 'prefix'] . Valid values: and , escape , fuzzy , near , not , or , phrase , precedence , prefix , whitespace . Default: All operators and special characters are enabled. Valid for: simple .<br>            phraseFields : An array of the text or text-array fields you want to use for phrase searches. When the terms in the search string appear in close proximity within a field, the field scores higher. You can specify a weight for each field to boost that score. The phraseSlop option controls how much the matches can deviate from the search string and still be boosted. To specify a field weight, append a caret (^ ) symbol and the weight to the field name. For example, to boost phrase matches in the title field over the abstract field, you could specify: 'phraseFields':['title^3', 'plot'] Valid values: The name of any text or text-array field and an optional numeric value greater than zero. Default: No fields. If you don't specify any fields with phraseFields , proximity scoring is disabled even if phraseSlop is specified. Valid for: dismax .<br>            phraseSlop : An integer value that specifies how much matches can deviate from the search phrase and still be boosted according to the weights specified in the phraseFields option; for example, phraseSlop: 2 . You must also specify phraseFields to enable proximity scoring. Valid values: positive integers. Default: 0. Valid for: dismax .<br>            explicitPhraseSlop : An integer value that specifies how much a match can deviate from the search phrase when the phrase is enclosed in double quotes in the search string. (Phrases that exceed this proximity distance are not considered a match.) For example, to specify a slop of three for dismax phrase queries, you would specify 'explicitPhraseSlop':3 . Valid values: positive integers. Default: 0. Valid for: dismax .<br>            tieBreaker : When a term in the search string is found in a document's field, a score is calculated for that field based on how common the word is in that field compared to other documents. If the term occurs in multiple fields within a document, by default only the highest scoring field contributes to the document's overall score. You can specify a tieBreaker value to enable the matches in lower-scoring fields to contribute to the document's score. That way, if two documents have the same max field score for a particular term, the score for the document that has matches in more fields will be higher. The formula for calculating the score with a tieBreaker is (max field score) + (tieBreaker)  (sum of the scores for the rest of the matching fields) . Set tieBreaker to 0 to disregard all but the highest scoring field (pure max): 'tieBreaker':0 . Set to 1 to sum the scores from all fields (pure sum): 'tieBreaker':1 . Valid values: 0.0 to 1.0. Default: 0.0. Valid for: dismax .<br>            <br><br>    :type queryParser: string<br>    :param queryParser: Specifies which query parser to use to process the request. If queryParser is not specified, Amazon CloudSearch uses the simple query parser.<br>            Amazon CloudSearch supports four query parsers:<br>            simple : perform simple searches of text and text-array fields. By default, the simple query parser searches all text and text-array fields. You can specify which fields to search by with the queryOptions parameter. If you prefix a search term with a plus sign (+) documents must contain the term to be considered a match. (This is the default, unless you configure the default operator with the queryOptions parameter.) You can use the - (NOT), | (OR), and  (wildcard) operators to exclude particular terms, find results that match any of the specified terms, or search for a prefix. To search for a phrase rather than individual terms, enclose the phrase in double quotes. For more information, see Searching for Text in the Amazon CloudSearch Developer Guide .<br>            structured : perform advanced searches by combining multiple expressions to define the search criteria. You can also search within particular fields, search for values and ranges of values, and use advanced options such as term boosting, matchall , and near . For more information, see Constructing Compound Queries in the Amazon CloudSearch Developer Guide .<br>            lucene : search using the Apache Lucene query parser syntax. For more information, see Apache Lucene Query Parser Syntax .<br>            dismax : search using the simplified subset of the Apache Lucene query parser syntax defined by the DisMax query parser. For more information, see DisMax Query Parser Syntax .<br>            <br><br>    :type returnFields: string<br>    :param returnFields: Specifies the field and expression values to include in the response. Multiple fields or expressions are specified as a comma-separated list. By default, a search response includes all return enabled fields (_all_fields ). To return only the document IDs for the matching documents, specify _no_fields . To retrieve the relevance score calculated for each document, specify _score .<br><br>    :type size: integer<br>    :param size: Specifies the maximum number of search hits to include in the response.<br><br>    :type sort: string<br>    :param sort: Specifies the fields or custom expressions to use to sort the search results. Multiple fields or expressions are specified as a comma-separated list. You must specify the sort direction (asc or desc ) for each field; for example, year desc,title asc . To use a field to sort results, the field must be sort-enabled in the domain configuration. Array type fields cannot be used for sorting. If no sort parameter is specified, results are sorted by their default relevance scores in descending order: _score desc . You can also sort by document ID (_id asc ) and version (_version desc ).<br>            For more information, see Sorting Results in the Amazon CloudSearch Developer Guide .<br>            <br><br>    :type start: integer<br>    :param start: Specifies the offset of the first search hit you want to return. Note that the result set is zero-based; the first result is at index 0. You can specify either the start or cursor parameter in a request, they are mutually exclusive.<br>            For more information, see Paginating Results in the Amazon CloudSearch Developer Guide .<br>            <br><br>    :type stats: string<br>    :param stats: Specifies one or more fields for which to get statistics information. Each specified field must be facet-enabled in the domain configuration. The fields are specified in JSON using the form:<br>            {'FIELD-A':{},'FIELD-B':{}}<br>            There are currently no options supported for statistics.<br>            <br><br>    :rtype: dict<br>    :return: {<br>        'status': {<br>            'timems': 123,<br>            'rid': 'string'<br>        },<br>        'hits': {<br>            'found': 123,<br>            'start': 123,<br>            'cursor': 'string',<br>            'hit': [<br>                {<br>                    'id': 'string',<br>                    'fields': {<br>                        'string': [<br>                            'string',<br>                        ]<br>                    },<br>                    'exprs': {<br>                        'string': 'string'<br>                    },<br>                    'highlights': {<br>                        'string': 'string'<br>                    }<br>                },<br>            ]<br>        },<br>        'facets': {<br>            'string': {<br>                'buckets': [<br>                    {<br>                        'value': 'string',<br>                        'count': 123<br>                    },<br>                ]<br>            }<br>        },<br>        'stats': {<br>            'string': {<br>                'min': 'string',<br>                'max': 'string',<br>                'count': 123,<br>                'missing': 123,<br>                'sum': 123.0,<br>                'sumOfSquares': 123.0,<br>                'mean': 'string',<br>                'stddev': 123.0<br>            }<br>        }<br>    }<br>    <br>    <br>    :returns: <br>    cursor (string) -- Retrieves a cursor value you can use to page through large result sets. Use the size parameter to control the number of hits to include in each response. You can specify either the cursor or start parameter in a request; they are mutually exclusive. To get the first cursor, set the cursor value to initial . In subsequent requests, specify the cursor value returned in the hits section of the response.<br>    For more information, see Paginating Results in the Amazon CloudSearch Developer Guide .<br>    <br>    expr (string) -- Defines one or more numeric expressions that can be used to sort results or specify search or filter criteria. You can also specify expressions as return fields.<br>    You specify the expressions in JSON using the form {"EXPRESSIONNAME":"EXPRESSION"} . You can define and use multiple expressions in a search request. For example:<br>    {"expression1":"_scorerating", "expression2":"(1/rank)year"}<br>    For information about the variables, operators, and functions you can use in expressions, see Writing Expressions in the Amazon CloudSearch Developer Guide .<br>    <br>    facet (string) -- Specifies one or more fields for which to get facet information, and options that control how the facet information is returned. Each specified field must be facet-enabled in the domain configuration. The fields and options are specified in JSON using the form {"FIELD":{"OPTION":VALUE,"OPTION:"STRING"},"FIELD":{"OPTION":VALUE,"OPTION":"STRING"}} .<br>    You can specify the following faceting options:<br>    <br>    buckets specifies an array of the facet values or ranges to count. Ranges are specified using the same syntax that you use to search for a range of values. For more information, see Searching for a Range of Values in the Amazon CloudSearch Developer Guide . Buckets are returned in the order they are specified in the request. The sort and size options are not valid if you specify buckets .<br>    size specifies the maximum number of facets to include in the results. By default, Amazon CloudSearch returns counts for the top 10. The size parameter is only valid when you specify the sort option; it cannot be used in conjunction with buckets .<br>    sort specifies how you want to sort the facets in the results: bucket or count . Specify bucket to sort alphabetically or numerically by facet value (in ascending order). Specify count to sort by the facet counts computed for each facet value (in descending order). To retrieve facet counts for particular values or ranges of values, use the buckets option instead of sort .<br>    <br>    If no facet options are specified, facet counts are computed for all field values, the facets are sorted by facet count, and the top 10 facets are returned in the results.<br>    To count particular buckets of values, use the buckets option. For example, the following request uses the buckets option to calculate and return facet counts by decade.<br>    {"year":{"buckets":["[1970,1979]","[1980,1989]","[1990,1999]","[2000,2009]","[2010,}"]}}<br>    To sort facets by facet count, use the count option. For example, the following request sets the sort option to count to sort the facet values by facet count, with the facet values that have the most matching documents listed first. Setting the size option to 3 returns only the top three facet values.<br>    {"year":{"sort":"count","size":3}}<br>    To sort the facets by value, use the bucket option. For example, the following request sets the sort option to bucket to sort the facet values numerically by year, with earliest year listed first.<br>    {"year":{"sort":"bucket"}}<br>    For more information, see Getting and Using Facet Information in the Amazon CloudSearch Developer Guide .<br>    <br>    filterQuery (string) -- Specifies a structured query that filters the results of a search without affecting how the results are scored and sorted. You use filterQuery in conjunction with the query parameter to filter the documents that match the constraints specified in the query parameter. Specifying a filter controls only which matching documents are included in the results, it has no effect on how they are scored and sorted. The filterQuery parameter supports the full structured query syntax.<br>    For more information about using filters, see Filtering Matching Documents in the Amazon CloudSearch Developer Guide .<br>    <br>    highlight (string) -- Retrieves highlights for matches in the specified text or text-array fields. Each specified field must be highlight enabled in the domain configuration. The fields and options are specified in JSON using the form {"FIELD":{"OPTION":VALUE,"OPTION:"STRING"},"FIELD":{"OPTION":VALUE,"OPTION":"STRING"}} .<br>    You can specify the following highlight options:<br>    <br>    format : specifies the format of the data in the text field: text or html . When data is returned as HTML, all non-alphanumeric characters are encoded. The default is html .<br>    max_phrases : specifies the maximum number of occurrences of the search term(s) you want to highlight. By default, the first occurrence is highlighted.<br>    pre_tag : specifies the string to prepend to an occurrence of a search term. The default for HTML highlights is lt;emgt; . The default for text highlights is  .<br>    post_tag : specifies the string to append to an occurrence of a search term. The default for HTML highlights is lt;/emgt; . The default for text highlights is  .<br>    <br>    If no highlight options are specified for a field, the returned field text is treated as HTML and the first match is highlighted with emphasis tags: lt;emsearch-termlt;/emgt; .<br>    For example, the following request retrieves highlights for the actors and title fields.<br>    <br>    { "actors": {}, "title": {"format": "text","max_phrases": 2,"pre_tag": "","post_tag": " "} }<br>    <br>    partial (boolean) -- Enables partial results to be returned if one or more index partitions are unavailable. When your search index is partitioned across multiple search instances, by default Amazon CloudSearch only returns results if every partition can be queried. This means that the failure of a single search instance can result in 5xx (internal server) errors. When you enable partial results, Amazon CloudSearch returns whatever results are available and includes the percentage of documents searched in the search results (percent-searched). This enables you to more gracefully degrade your users' search experience. For example, rather than displaying no results, you could display the partial results and a message indicating that the results might be incomplete due to a temporary system outage.<br>    query (string) -- [REQUIRED]<br>    Specifies the search criteria for the request. How you specify the search criteria depends on the query parser used for the request and the parser options specified in the queryOptions parameter. By default, the simple query parser is used to process requests. To use the structured , lucene , or dismax query parser, you must also specify the queryParser parameter.<br>    For more information about specifying search criteria, see Searching Your Data in the Amazon CloudSearch Developer Guide .<br>    <br>    queryOptions (string) -- Configures options for the query parser specified in the queryParser parameter. You specify the options in JSON using the following form {"OPTION1":"VALUE1","OPTION2":VALUE2"..."OPTIONN":"VALUEN"}.<br>    The options you can configure vary according to which parser you use:<br>    <br>    defaultOperator : The default operator used to combine individual terms in the search string. For example: defaultOperator: 'or' . For the dismax parser, you specify a percentage that represents the percentage of terms in the search string (rounded down) that must match, rather than a default operator. A value of 0% is the equivalent to OR, and a value of 100% is equivalent to AND. The percentage must be specified as a value in the range 0-100 followed by the percent (%) symbol. For example, defaultOperator: 50% . Valid values: and , or , a percentage in the range 0%-100% (dismax ). Default: and (simple , structured , lucene ) or 100 (dismax ). Valid for: simple , structured , lucene , and dismax .<br>    fields : An array of the fields to search when no fields are specified in a search. If no fields are specified in a search and this option is not specified, all text and text-array fields are searched. You can specify a weight for each field to control the relative importance of each field when Amazon CloudSearch calculates relevance scores. To specify a field weight, append a caret (^ ) symbol and the weight to the field name. For example, to boost the importance of the title field over the description field you could specify: "fields":["title^5","description"] . Valid values: The name of any configured field and an optional numeric value greater than zero. Default: All text and text-array fields. Valid for: simple , structured , lucene , and dismax .<br>    operators : An array of the operators or special characters you want to disable for the simple query parser. If you disable the and , or , or not operators, the corresponding operators (+ , | , - ) have no special meaning and are dropped from the search string. Similarly, disabling prefix disables the wildcard operator ( ) and disabling phrase disables the ability to search for phrases by enclosing phrases in double quotes. Disabling precedence disables the ability to control order of precedence using parentheses. Disabling near disables the ability to use the ~ operator to perform a sloppy phrase search. Disabling the fuzzy operator disables the ability to use the ~ operator to perform a fuzzy search. escape disables the ability to use a backslash (\ ) to escape special characters within the search string. Disabling whitespace is an advanced option that prevents the parser from tokenizing on whitespace, which can be useful for Vietnamese. (It prevents Vietnamese words from being split incorrectly.) For example, you could disable all operators other than the phrase operator to support just simple term and phrase queries: "operators":["and","not","or", "prefix"] . Valid values: and , escape , fuzzy , near , not , or , phrase , precedence , prefix , whitespace . Default: All operators and special characters are enabled. Valid for: simple .<br>    phraseFields : An array of the text or text-array fields you want to use for phrase searches. When the terms in the search string appear in close proximity within a field, the field scores higher. You can specify a weight for each field to boost that score. The phraseSlop option controls how much the matches can deviate from the search string and still be boosted. To specify a field weight, append a caret (^ ) symbol and the weight to the field name. For example, to boost phrase matches in the title field over the abstract field, you could specify: "phraseFields":["title^3", "plot"] Valid values: The name of any text or text-array field and an optional numeric value greater than zero. Default: No fields. If you don't specify any fields with phraseFields , proximity scoring is disabled even if phraseSlop is specified. Valid for: dismax .<br>    phraseSlop : An integer value that specifies how much matches can deviate from the search phrase and still be boosted according to the weights specified in the phraseFields option; for example, phraseSlop: 2 . You must also specify phraseFields to enable proximity scoring. Valid values: positive integers. Default: 0. Valid for: dismax .<br>    explicitPhraseSlop : An integer value that specifies how much a match can deviate from the search phrase when the phrase is enclosed in double quotes in the search string. (Phrases that exceed this proximity distance are not considered a match.) For example, to specify a slop of three for dismax phrase queries, you would specify "explicitPhraseSlop":3 . Valid values: positive integers. Default: 0. Valid for: dismax .<br>    tieBreaker : When a term in the search string is found in a document's field, a score is calculated for that field based on how common the word is in that field compared to other documents. If the term occurs in multiple fields within a document, by default only the highest scoring field contributes to the document's overall score. You can specify a tieBreaker value to enable the matches in lower-scoring fields to contribute to the document's score. That way, if two documents have the same max field score for a particular term, the score for the document that has matches in more fields will be higher. The formula for calculating the score with a tieBreaker is (max field score) + (tieBreaker)  (sum of the scores for the rest of the matching fields) . Set tieBreaker to 0 to disregard all but the highest scoring field (pure max): "tieBreaker":0 . Set to 1 to sum the scores from all fields (pure sum): "tieBreaker":1 . Valid values: 0.0 to 1.0. Default: 0.0. Valid for: dismax .<br>    <br>    <br>    queryParser (string) -- Specifies which query parser to use to process the request. If queryParser is not specified, Amazon CloudSearch uses the simple query parser.<br>    Amazon CloudSearch supports four query parsers:<br>    <br>    simple : perform simple searches of text and text-array fields. By default, the simple query parser searches all text and text-array fields. You can specify which fields to search by with the queryOptions parameter. If you prefix a search term with a plus sign (+) documents must contain the term to be considered a match. (This is the default, unless you configure the default operator with the queryOptions parameter.) You can use the - (NOT), | (OR), and  (wildcard) operators to exclude particular terms, find results that match any of the specified terms, or search for a prefix. To search for a phrase rather than individual terms, enclose the phrase in double quotes. For more information, see Searching for Text in the Amazon CloudSearch Developer Guide .<br>    structured : perform advanced searches by combining multiple expressions to define the search criteria. You can also search within particular fields, search for values and ranges of values, and use advanced options such as term boosting, matchall , and near . For more information, see Constructing Compound Queries in the Amazon CloudSearch Developer Guide .<br>    lucene : search using the Apache Lucene query parser syntax. For more information, see Apache Lucene Query Parser Syntax .<br>    dismax : search using the simplified subset of the Apache Lucene query parser syntax defined by the DisMax query parser. For more information, see DisMax Query Parser Syntax .<br>    <br>    <br>    returnFields (string) -- Specifies the field and expression values to include in the response. Multiple fields or expressions are specified as a comma-separated list. By default, a search response includes all return enabled fields (_all_fields ). To return only the document IDs for the matching documents, specify _no_fields . To retrieve the relevance score calculated for each document, specify _score .<br>    size (integer) -- Specifies the maximum number of search hits to include in the response.<br>    sort (string) -- Specifies the fields or custom expressions to use to sort the search results. Multiple fields or expressions are specified as a comma-separated list. You must specify the sort direction (asc or desc ) for each field; for example, year desc,title asc . To use a field to sort results, the field must be sort-enabled in the domain configuration. Array type fields cannot be used for sorting. If no sort parameter is specified, results are sorted by their default relevance scores in descending order: _score desc . You can also sort by document ID (_id asc ) and version (_version desc ).<br>    For more information, see Sorting Results in the Amazon CloudSearch Developer Guide .<br>    <br>    start (integer) -- Specifies the offset of the first search hit you want to return. Note that the result set is zero-based; the first result is at index 0. You can specify either the start or cursor parameter in a request, they are mutually exclusive.<br>    For more information, see Paginating Results in the Amazon CloudSearch Developer Guide .<br>    <br>    stats (string) -- Specifies one or more fields for which to get statistics information. Each specified field must be facet-enabled in the domain configuration. The fields are specified in JSON using the form:<br>    <br>    {"FIELD-A":{},"FIELD-B":{}}<br>    There are currently no options supported for statistics.<br>    <br>    <br>    """<br>    pass

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_311157.txt
> async def send_cluster_commands(self, stack, raise_on_error=True, allow_redirections=True):<br>        """<br>        Send a bunch of cluster commands to the redis cluster.<br><br>        `allow_redirections` If the pipeline should follow `ASK` & `MOVED` responses<br>        automatically. If set to false it will raise RedisClusterException.<br>        """<br>        # the first time sending the commands we send all of the commands that were queued up.<br>        # if we have to run through it again, we only retry the commands that failed.<br>        attempt = sorted(stack, key=lambda x: x.position)<br><br>        # build a list of node objects based on node names we need to<br>        nodes = {}<br><br>        # as we move through each command that still needs to be processed,<br>        # we figure out the slot number that command maps to, then from the slot determine the node.<br>        for c in attempt:<br>            # refer to our internal node - slot table that tells us where a given<br>            # command should route to.<br>            slot = self._determine_slot(c.args)<br>            node = self.connection_pool.get_node_by_slot(slot)<br><br>            # little hack to make sure the node name is populated. probably could clean this up.<br>            self.connection_pool.nodes.set_node_name(node)<br><br>            # now that we know the name of the node ( it's just a string in the form of host:port )<br>            # we can build a list of commands for each node.<br>            node_name = node['name']<br>            if node_name not in nodes:<br>                nodes[node_name] = NodeCommands(self.parse_response, self.connection_pool.get_connection_by_node(node))<br><br>            nodes[node_name].append(c)<br><br>        # send the commands in sequence.<br>        # we  write to all the open sockets for each node first, before reading anything<br>        # this allows us to flush all the requests out across the network essentially in parallel<br>        # so that we can read them all in parallel as they come back.<br>        # we dont' multiplex on the sockets as they come available, but that shouldn't make too much difference.<br>        node_commands = nodes.values()<br>        for n in node_commands:<br>            await n.write()<br><br>        for n in node_commands:<br>            await n.read()<br><br>        # release all of the redis connections we allocated earlier back into the connection pool.<br>        # we used to do this step as part of a try/finally block, but it is really dangerous to<br>        # release connections back into the pool if for some reason the socket has data still left in it<br>        # from a previous operation. The write and read operations already have try/catch around them for<br>        # all known types of errors including connection and socket level errors.<br>        # So if we hit an exception, something really bad happened and putting any of<br>        # these connections back into the pool is a very bad idea.<br>        # the socket might have unread buffer still sitting in it, and then the<br>        # next time we read from it we pass the buffered result back from a previous<br>        # command and every single request after to that connection will always get<br>        # a mismatched result. (not just theoretical, I saw this happen on production x.x).<br>        for n in nodes.values():<br>            self.connection_pool.release(n.connection)<br><br>        # if the response isn't an exception it is a valid response from the node<br>        # we're all done with that command, YAY!<br>        # if we have more commands to attempt, we've run into problems.<br>        # collect all the commands we are allowed to retry.<br>        # (MOVED, ASK, or connection errors or timeout errors)<br>        attempt = sorted([c for c in attempt if isinstance(c.result, ERRORS_ALLOW_RETRY)], key=lambda x: x.position)<br>        if attempt and allow_redirections:<br>            # RETRY MAGIC HAPPENS HERE!<br>            # send these remaing comamnds one at a time using `execute_command`<br>            # in the main client. This keeps our retry logic in one place mostly,<br>            # and allows us to be more confident in correctness of behavior.<br>            # at this point any speed gains from pipelining have been lost<br>            # anyway, so we might as well make the best attempt to get the correct<br>            # behavior.<br>            #<br>            # The client command will handle retries for each individual command<br>            # sequentially as we pass each one into `execute_command`. Any exceptions<br>            # that bubble out should only appear once all retries have been exhausted.<br>            #<br>            # If a lot of commands have failed, we'll be setting the<br>            # flag to rebuild the slots table from scratch. So MOVED errors should<br>            # correct themselves fairly quickly.<br>            await self.connection_pool.nodes.increment_reinitialize_counter(len(attempt))<br>            for c in attempt:<br>                try:<br>                    # send each command individually like we do in the main client.<br>                    c.result = await super(StrictClusterPipeline, self).execute_command(c.args, c.options)<br>                except RedisError as e:<br>                    c.result = e<br><br>        # turn the response back into a simple flat array that corresponds<br>        # to the sequence of commands issued in the stack in pipeline.execute()<br>        response = [c.result for c in sorted(stack, key=lambda x: x.position)]<br><br>        if raise_on_error:<br>            self.raise_first_error(stack)<br><br>        return response


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_1598.txt
> def minOperations(logs):<br>    """The Leetcode file system keeps a log each time some user performs a _change folder_ operation.<br><br>The operations are described below:<br><br>   `"../ "` : Move to the parent folder of the current folder. (If you are already in the main folder, remain in the same folder).<br>   `"./ "` : Remain in the same folder.<br>   `"x/ "` : Move to the child folder named `x` (This folder is guaranteed to always exist).<br><br>You are given a list of strings `logs` where `logs[i]` is the operation performed by the user at the `ith` step.<br><br>The file system starts in the main folder, then the operations in `logs` are performed.<br><br>Return _the minimum number of operations needed to go back to the main folder after the change folder operations._<br><br>Example 1:<br><br>Input: logs = \[ "d1/ ", "d2/ ", "../ ", "d21/ ", "./ "\]<br>Output: 2<br>Explanation: Use this change folder operation  "../ " 2 times and go back to the main folder.<br><br>Example 2:<br><br>Input: logs = \[ "d1/ ", "d2/ ", "./ ", "d3/ ", "../ ", "d31/ "\]<br>Output: 3<br><br>Example 3:<br><br>Input: logs = \[ "d1/ ", "../ ", "../ ", "../ "\]<br>Output: 0<br><br>Constraints:<br><br>   `1 <= logs.length <= 103`<br>   `2 <= logs[i].length <= 10`<br>   `logs[i]` contains lowercase English letters, digits, `'.'`, and `'/'`.<br>   `logs[i]` follows the format described in the statement.<br>   Folder names consist of lowercase English letters and digits."""<br><br>    depth = 0<br>    for log in logs:<br>        if log == "../":<br>            depth = max(0, depth - 1)<br>        elif log != "./":<br>            depth += 1<br>    return depth
