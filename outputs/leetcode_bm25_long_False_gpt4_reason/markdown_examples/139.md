# Query `139`

## Original Question

*There are some prizes on the X-axis. You are given an integer array `prizePositions` that is sorted in non-decreasing order, where `prizePositions[i]` is the position of the `ith` prize. There could be different prizes at the same position on the line. You are also given an integer `k`.

You are allowed to select two segments with integer endpoints. The length of each segment must be `k`. You will collect all prizes whose position falls within at least one of the two selected segments (including the endpoints of the segments). The two selected segments may intersect.

   For example if `k = 2`, you can choose segments `[1, 3]` and `[2, 4]`, and you will win any prize i that satisfies `1 <= prizePositions[i] <= 3` or `2 <= prizePositions[i] <= 4`.

Return _the maximum number of prizes you can win if you choose the two segments optimally_.

Example 1:

Input: prizePositions = \[1,1,2,2,3,3,5\], k = 2
Output: 7
Explanation: In this example, you can win all 7 prizes by selecting two segments \[1, 3\] and \[3, 5\].

Example 2:

Input: prizePositions = \[1,2,3,4\], k = 0
Output: 2
Explanation: For this example, one choice for the segments is `[3, 3]` and `[4, 4],` and you will be able to get `2` prizes. 

Constraints:

   `1 <= prizePositions.length <= 105`
   `1 <= prizePositions[i] <= 109`
   `0 <= k <= 109`
   `prizePositions` is sorted in non-decreasing order.

.spoilerbutton {display:block; border:dashed; padding: 0px 0px; margin:10px 0px; font-size:150%; font-weight: bold; color:#000000; background-color:cyan; outline:0; } .spoiler {overflow:hidden;} .spoiler > div {-webkit-transition: all 0s ease;-moz-transition: margin 0s ease;-o-transition: all 0s ease;transition: margin 0s ease;} .spoilerbutton\[value="Show Message"\] + .spoiler > div {margin-top:-500%;} .spoilerbutton\[value="Hide Message"\] + .spoiler {padding:5px;}*

## gpt4 Reasoning

```markdown
The essential problem here is to determine the maximum number of prizes one can win by optimally choosing two segments of length `k` along the X-axis, where the positions of the prizes are given in a sorted integer array `prizePositions`. This problem is essentially about finding the optimal placement of these two segments to cover the maximum number of prize positions. 

### Key Considerations:

1. **Sorted `prizePositions`**: The fact that `prizePositions` is sorted in non-decreasing order is very helpful. It allows us to efficiently analyze the distribution of prizes along the X-axis without needing to sort the array ourselves, saving time and computational resources.

2. **Segment Length (`k`)**: The length of each segment is crucial. If `k` is 0, it means each segment can only cover exactly one position, making the problem about choosing the two positions with the most prizes. If `k` is greater than 0, each segment can cover multiple positions, and the strategy involves finding the densest clusters of prizes within a range of `k`.

3. **Overlap**: The problem allows the two segments to intersect, which means their effective combined length could be anywhere from `k` to `2k` depending on their placement. This flexibility can be exploited to cover a wider range of prize positions when they are densely packed in a certain area.

### Steps to Address the Problem:

1. **Identify Dense Regions**: Since the array is sorted, we can scan through `prizePositions` to identify regions where prizes are densely packed. This can be done by checking the difference between positions as we move through the array.

2. **Count Prizes in Segments**: For any starting position, count the number of prizes that would fall within a segment of length `k`. This requires considering the start and end points of a segment and counting the prizes that fall within these points.

3. **Optimal Placement of Two Segments**: The key challenge is determining where to place these two segments. One approach is to iterate through possible starting points for the first segment and, for each, determine the best starting point for the second segment to maximize the total number of prizes covered. This might involve checking every possible combination of two segments, which could be computationally expensive.

4. **Handling Overlaps**: When considering the placement of the second segment, it's important to account for potential overlaps with the first segment. The overlap could mean adjusting the count of prizes covered by the second segment to avoid double-counting.

5. **Optimization**: Given the potential for a large number of prize positions and the need to check many segment placements, it would be beneficial to look for ways to optimize this process. This could involve using a sliding window technique to efficiently move the segments along the X-axis without recalculating the count of covered prizes from scratch every time.

### Conclusion:

The solution involves a combination of analyzing the distribution of prizes to identify where they are most densely packed and strategically placing two segments of length `k` to cover as many of these prizes as possible. The challenge lies in efficiently calculating the optimal placement of these segments, especially in a way that scales well with large inputs. This problem touches on concepts of array manipulation, sliding window algorithms, and perhaps even dynamic programming if we look for ways to optimize the computation of overlapping segments.
```

### Metrics

```
recip_rank: 0.0026
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_144360.txt
> def place(vertices_resources, nets, machine, constraints,<br>          effort=1.0, random=default_random, on_temperature_change=None,<br>          kernel=default_kernel, kernel_kwargs={}):<br>    """A flat Simulated Annealing based placement algorithm.<br><br>    This placement algorithm uses simulated annealing directly on the supplied<br>    problem graph with the objective of reducing wire lengths (and thus,<br>    indirectly, the potential for congestion). Though computationally<br>    expensive, this placer produces relatively good placement solutions.<br><br>    The annealing temperature schedule used by this algorithm is taken from<br>    "VPR: A New Packing, Placement and Routing Tool for FPGA Research" by<br>    Vaughn Betz and Jonathan Rose from the "1997 International Workshop on<br>    Field Programmable Logic and Applications".<br><br>    Two implementations of the algorithm's kernel are available:<br><br>     :py:class:`~rig.place_and_route.place.sa.python_kernel.PythonKernel` A<br>      pure Python implementation which is available on all platforms supported<br>      by Rig.<br>     :py:class:`~rig.place_and_route.place.sa.c_kernel.CKernel` A C<br>      implementation which is typically 50-150x faster than the basic Python<br>      kernel. Since this implementation requires a C compiler during<br>      installation, it is an optional feature of Rig. See the<br>      :py:class:`CKernel's documentation<br>      <rig.place_and_route.place.sa.c_kernel.CKernel` for details.<br><br>    The fastest kernel installed is used by default and can be manually chosen<br>    using the ``kernel`` argument.<br><br>    This algorithm produces INFO level logging information describing the<br>    progress made by the algorithm.<br><br>    .. warning:<br>        This algorithm does not attempt to produce good solutions to the<br>        bin-packing problem of optimally fitting vertices into chips and it may<br>        fail if a good placement requires good bin packing.<br><br>    Parameters<br>    ----------<br>    effort : float<br>        A scaling factor for the number of iterations the algorithm should run<br>        for. 1.0 is probably about as low as you'll want to go in practice and<br>        runtime increases linearly as you increase this parameter.<br>    random : :py:class:`random.Random`<br>        A Python random number generator. Defaults to ``import random`` but can<br>        be set to your own instance of :py:class:`random.Random` to allow you<br>        to control the seed and produce deterministic results. For results to<br>        be deterministic, vertices_resources must be supplied as an<br>        :py:class:`collections.OrderedDict`.<br>    on_temperature_change : callback_function or None<br>        An (optional) callback function which is called every time the<br>        temperature is changed. This callback can be used to provide status<br>        updates<br><br>        The callback function is passed the following arguments:<br><br>         ``iteration_count``: the number of iterations the placer has<br>          attempted (integer)<br>         ``placements``: The current placement solution.<br>         ``cost``: the weighted sum over all nets of bounding-box size.<br>          (float)<br>         ``acceptance_rate``: the proportion of iterations which have resulted<br>          in an accepted change since the last callback call. (float between<br>          0.0 and 1.0)<br>         ``temperature``: The current annealing temperature. (float)<br>         ``distance_limit``: The maximum distance any swap may be made over.<br>          (integer)<br><br>        If the callback returns False, the anneal is terminated immediately and<br>        the current solution is returned.<br>    kernel : :py:class:`~rig.place_and_route.place.sa.kernel.Kernel`<br>        A simulated annealing placement kernel. A sensible default will be<br>        chosen based on the available kernels on this machine. The kernel may<br>        not be used if the placement problem has a trivial solution.<br>    kernel_kwargs : dict<br>        Optional kernel-specific keyword arguments to pass to the kernel<br>        constructor.<br>    """<br>    # Special case: just return immediately when there's nothing to place<br>    if len(vertices_resources) == 0:<br>        return {}<br><br>    # Within the algorithm we modify the resource availability values in the<br>    # machine to account for the effects of the current placement. As a result,<br>    # an internal copy of the structure must be made.<br>    machine = machine.copy()<br><br>    # {vertex: (x, y), ...} gives the location of all vertices whose position<br>    # is fixed by a LocationConstraint.<br>    fixed_vertices = {}<br><br>    # Handle constraints<br>    vertices_resources, nets, constraints, substitutions = \<br>        apply_same_chip_constraints(vertices_resources, nets, constraints)<br>    for constraint in constraints:<br>        if isinstance(constraint, LocationConstraint):<br>            # Location constraints are handled by recording the set of fixed<br>            # vertex locations and subtracting their resources from the chips<br>            # they're allocated to. These vertices will then not be added to<br>            # the internal placement data structure to prevent annealing from<br>            # moving them. They will be re-introduced at the last possible<br>            # moment.<br>            location = constraint.location<br>            if location not in machine:<br>                raise InvalidConstraintError(<br>                    "Chip requested by {} unavailable".format(machine))<br>            vertex = constraint.vertex<br><br>            # Record the constrained vertex's location<br>            fixed_vertices[vertex] = location<br><br>            # Make sure the vertex fits at the requested location (updating the<br>            # resource availability after placement)<br>            resources = vertices_resources[vertex]<br>            machine[location] = subtract_resources(machine[location],<br>                                                   resources)<br>            if overallocated(machine[location]):<br>                raise InsufficientResourceError(<br>                    "Cannot meet {}".format(constraint))<br>        elif isinstance(constraint,  # pragma: no branch<br>                        ReserveResourceConstraint):<br>            apply_reserve_resource_constraint(machine, constraint)<br><br>    # Initially randomly place the movable vertices<br>    movable_vertices = {v for v in vertices_resources<br>                        if v not in fixed_vertices}<br>    initial_placements = _initial_placement(movable_vertices,<br>                                            vertices_resources,<br>                                            machine, random)<br><br>    # Include the fixed vertices in initial placement<br>    initial_placements.update(fixed_vertices)<br><br>    # Filter out empty or singleton nets and those weighted as zero since they<br>    # cannot influence placement.<br>    nets = [n for n in nets if len(set(n))  1 and n.weight  0.0]<br><br>    # Special cases where no placement effort is required:<br>    #  There is only one chip<br>    #  There are no resource types to be consumed<br>    #  No effort is to be made<br>    #  No movable vertices<br>    #  There are no nets (and moving things has no effect)<br>    trivial = ((machine.width, machine.height) == (1, 1) or<br>               len(machine.chip_resources) == 0 or<br>               effort == 0.0 or<br>               len(movable_vertices) == 0 or<br>               len(nets) == 0)<br>    if trivial:<br>        logger.info("Placement has trivial solution. SA not used.")<br>        finalise_same_chip_constraints(substitutions, initial_placements)<br>        return initial_placements<br><br>    # Intialise the algorithm kernel<br>    k = kernel(vertices_resources, movable_vertices, set(fixed_vertices),<br>               initial_placements, nets, machine, random, kernel_kwargs)<br><br>    logger.info("SA placement kernel: %s", kernel.__name__)<br><br>    # Specifies the maximum distance any swap can span. Initially consider<br>    # swaps that span the entire machine.<br>    distance_limit = max(machine.width, machine.height)<br><br>    # Determine initial temperature according to the heuristic used by VPR: 20<br>    # times the standard deviation of len(movable_vertices) random swap costs.<br>    # The arbitrary very-high temperature is used to cause "all" swaps to be<br>    # accepted.<br>    _0, _1, cost_delta_sd = k.run_steps(len(movable_vertices),<br>                                        distance_limit,<br>                                        1e100)<br>    temperature = 20.0  cost_delta_sd<br><br>    # The number of swap-attempts between temperature changes is selected by<br>    # the heuristic used by VPR. This value is scaled linearly by the effort<br>    # parameter.<br>    num_steps = max(1, int(effort  len(vertices_resources)1.33))<br><br>    logger.info("Initial placement temperature: %0.1f", temperature)<br><br>    # Counter for the number of swap attempts made (used for diagnostic<br>    # purposes)<br>    iteration_count = 0<br><br>    # Holds the total cost of the current placement. This default value chosen<br>    # to ensure the loop below iterates at least once.<br>    current_cost = 0.0<br><br>    # The annealing algorithm runs until a heuristic termination condition<br>    # (taken from VPR) is hit. The heuristic waits until the temperature falls<br>    # below a small fraction of the average net cost.<br>    while temperature  (0.005  current_cost) / len(nets):<br>        # Run an iteration at the current temperature<br>        num_accepted, current_cost, _ = k.run_steps(<br>            num_steps, int(math.ceil(distance_limit)), temperature)<br><br>        # The ratio of accepted-to-not-accepted changes<br>        r_accept = num_accepted / float(num_steps)<br><br>        # Special case: Can't do better than 0 cost! This is a special case<br>        # since the normal termination condition will not terminate if the cost<br>        # doesn't drop below 0.<br>        if current_cost == 0:<br>            break<br><br>        # The temperature is reduced by a factor heuristically based on the<br>        # acceptance rate. The schedule below attempts to maximise the time<br>        # spent at temperatures where a large portion (but not all) of changes<br>        # are being accepted. If lots of changes are being accepted (e.g.<br>        # during high-temperature periods) then most of them are likely not to<br>        # be beneficial. If few changes are being accepted, we're probably<br>        # pretty close to the optimal placement.<br>        if r_accept  0.96:<br>            alpha = 0.5<br>        elif r_accept  0.8:<br>            alpha = 0.9<br>        elif r_accept  0.15:<br>            alpha = 0.95<br>        else:<br>            alpha = 0.8<br>        temperature = alpha  temperature<br><br>        # According to:<br>        #  M. Huang, F. Romeo, and A. Sangiovanni-Vincentelli, "An Efficient<br>        #   General Cooling Schedule for Simulated Annealing" ICCAD, 1986, pp.<br>        #   381 - 384 and J. Lam<br>        #  J. Delosme, "Performance of a New Annealing Schedule" DAC, 1988,<br>        #   pp. 306 - 311.<br>        # It is desirable to keep the acceptance ratio as close to 0.44 for as<br>        # long as possible. As a result, when r_accept falls below this we can<br>        # help increase the acceptance rate by reducing the set of possible<br>        # swap candidates based on the observation that near the end of<br>        # placement, most things are near their optimal location and thus long<br>        # distance swaps are unlikely to be useful.<br>        distance_limit = 1.0 - 0.44 + r_accept<br>        distance_limit = min(max(distance_limit, 1.0),<br>                             max(machine.width, machine.height))<br><br>        iteration_count += num_steps<br>        logger.debug("Iteration: %d, "<br>                     "Cost: %0.1f, "<br>                     "Kept: %0.1f%%, "<br>                     "Temp: %0.3f, "<br>                     "Dist: %d.",<br>                     iteration_count, current_cost,<br>                     r_accept100, temperature, math.ceil(distance_limit))<br><br>        # Call the user callback before the next iteration, terminating if<br>        # requested.<br>        if on_temperature_change is not None:<br>            placements = k.get_placements().copy()<br>            finalise_same_chip_constraints(substitutions, placements)<br>            ret_val = on_temperature_change(iteration_count,<br>                                            placements,<br>                                            current_cost,<br>                                            r_accept,<br>                                            temperature,<br>                                            distance_limit)<br>            if ret_val is False:<br>                break<br><br>    logger.info("Anneal terminated after %d iterations.", iteration_count)<br><br>    placements = k.get_placements()<br>    finalise_same_chip_constraints(substitutions, placements)<br><br>    return placements

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_309604.txt
> def calcSegments(x, v):<br>    """<br>    Find index vectors `rise` and `fall` such that `rise` holds the indeces `i`<br>    such that x[i+1]x[i] and `fall` holds indeces `j` such that either<br>    - x[j+1] < x[j] or,<br>    - x[j]x[j-1] and v[j]<v[j-1].<br><br>    The vectors are essential to the DCEGM algorithm, as they definite the<br>    relevant intervals to be used to construct the upper envelope of potential<br>    solutions to the (necessary) first order conditions.<br><br>    Parameters<br>    ----------<br>    x : np.ndarray<br>        array of points where `v` is evaluated<br>    v : np.ndarray<br>        array of values of some function of `x`<br><br>    Returns<br>    -------<br>    rise : np.ndarray<br>        see description above<br>    fall : np.ndarray<br>        see description above<br>    """<br>    # NOTE: assumes that the first segment is in fact increasing (forced in EGM<br>    # by augmentation with the constrained segment).<br>    # elements in common grid g<br><br>    # Identify index intervals of falling and rising regions<br>    # We need these to construct the upper envelope because we need to discard<br>    # solutions from the inverted Euler equations that do not represent optimal<br>    # choices (the FOCs are only necessary in these models).<br>    #<br>    # `fall` is a vector of indeces that represent the first elements in all<br>    # of the falling segments (the curve can potentially fold several times)<br>    fall = np.empty(0, dtype=int) # initialize with empty and then add the last point below while-loop<br><br>    rise = np.array([0]) # Initialize such thatthe lowest point is the first grid point<br>    i = 1 # Initialize<br>    while i <= len(x) - 2:<br>        # Check if the next (`ip1` stands for i plus 1) grid point is below the<br>        # current one, such that the line is folding back.<br>        ip1_falls = x[i+1] < x[i] # true if grid decreases on index increment<br>        i_rose = x[i]  x[i-1] # true if grid decreases on index decrement<br>        val_fell = v[i] < v[i-1] # true if value rises on index decrement<br><br>        if (ip1_falls and i_rose) or (val_fell and i_rose):<br><br>            # we are in a region where the endogenous grid is decreasing or<br>            # the value function rises by stepping back in the grid.<br>            fall = np.append(fall, i) # add the index to the vector<br><br>            # We now iterate from the current index onwards until we find point<br>            # where resources rises again. Unfortunately, we need to check<br>            # each points, as there can be multiple spells of falling endogenous<br>            # grids, so we cannot use bisection or some other fast algorithm.<br>            k = i<br>            while x[k+1] < x[k]:<br>                k = k + 1<br>            # k now holds either the next index the starts a new rising<br>            # region, or it holds the length of M, `m_len`.<br><br>            rise = np.append(rise, k)<br><br>            # Set the index to the point where resources again is rising<br>            i = k<br><br>        i = i + 1<br><br>    # Add the last index for convenience (then all segments are complete, as<br>    # len(fall) == len(rise), and we can form them by range(rise[j], fall[j]+1).<br>    fall = np.append(fall, len(v)-1)<br><br>    return rise, fall

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_251863.txt
> def spectral_clustering(geom, K, eigen_solver = 'dense', random_state = None, solver_kwds = None, <br>                        renormalize = True, stabalize = True, additional_vectors = 0):<br>    """<br>    Spectral clustering for find K clusters by using the eigenvectors of a <br>    matrix which is derived from a set of similarities S.<br><br>    Parameters<br>    -----------<br>    S: array-like,shape(n_sample,n_sample)<br>        similarity matrix <br>    K: integer<br>        number of K clusters<br>    eigen_solver : {'auto', 'dense', 'arpack', 'lobpcg', or 'amg'}<br>        'auto' :<br>            algorithm will attempt to choose the best method for input data<br>        'dense' :<br>            use standard dense matrix operations for the eigenvalue decomposition.<br>            For this method, M must be an array or matrix type.  This method should be avoided for large problems.<br>        'arpack' :<br>            use arnoldi iteration in shift-invert mode. For this method,<br>            M may be a dense matrix, sparse matrix, or general linear operator.<br>            Warning: ARPACK can be unstable for some problems.  It is best to<br>            try several random seeds in order to check results.<br>        'lobpcg' :<br>            Locally Optimal Block Preconditioned Conjugate Gradient Method.<br>            A preconditioned eigensolver for large symmetric positive definite<br>            (SPD) generalized eigenproblems.<br>        'amg' :<br>            AMG requires pyamg to be installed. It can be faster on very large,<br>            sparse problems, but may also lead to instabilities.  <br>            <br>    random_state : numpy.RandomState or int, optional<br>        The generator or seed used to determine the starting vector for arpack<br>        iterations.  Defaults to numpy.random.RandomState    <br>    solver_kwds : any additional keyword arguments to pass to the selected eigen_solver <br>    renormalize : (bool) whether or not to set the rows of the eigenvectors to have norm 1 <br>                 this can improve label quality<br>    stabalize : (bool) whether or not to compute the (more stable) eigenvectors of L = D^-1/2SD^-1/2<br>                instead of P = D^-1S <br>    additional_vectors : (int) compute additional eigen vectors when computing eigen decomposition.<br>        When eigen_solver = 'amg' or 'lopcg' often if a small number of eigen values is sought the<br>        largest eigenvalue returned is not equal to 1 (it should be). This can usually be fixed<br>        by requesting more than K eigenvalues until the first eigenvalue is close to 1 and then<br>        omitted. The remaining K-1 eigenvectors should be informative. <br>    Returns<br>    -------<br>    labels: array-like, shape (1,n_samples)<br>    """ <br>    # Step 1: get similarity matrix<br>    if geom.affinity_matrix is None:<br>        S = geom.compute_affinity_matrix()<br>    else:<br>        S = geom.affinity_matrix<br>        <br>    # Check for stability method, symmetric solvers require this<br>    if eigen_solver in ['lobpcg', 'amg']:<br>        stabalize = True<br>    if stabalize:<br>        geom.laplacian_type = 'symmetricnormalized'<br>        return_lapsym = True<br>    else:<br>        geom.laplacian_type = 'randomwalk'<br>        return_lapsym = False<br>    <br>    # Step 2: get the Laplacian matrix<br>    P = geom.compute_laplacian_matrix(return_lapsym = return_lapsym)<br>    # by default the Laplacian is subtracted from the Identify matrix (this step may not be needed)<br>    P += identity(P.shape[0])        <br>    <br>    # Step 3: Compute the top K eigenvectors and drop the first <br>    if eigen_solver in ['auto', 'amg', 'lobpcg']:<br>        n_components = 2int(np.log(P.shape[0]))K + 1<br>        n_components += int(additional_vectors)<br>    else:<br>        n_components = K<br>    n_components = min(n_components, P.shape[0])<br>    (lambdas, eigen_vectors) = eigen_decomposition(P, n_components=n_components, eigen_solver=eigen_solver, <br>                                                   random_state=random_state, drop_first = True,<br>                                                   solver_kwds=solver_kwds)<br>    # the first vector is usually uninformative <br>    if eigen_solver in ['auto', 'lobpcg', 'amg']:<br>        if np.abs(lambdas[0] - 1)  1e-4:<br>            warnings.warn("largest eigenvalue not equal to 1. Results may be poor. Try increasing additional_vectors parameter")<br>    eigen_vectors = eigen_vectors[:, 1:K]<br>    lambdas = lambdas[1:K]<br>    <br>    # If stability method chosen, adjust eigenvectors<br>    if stabalize:<br>        w = np.array(geom.laplacian_weights)<br>        eigen_vectors /= np.sqrt(w[:,np.newaxis])<br>        eigen_vectors /= np.linalg.norm(eigen_vectors, axis = 0)    <br>    <br>    # If renormalize: set each data point to unit length<br>    if renormalize:<br>        norms = np.linalg.norm(eigen_vectors, axis=1)<br>        eigen_vectors /= norms[:,np.newaxis]<br>    <br>    # Step 4: run k-means clustering<br>    labels =  k_means_clustering(eigen_vectors,K)    <br>    return labels, eigen_vectors, P

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_162355.txt
> def ranking_game(n, steps=10, random_state=None):<br>    """<br>    Return a NormalFormGame instance of (the 2-player version of) the<br>    "ranking game" studied by Goldberg et al. (2013), where each player<br>    chooses an effort level associated with a score and a cost which are<br>    both increasing functions with randomly generated step sizes. The<br>    player with the higher score wins the first prize, whose value is 1,<br>    and the other player obtains the "second prize" of value 0; in the<br>    case of a tie, the first prize is split and each player receives a<br>    value of 0.5. The payoff of a player is given by the value of the<br>    prize minus the cost of the effort.<br><br>    Parameters<br>    ----------<br>    n : scalar(int)<br>        Number of actions, i.e, number of possible effort levels.<br>    steps : scalar(int), optional(default=10)<br>        Parameter determining the upper bound for the size of the random<br>        steps for the scores and costs for each player: The step sizes<br>        for the scores are drawn from `1`, ..., `steps`, while those for<br>        the costs are multiples of `1/(nsteps)`, where the cost of<br>        effort level `0` is 0, and the maximum possible cost of effort<br>        level `n-1` is less than or equal to 1.<br>    random_state : int or np.random.RandomState, optional<br>        Random seed (integer) or np.random.RandomState instance to set<br>        the initial state of the random number generator for<br>        reproducibility. If None, a randomly initialized RandomState is<br>        used.<br><br>    Returns<br>    -------<br>    g : NormalFormGame<br><br>    Examples<br>    --------<br>     g = ranking_game(5, random_state=1234)<br>     g.players[0]<br>    Player([[ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ],<br>            [ 0.82, -0.18, -0.18, -0.18, -0.18],<br>            [ 0.8 ,  0.8 , -0.2 , -0.2 , -0.2 ],<br>            [ 0.68,  0.68,  0.68, -0.32, -0.32],<br>            [ 0.66,  0.66,  0.66,  0.66, -0.34]])<br>     g.players[1]<br>    Player([[ 1.  ,  0.  ,  0.  ,  0.  ,  0.  ],<br>            [ 0.8 ,  0.8 , -0.2 , -0.2 , -0.2 ],<br>            [ 0.66,  0.66,  0.66, -0.34, -0.34],<br>            [ 0.6 ,  0.6 ,  0.6 ,  0.6 , -0.4 ],<br>            [ 0.58,  0.58,  0.58,  0.58,  0.58]])<br><br>    """<br>    payoff_arrays = tuple(np.empty((n, n)) for i in range(2))<br>    random_state = check_random_state(random_state)<br><br>    scores = random_state.randint(1, steps+1, size=(2, n))<br>    scores.cumsum(axis=1, out=scores)<br><br>    costs = np.empty((2, n-1))<br>    costs[:] = random_state.randint(1, steps+1, size=(2, n-1))<br>    costs.cumsum(axis=1, out=costs)<br>    costs[:] /= (n  steps)<br><br>    _populate_ranking_payoff_arrays(payoff_arrays, scores, costs)<br>    g = NormalFormGame(<br>        [Player(payoff_array) for payoff_array in payoff_arrays]<br>    )<br>    return g

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_144450.txt
> def ner_net(source, destinations, width, height, wrap_around=False, radius=10):<br>    """Produce a shortest path tree for a given net using NER.<br><br>    This is the kernel of the NER algorithm.<br><br>    Parameters<br>    ----------<br>    source : (x, y)<br>        The coordinate of the source vertex.<br>    destinations : iterable([(x, y), ...])<br>        The coordinates of destination vertices.<br>    width : int<br>        Width of the system (nodes)<br>    height : int<br>        Height of the system (nodes)<br>    wrap_around : bool<br>        True if wrap-around links should be used, false if they should be<br>        avoided.<br>    radius : int<br>        Radius of area to search from each node. 20 is arbitrarily selected in<br>        the paper and shown to be acceptable in practice.<br><br>    Returns<br>    -------<br>    (:py:class:`~.rig.place_and_route.routing_tree.RoutingTree`,<br>     {(x,y): :py:class:`~.rig.place_and_route.routing_tree.RoutingTree`, ...})<br>        A RoutingTree is produced rooted at the source and visiting all<br>        destinations but which does not contain any vertices etc. For<br>        convenience, a dictionarry mapping from destination (x, y) coordinates<br>        to the associated RoutingTree is provided to allow the caller to insert<br>        these items.<br>    """<br>    # Map from (x, y) to RoutingTree objects<br>    route = {source: RoutingTree(source)}<br><br>    # Handle each destination, sorted by distance from the source, closest<br>    # first.<br>    for destination in sorted(destinations,<br>                              key=(lambda destination:<br>                                   shortest_mesh_path_length(<br>                                       to_xyz(source), to_xyz(destination))<br>                                   if not wrap_around else<br>                                   shortest_torus_path_length(<br>                                       to_xyz(source), to_xyz(destination),<br>                                       width, height))):<br>        # We shall attempt to find our nearest neighbouring placed node.<br>        neighbour = None<br><br>        # Try to find a nearby (within radius hops) node in the routing tree<br>        # that we can route to (falling back on just routing to the source).<br>        #<br>        # In an implementation according to the algorithm's original<br>        # specification looks for nodes at each point in a growing set of rings<br>        # of concentric hexagons. If it doesn't find any destinations this<br>        # means an awful lot of checks: 1261 for the default radius of 20.<br>        #<br>        # An alternative (but behaviourally identical) implementation scans the<br>        # list of all route nodes created so far and finds the closest node<br>        # which is < radius hops (falling back on the origin if no node is<br>        # closer than radius hops).  This implementation requires one check per<br>        # existing route node. In most routes this is probably a lot less than<br>        # 1261 since most routes will probably have at most a few hundred route<br>        # nodes by the time the last destination is being routed.<br>        #<br>        # Which implementation is best is a difficult question to answer:<br>        #  In principle nets with quite localised connections (e.g.<br>        #   nearest-neighbour or centroids traffic) may route slightly more<br>        #   quickly with the original algorithm since it may very quickly find<br>        #   a neighbour.<br>        #  In nets which connect very spaced-out destinations the second<br>        #   implementation may be quicker since in such a scenario it is<br>        #   unlikely that a neighbour will be found.<br>        #  In extremely high-fan-out nets (e.g. broadcasts), the original<br>        #   method is very likely to perform far better than the alternative<br>        #   method since most iterations will complete immediately while the<br>        #   alternative method must scan all the route vertices.<br>        # As such, it should be clear that neither method alone is 'best' and<br>        # both have degenerate performance in certain completely reasonable<br>        # styles of net. As a result, a simple heuristic is used to decide<br>        # which technique to use.<br>        #<br>        # The following micro-benchmarks are crude estimate of the<br>        # runtime-per-iteration of each approach (at least in the case of a<br>        # torus topology)::<br>        #<br>        #     $ # Original approach<br>        #     $ python -m timeit --setup 'x, y, w, h, r = 1, 2, 5, 10, \<br>        #                                     {x:None for x in range(10)}' \<br>        #                        'x += 1; y += 1; x %= w; y %= h; (x, y) in r'<br>        #     1000000 loops, best of 3: 0.207 usec per loop<br>        #     $ # Alternative approach<br>        #     $ python -m timeit --setup 'from rig.geometry import \<br>        #                                 shortest_torus_path_length' \<br>        #                        'shortest_torus_path_length( \<br>        #                             (0, 1, 2), (3, 2, 1), 10, 10)'<br>        #     1000000 loops, best of 3: 0.666 usec per loop<br>        #<br>        # From this we can approximately suggest that the alternative approach<br>        # is 3x more expensive per iteration. A very crude heuristic is to use<br>        # the original approach when the number of route nodes is more than<br>        # 1/3rd of the number of routes checked by the original method.<br>        concentric_hexagons = memoized_concentric_hexagons(radius)<br>        if len(concentric_hexagons) < len(route) / 3:<br>            # Original approach: Start looking for route nodes in a concentric<br>            # spiral pattern out from the destination node.<br>            for x, y in concentric_hexagons:<br>                x += destination[0]<br>                y += destination[1]<br>                if wrap_around:<br>                    x %= width<br>                    y %= height<br>                if (x, y) in route:<br>                    neighbour = (x, y)<br>                    break<br>        else:<br>            # Alternative approach: Scan over every route node and check to see<br>            # if any are < radius, picking the closest one if so.<br>            neighbour = None<br>            neighbour_distance = None<br>            for candidate_neighbour in route:<br>                if wrap_around:<br>                    distance = shortest_torus_path_length(<br>                        to_xyz(candidate_neighbour), to_xyz(destination),<br>                        width, height)<br>                else:<br>                    distance = shortest_mesh_path_length(<br>                        to_xyz(candidate_neighbour), to_xyz(destination))<br><br>                if distance <= radius and (neighbour is None or<br>                                           distance < neighbour_distance):<br>                    neighbour = candidate_neighbour<br>                    neighbour_distance = distance<br><br>        # Fall back on routing directly to the source if no nodes within radius<br>        # hops of the destination was found.<br>        if neighbour is None:<br>            neighbour = source<br><br>        # Find the shortest vector from the neighbour to this destination<br>        if wrap_around:<br>            vector = shortest_torus_path(to_xyz(neighbour),<br>                                         to_xyz(destination),<br>                                         width, height)<br>        else:<br>            vector = shortest_mesh_path(to_xyz(neighbour), to_xyz(destination))<br><br>        # The longest-dimension-first route may inadvertently pass through an<br>        # already connected node. If the route is allowed to pass through that<br>        # node it would create a cycle in the route which would be VeryBad(TM).<br>        # As a result, we work backward through the route and truncate it at<br>        # the first point where the route intersects with a connected node.<br>        ldf = longest_dimension_first(vector, neighbour, width, height)<br>        i = len(ldf)<br>        for direction, (x, y) in reversed(ldf):<br>            i -= 1<br>            if (x, y) in route:<br>                # We've just bumped into a node which is already part of the<br>                # route, this becomes our new neighbour and we truncate the LDF<br>                # route. (Note ldf list is truncated just after the current<br>                # position since it gives (direction, destination) pairs).<br>                neighbour = (x, y)<br>                ldf = ldf[i + 1:]<br>                break<br><br>        # Take the longest dimension first route.<br>        last_node = route[neighbour]<br>        for direction, (x, y) in ldf:<br>            this_node = RoutingTree((x, y))<br>            route[(x, y)] = this_node<br><br>            last_node.children.append((Routes(direction), this_node))<br>            last_node = this_node<br><br>    return (route[source], route)


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_123.txt
> def maxProfit(prices):<br>    """You are given an array `prices` where `prices[i]` is the price of a given stock on the `ith` day.<br><br>Find the maximum profit you can achieve. You may complete at most two transactions.<br><br>Note: You may not engage in multiple transactions simultaneously (i.e., you must sell the stock before you buy again).<br><br>Example 1:<br><br>Input: prices = \[3,3,5,0,0,3,1,4\]<br>Output: 6<br>Explanation: Buy on day 4 (price = 0) and sell on day 6 (price = 3), profit = 3-0 = 3.<br>Then buy on day 7 (price = 1) and sell on day 8 (price = 4), profit = 4-1 = 3.<br><br>Example 2:<br><br>Input: prices = \[1,2,3,4,5\]<br>Output: 4<br>Explanation: Buy on day 1 (price = 1) and sell on day 5 (price = 5), profit = 5-1 = 4.<br>Note that you cannot buy on day 1, buy on day 2 and sell them later, as you are engaging multiple transactions at the same time. You must sell before buying again.<br><br>Example 3:<br><br>Input: prices = \[7,6,4,3,1\]<br>Output: 0<br>Explanation: In this case, no transaction is done, i.e. max profit = 0.<br><br>Constraints:<br><br>   `1 <= prices.length <= 105`<br>   `0 <= prices[i] <= 105`"""<br><br>    buy1, buy2 = float("inf"), float("inf")<br>    profit1, profit2 = 0, 0<br><br>    for price in prices:<br>        buy1 = min(buy1, price)<br>        profit1 = max(profit1, price - buy1)<br>        buy2 = min(buy2, price - profit1)<br>        profit2 = max(profit2, price - buy2)<br><br>    return profit2

### GROUND TRUTH 1, ranked not in top 100, leetcode/leetcode_2054.txt
> def find_chair(times, target_friend):<br>    """You are given a 0-indexed 2D integer array of `events` where `events[i] = [startTimei, endTimei, valuei]`. The `ith` event starts at `startTimei` and ends at `endTimei`, and if you attend this event, you will receive a value of `valuei`. You can choose at most two non-overlapping events to attend such that the sum of their values is maximized.<br><br>Return _this maximum sum._<br><br>Note that the start time and end time is inclusive: that is, you cannot attend two events where one of them starts and the other ends at the same time. More specifically, if you attend an event with end time `t`, the next event must start at or after `t + 1`.<br><br>Example 1:<br><br>Input: events = \[\[1,3,2\],\[4,5,2\],\[2,4,3\]\]<br>Output: 4<br>Explanation: Choose the green events, 0 and 1 for a sum of 2 + 2 = 4.<br><br>Example 2:<br><br>Input: events = \[\[1,3,2\],\[4,5,2\],\[1,5,5\]\]<br>Output: 5<br>Explanation: Choose event 2 for a sum of 5.<br><br>Example 3:<br><br>Input: events = \[\[1,5,3\],\[1,5,1\],\[6,6,5\]\]<br>Output: 8<br>Explanation: Choose events 0 and 2 for a sum of 3 + 5 = 8.<br><br>Constraints:<br><br>   `2 <= events.length <= 105`<br>   `events[i].length == 3`<br>   `1 <= startTimei <= endTimei <= 109`<br>   `1 <= valuei <= 106`"""<br><br>    n = len(times)<br>    events = []<br>    for i, (arrival, leaving) in enumerate(times):<br>        events.append((arrival, i))<br>        events.append((leaving, ~i))<br>    events.sort()<br>    chairs = [-1]  n<br>    available_chairs = sorted(range(n))<br>    for time, friend_id in events:<br>        if friend_id = 0:<br>            chairs[friend_id] = available_chairs.pop(0)<br>            if friend_id == target_friend:<br>                return chairs[friend_id]<br>        else:<br>            available_chairs.append(chairs[~friend_id])<br>            available_chairs.sort()<br>    return -1
