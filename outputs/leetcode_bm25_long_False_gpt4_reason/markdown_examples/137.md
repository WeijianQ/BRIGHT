# Query `137`

## Original Question

*There are `k` workers who want to move `n` boxes from an old warehouse to a new one. You are given the two integers `n` and `k`, and a 2D integer array `time` of size `k x 4` where `time[i] = [leftToRighti, pickOldi, rightToLefti, putNewi]`.

The warehouses are separated by a river and connected by a bridge. The old warehouse is on the right bank of the river, and the new warehouse is on the left bank of the river. Initially, all `k` workers are waiting on the left side of the bridge. To move the boxes, the `ith` worker (0-indexed) can :

   Cross the bridge from the left bank (new warehouse) to the right bank (old warehouse) in `leftToRighti` minutes.
   Pick a box from the old warehouse and return to the bridge in `pickOldi` minutes. Different workers can pick up their boxes simultaneously.
   Cross the bridge from the right bank (old warehouse) to the left bank (new warehouse) in `rightToLefti` minutes.
   Put the box in the new warehouse and return to the bridge in `putNewi` minutes. Different workers can put their boxes simultaneously.

A worker `i` is less efficient than a worker `j` if either condition is met:

   `leftToRighti + rightToLefti > leftToRightj + rightToLeftj`
   `leftToRighti + rightToLefti == leftToRightj + rightToLeftj` and `i > j`

The following rules regulate the movement of the workers through the bridge :

   If a worker `x` reaches the bridge while another worker `y` is crossing the bridge, `x` waits at their side of the bridge.
   If the bridge is free, the worker waiting on the right side of the bridge gets to cross the bridge. If more than one worker is waiting on the right side, the one with the lowest efficiency crosses first.
   If the bridge is free and no worker is waiting on the right side, and at least one box remains at the old warehouse, the worker on the left side of the river gets to cross the bridge. If more than one worker is waiting on the left side, the one with the lowest efficiency crosses first.

Return _the instance of time at which the last worker reaches the left bank of the river after all n boxes have been put in the new warehouse_.

Example 1:

Input: n = 1, k = 3, time = \[\[1,1,2,1\],\[1,1,3,1\],\[1,1,4,1\]\]
Output: 6
Explanation: 
From 0 to 1: worker 2 crosses the bridge from the left bank to the right bank.
From 1 to 2: worker 2 picks up a box from the old warehouse.
From 2 to 6: worker 2 crosses the bridge from the right bank to the left bank.
From 6 to 7: worker 2 puts a box at the new warehouse.
The whole process ends after 7 minutes. We return 6 because the problem asks for the instance of time at which the last worker reaches the left bank.

Example 2:

Input: n = 3, k = 2, time = \[\[1,9,1,8\],\[10,10,10,10\]\]
Output: 50
Explanation: 
From 0  to 10: worker 1 crosses the bridge from the left bank to the right bank.
From 10 to 20: worker 1 picks up a box from the old warehouse.
From 10 to 11: worker 0 crosses the bridge from the left bank to the right bank.
From 11 to 20: worker 0 picks up a box from the old warehouse.
From 20 to 30: worker 1 crosses the bridge from the right bank to the left bank.
From 30 to 40: worker 1 puts a box at the new warehouse.
From 30 to 31: worker 0 crosses the bridge from the right bank to the left bank.
From 31 to 39: worker 0 puts a box at the new warehouse.
From 39 to 40: worker 0 crosses the bridge from the left bank to the right bank.
From 40 to 49: worker 0 picks up a box from the old warehouse.
From 49 to 50: worker 0 crosses the bridge from the right bank to the left bank.
From 50 to 58: worker 0 puts a box at the new warehouse.
The whole process ends after 58 minutes. We return 50 because the problem asks for the instance of time at which the last worker reaches the left bank.

Constraints:

   `1 <= n, k <= 104`
   `time.length == k`
   `time[i].length == 4`
   `1 <= leftToRighti, pickOldi, rightToLefti, putNewi <= 1000`*

## gpt4 Reasoning

```markdown
The essential problem here is to calculate the minimum time it takes for all `n` boxes to be moved from the old warehouse to the new warehouse across a bridge, considering the time it takes each of the `k` workers to perform each part of the task and the rules governing their movement. The solution must account for the efficiency of each worker and the constraints on bridge crossing.

### Breaking Down the Problem:

1. **Efficiency Calculation**: We need to determine the efficiency of each worker based on their total time to cross the bridge back and forth (`leftToRighti + rightToLefti`). Workers are also ranked by their index if their total crossing times are equal.

2. **Worker Queue Management**: Workers on both sides of the bridge need to be managed in queues - one for the left bank and one for the right bank. The challenge is to manage these queues according to the rules specified, ensuring that the most efficient worker available crosses the bridge when it is free.

3. **Simulating the Process**: The movement of boxes involves several steps for each worker, and workers can operate in parallel. We need to simulate this process, keeping track of the time each worker spends on each part of the task and managing the bridge crossing rules.

4. **Calculating Total Time**: The objective is to find the time when the last worker reaches the left bank after all boxes have been moved. This requires tracking the progress of each box and the location of each worker.

### Relevant Information:

- The total number of boxes `n` and the total number of workers `k`.
- The time array `time[k][4]` detailing the time each worker takes for each part of the task.
- The rules for crossing the bridge and how workers' efficiency impacts the order of crossing.

### Approach:

1. **Initialize Variables**: Create data structures to keep track of workers' states, the number of boxes remaining, and the current time.

2. **Sort Workers by Efficiency**: Sort the workers based on their efficiency to easily determine who crosses the bridge first when it becomes free.

3. **Simulation**:
   - Loop through the process, incrementing the time as necessary until all boxes have been moved.
   - At each step, decide whether a worker can start crossing the bridge, pick up a box, cross back, or put down a box, based on their current state and the rules.
   - Manage queues for both sides of the bridge to ensure the correct worker crosses when the bridge is free.

4. **Bridge Crossing Rules**: Implement logic to enforce the rules about bridge crossing, ensuring that workers wait for the bridge to be free and that the most efficient worker available crosses first.

5. **Ending Condition**: The loop continues until all `n` boxes are moved to the new warehouse. The time when the last worker reaches the left bank (regardless of whether they are carrying a box) is the answer.

6. **Return the Final Time**: Once all boxes are moved, and all workers are back at the left bank, return the current time as the solution.

### Challenges:

- Simultaneously managing multiple workers and their tasks can be complex, especially considering the bridge's constraints.
- Efficiently updating the state of each worker and the bridge at each time step requires careful thought to avoid errors.

### Solution Sketch:

```python
# Pseudocode

# Sort workers by efficiency
sort workers by (leftToRight + rightToLeft, index)

# Initialize variables
current_time = 0
tasks = initialize a priority queue or similar structure to manage tasks and timing

# Loop until all boxes are moved
while boxes > 0:
    # Update tasks and worker states based on the current time
    update_tasks_and_workers(current_time)
    
    # Check if any worker can start crossing the bridge or perform other actions
    check_and_update_worker_actions()
    
    # Increment the current time to the next significant event (e.g., a worker finishing crossing the bridge)
    current_time = find_next_event_time()

# Return the time when the last worker reaches the left bank
return current_time
```

This approach focuses on simulating the entire process step by step, managing the efficiency and tasks of workers, and adhering to the specified rules for crossing the bridge.
```

### Metrics

```
recip_rank: 0.0417
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0400
P_50: 0.0200
P_100: 0.0100
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.5000
recall_50: 0.5000
recall_100: 0.5000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.1320
ndcg_cut_50: 0.1320
ndcg_cut_100: 0.1320
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0208
map_cut_50: 0.0208
map_cut_100: 0.0208
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_64048.txt
> def step_until_intersect(pos, field_line, sign, time,  direction=None,<br>                        step_size_goal=5., <br>                        field_step_size=None):   <br>    """Starting at pos, method steps along magnetic unit vector direction <br>    towards the supplied field line trace. Determines the distance of <br>    closest approach to field line.<br>    <br>    Routine is used when calculting the mapping of electric fields along <br>    magnetic field lines. Voltage remains constant along the field but the <br>    distance between field lines does not.This routine may be used to form the <br>    last leg when trying to trace out a closed field line loop.<br>    <br>    Routine will create a high resolution field line trace (.01 km step size) <br>    near the location of closest approach to better determine where the <br>    intersection occurs. <br>    <br>    Parameters<br>    ----------<br>    pos : array-like<br>        X, Y, and Z ECEF locations to start from<br>    field_line : array-like (:,3)<br>        X, Y, and Z ECEF locations of field line trace, produced by the<br>        field_line_trace method.<br>    sign : int<br>        if 1, move along positive unit vector. Negwtive direction for -1.<br>    time : datetime or float<br>        Date to perform tracing on (year + day/365 + hours/24. + etc.)<br>        Accounts for leap year if datetime provided.<br>    direction : string ('meridional', 'zonal', or 'aligned')<br>        Which unit vector direction to move slong when trying to intersect<br>        with supplied field line trace. See step_along_mag_unit_vector method<br>        for more.<br>    step_size_goal : float<br>        step size goal that method will try to match when stepping towards field line. <br>    <br>    Returns<br>    -------<br>    (float, array, float)<br>        Total distance taken along vector direction; the position after taking <br>        the step [x, y, z] in ECEF; distance of closest approach from input pos <br>        towards the input field line trace.<br>         <br>    """ <br>                                                         <br>    # work on a copy, probably not needed<br>    field_copy = field_line<br>    # set a high last minimum distance to ensure first loop does better than this<br>    last_min_dist = 2500000.<br>    # scalar is the distance along unit vector line that we are taking<br>    scalar = 0.<br>    # repeat boolean<br>    repeat=True<br>    # first run boolean<br>    first=True<br>    # factor is a divisor applied to the remaining distance between point and field line<br>    # I slowly take steps towards the field line and I don't want to overshoot<br>    # each time my minimum distance increases, I step back, increase factor, reducing<br>    # my next step size, then I try again<br>    factor = 1<br>    while repeat:<br>        # take a total step along magnetic unit vector<br>        # try to take steps near user provided step_size_goal<br>        unit_steps = np.abs(scalar//step_size_goal)<br>        if unit_steps == 0:<br>            unit_steps = 1<br>        # print (unit_steps, scalar/unit_steps)<br>        pos_step = step_along_mag_unit_vector(pos[0], pos[1], pos[2], time, <br>                                              direction=direction,<br>                                              num_steps=unit_steps, <br>                                              step_size=np.abs(scalar)/unit_steps,<br>                                              scalar=sign) <br>        # find closest point along field line trace<br>        diff = field_copy - pos_step<br>        diff_mag = np.sqrt((diff  2).sum(axis=1))<br>        min_idx = np.argmin(diff_mag)<br>        if first:<br>            # first time in while loop, create some information<br>            # make a high resolution field line trace around closest distance<br>            # want to take a field step size in each direction<br>            # maintain accuracy of high res trace below to be .01 km<br>            init = field_copy[min_idx,:]<br>            field_copy = full_field_line(init, time, 0.,<br>                                         step_size=0.01, <br>                                         max_steps=int(field_step_size/.01),<br>                                         recurse=False)<br>            # difference with position<br>            diff = field_copy - pos_step<br>            diff_mag = np.sqrt((diff  2).sum(axis=1))<br>            # find closest one<br>            min_idx = np.argmin(diff_mag)<br>            # # reduce number of elements we really need to check<br>            # field_copy = field_copy[min_idx-100:min_idx+100]<br>            # # difference with position<br>            # diff = field_copy - pos_step<br>            # diff_mag = np.sqrt((diff  2).sum(axis=1))<br>            # # find closest one<br>            # min_idx = np.argmin(diff_mag)<br>            first = False<br>            <br>        # pull out distance of closest point <br>        min_dist = diff_mag[min_idx]<br>        <br>        # check how the solution is doing<br>        # if well, add more distance to the total step and recheck if closer<br>        # if worse, step back and try a smaller step<br>        if min_dist  last_min_dist:<br>            # last step we took made the solution worse<br>            if factor  4:<br>                # we've tried enough, stop looping<br>                repeat = False<br>                # undo increment to last total distance<br>                scalar = scalar - last_min_dist/(2factor)<br>                # calculate latest position<br>                pos_step = step_along_mag_unit_vector(pos[0], pos[1], pos[2], <br>                                        time, <br>                                        direction=direction,<br>                                        num_steps=unit_steps, <br>                                        step_size=np.abs(scalar)/unit_steps,<br>                                        scalar=sign) <br>            else:<br>                # undo increment to last total distance<br>                scalar = scalar - last_min_dist/(2factor)<br>                # increase the divisor used to reduce the distance <br>                # actually stepped per increment<br>                factor = factor + 1.<br>                # try a new increment to total distance<br>                scalar = scalar + last_min_dist/(2factor)<br>        else:<br>            # we did better, move even closer, a fraction of remaining distance<br>            # increment scalar, but only by a fraction<br>            scalar = scalar + min_dist/(2factor)<br>            # we have a new standard to judge against, set it<br>            last_min_dist = min_dist.copy()<br><br>    # return magnitude of step<br>    return scalar, pos_step, min_dist

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_144360.txt
> def place(vertices_resources, nets, machine, constraints,<br>          effort=1.0, random=default_random, on_temperature_change=None,<br>          kernel=default_kernel, kernel_kwargs={}):<br>    """A flat Simulated Annealing based placement algorithm.<br><br>    This placement algorithm uses simulated annealing directly on the supplied<br>    problem graph with the objective of reducing wire lengths (and thus,<br>    indirectly, the potential for congestion). Though computationally<br>    expensive, this placer produces relatively good placement solutions.<br><br>    The annealing temperature schedule used by this algorithm is taken from<br>    "VPR: A New Packing, Placement and Routing Tool for FPGA Research" by<br>    Vaughn Betz and Jonathan Rose from the "1997 International Workshop on<br>    Field Programmable Logic and Applications".<br><br>    Two implementations of the algorithm's kernel are available:<br><br>     :py:class:`~rig.place_and_route.place.sa.python_kernel.PythonKernel` A<br>      pure Python implementation which is available on all platforms supported<br>      by Rig.<br>     :py:class:`~rig.place_and_route.place.sa.c_kernel.CKernel` A C<br>      implementation which is typically 50-150x faster than the basic Python<br>      kernel. Since this implementation requires a C compiler during<br>      installation, it is an optional feature of Rig. See the<br>      :py:class:`CKernel's documentation<br>      <rig.place_and_route.place.sa.c_kernel.CKernel` for details.<br><br>    The fastest kernel installed is used by default and can be manually chosen<br>    using the ``kernel`` argument.<br><br>    This algorithm produces INFO level logging information describing the<br>    progress made by the algorithm.<br><br>    .. warning:<br>        This algorithm does not attempt to produce good solutions to the<br>        bin-packing problem of optimally fitting vertices into chips and it may<br>        fail if a good placement requires good bin packing.<br><br>    Parameters<br>    ----------<br>    effort : float<br>        A scaling factor for the number of iterations the algorithm should run<br>        for. 1.0 is probably about as low as you'll want to go in practice and<br>        runtime increases linearly as you increase this parameter.<br>    random : :py:class:`random.Random`<br>        A Python random number generator. Defaults to ``import random`` but can<br>        be set to your own instance of :py:class:`random.Random` to allow you<br>        to control the seed and produce deterministic results. For results to<br>        be deterministic, vertices_resources must be supplied as an<br>        :py:class:`collections.OrderedDict`.<br>    on_temperature_change : callback_function or None<br>        An (optional) callback function which is called every time the<br>        temperature is changed. This callback can be used to provide status<br>        updates<br><br>        The callback function is passed the following arguments:<br><br>         ``iteration_count``: the number of iterations the placer has<br>          attempted (integer)<br>         ``placements``: The current placement solution.<br>         ``cost``: the weighted sum over all nets of bounding-box size.<br>          (float)<br>         ``acceptance_rate``: the proportion of iterations which have resulted<br>          in an accepted change since the last callback call. (float between<br>          0.0 and 1.0)<br>         ``temperature``: The current annealing temperature. (float)<br>         ``distance_limit``: The maximum distance any swap may be made over.<br>          (integer)<br><br>        If the callback returns False, the anneal is terminated immediately and<br>        the current solution is returned.<br>    kernel : :py:class:`~rig.place_and_route.place.sa.kernel.Kernel`<br>        A simulated annealing placement kernel. A sensible default will be<br>        chosen based on the available kernels on this machine. The kernel may<br>        not be used if the placement problem has a trivial solution.<br>    kernel_kwargs : dict<br>        Optional kernel-specific keyword arguments to pass to the kernel<br>        constructor.<br>    """<br>    # Special case: just return immediately when there's nothing to place<br>    if len(vertices_resources) == 0:<br>        return {}<br><br>    # Within the algorithm we modify the resource availability values in the<br>    # machine to account for the effects of the current placement. As a result,<br>    # an internal copy of the structure must be made.<br>    machine = machine.copy()<br><br>    # {vertex: (x, y), ...} gives the location of all vertices whose position<br>    # is fixed by a LocationConstraint.<br>    fixed_vertices = {}<br><br>    # Handle constraints<br>    vertices_resources, nets, constraints, substitutions = \<br>        apply_same_chip_constraints(vertices_resources, nets, constraints)<br>    for constraint in constraints:<br>        if isinstance(constraint, LocationConstraint):<br>            # Location constraints are handled by recording the set of fixed<br>            # vertex locations and subtracting their resources from the chips<br>            # they're allocated to. These vertices will then not be added to<br>            # the internal placement data structure to prevent annealing from<br>            # moving them. They will be re-introduced at the last possible<br>            # moment.<br>            location = constraint.location<br>            if location not in machine:<br>                raise InvalidConstraintError(<br>                    "Chip requested by {} unavailable".format(machine))<br>            vertex = constraint.vertex<br><br>            # Record the constrained vertex's location<br>            fixed_vertices[vertex] = location<br><br>            # Make sure the vertex fits at the requested location (updating the<br>            # resource availability after placement)<br>            resources = vertices_resources[vertex]<br>            machine[location] = subtract_resources(machine[location],<br>                                                   resources)<br>            if overallocated(machine[location]):<br>                raise InsufficientResourceError(<br>                    "Cannot meet {}".format(constraint))<br>        elif isinstance(constraint,  # pragma: no branch<br>                        ReserveResourceConstraint):<br>            apply_reserve_resource_constraint(machine, constraint)<br><br>    # Initially randomly place the movable vertices<br>    movable_vertices = {v for v in vertices_resources<br>                        if v not in fixed_vertices}<br>    initial_placements = _initial_placement(movable_vertices,<br>                                            vertices_resources,<br>                                            machine, random)<br><br>    # Include the fixed vertices in initial placement<br>    initial_placements.update(fixed_vertices)<br><br>    # Filter out empty or singleton nets and those weighted as zero since they<br>    # cannot influence placement.<br>    nets = [n for n in nets if len(set(n))  1 and n.weight  0.0]<br><br>    # Special cases where no placement effort is required:<br>    #  There is only one chip<br>    #  There are no resource types to be consumed<br>    #  No effort is to be made<br>    #  No movable vertices<br>    #  There are no nets (and moving things has no effect)<br>    trivial = ((machine.width, machine.height) == (1, 1) or<br>               len(machine.chip_resources) == 0 or<br>               effort == 0.0 or<br>               len(movable_vertices) == 0 or<br>               len(nets) == 0)<br>    if trivial:<br>        logger.info("Placement has trivial solution. SA not used.")<br>        finalise_same_chip_constraints(substitutions, initial_placements)<br>        return initial_placements<br><br>    # Intialise the algorithm kernel<br>    k = kernel(vertices_resources, movable_vertices, set(fixed_vertices),<br>               initial_placements, nets, machine, random, kernel_kwargs)<br><br>    logger.info("SA placement kernel: %s", kernel.__name__)<br><br>    # Specifies the maximum distance any swap can span. Initially consider<br>    # swaps that span the entire machine.<br>    distance_limit = max(machine.width, machine.height)<br><br>    # Determine initial temperature according to the heuristic used by VPR: 20<br>    # times the standard deviation of len(movable_vertices) random swap costs.<br>    # The arbitrary very-high temperature is used to cause "all" swaps to be<br>    # accepted.<br>    _0, _1, cost_delta_sd = k.run_steps(len(movable_vertices),<br>                                        distance_limit,<br>                                        1e100)<br>    temperature = 20.0  cost_delta_sd<br><br>    # The number of swap-attempts between temperature changes is selected by<br>    # the heuristic used by VPR. This value is scaled linearly by the effort<br>    # parameter.<br>    num_steps = max(1, int(effort  len(vertices_resources)1.33))<br><br>    logger.info("Initial placement temperature: %0.1f", temperature)<br><br>    # Counter for the number of swap attempts made (used for diagnostic<br>    # purposes)<br>    iteration_count = 0<br><br>    # Holds the total cost of the current placement. This default value chosen<br>    # to ensure the loop below iterates at least once.<br>    current_cost = 0.0<br><br>    # The annealing algorithm runs until a heuristic termination condition<br>    # (taken from VPR) is hit. The heuristic waits until the temperature falls<br>    # below a small fraction of the average net cost.<br>    while temperature  (0.005  current_cost) / len(nets):<br>        # Run an iteration at the current temperature<br>        num_accepted, current_cost, _ = k.run_steps(<br>            num_steps, int(math.ceil(distance_limit)), temperature)<br><br>        # The ratio of accepted-to-not-accepted changes<br>        r_accept = num_accepted / float(num_steps)<br><br>        # Special case: Can't do better than 0 cost! This is a special case<br>        # since the normal termination condition will not terminate if the cost<br>        # doesn't drop below 0.<br>        if current_cost == 0:<br>            break<br><br>        # The temperature is reduced by a factor heuristically based on the<br>        # acceptance rate. The schedule below attempts to maximise the time<br>        # spent at temperatures where a large portion (but not all) of changes<br>        # are being accepted. If lots of changes are being accepted (e.g.<br>        # during high-temperature periods) then most of them are likely not to<br>        # be beneficial. If few changes are being accepted, we're probably<br>        # pretty close to the optimal placement.<br>        if r_accept  0.96:<br>            alpha = 0.5<br>        elif r_accept  0.8:<br>            alpha = 0.9<br>        elif r_accept  0.15:<br>            alpha = 0.95<br>        else:<br>            alpha = 0.8<br>        temperature = alpha  temperature<br><br>        # According to:<br>        #  M. Huang, F. Romeo, and A. Sangiovanni-Vincentelli, "An Efficient<br>        #   General Cooling Schedule for Simulated Annealing" ICCAD, 1986, pp.<br>        #   381 - 384 and J. Lam<br>        #  J. Delosme, "Performance of a New Annealing Schedule" DAC, 1988,<br>        #   pp. 306 - 311.<br>        # It is desirable to keep the acceptance ratio as close to 0.44 for as<br>        # long as possible. As a result, when r_accept falls below this we can<br>        # help increase the acceptance rate by reducing the set of possible<br>        # swap candidates based on the observation that near the end of<br>        # placement, most things are near their optimal location and thus long<br>        # distance swaps are unlikely to be useful.<br>        distance_limit = 1.0 - 0.44 + r_accept<br>        distance_limit = min(max(distance_limit, 1.0),<br>                             max(machine.width, machine.height))<br><br>        iteration_count += num_steps<br>        logger.debug("Iteration: %d, "<br>                     "Cost: %0.1f, "<br>                     "Kept: %0.1f%%, "<br>                     "Temp: %0.3f, "<br>                     "Dist: %d.",<br>                     iteration_count, current_cost,<br>                     r_accept100, temperature, math.ceil(distance_limit))<br><br>        # Call the user callback before the next iteration, terminating if<br>        # requested.<br>        if on_temperature_change is not None:<br>            placements = k.get_placements().copy()<br>            finalise_same_chip_constraints(substitutions, placements)<br>            ret_val = on_temperature_change(iteration_count,<br>                                            placements,<br>                                            current_cost,<br>                                            r_accept,<br>                                            temperature,<br>                                            distance_limit)<br>            if ret_val is False:<br>                break<br><br>    logger.info("Anneal terminated after %d iterations.", iteration_count)<br><br>    placements = k.get_placements()<br>    finalise_same_chip_constraints(substitutions, placements)<br><br>    return placements

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_306405.txt
> def closest_point_of_approach(<br>    traffic: Traffic,<br>    lateral_separation: float,<br>    vertical_separation: float,<br>    projection: Union[pyproj.Proj, crs.Projection, None] = None,<br>    round_t: str = "d",<br>    max_workers: int = 4,<br>) - CPA:<br>    """<br>    Computes a CPA dataframe for all pairs of trajectories candidates for<br>    being separated by less than lateral_separation in vertical_separation.<br><br>    In order to be computed efficiently, the method needs the following<br>    parameters:<br><br>    - projection: a first filtering is applied on the bounding boxes of<br>    trajectories, expressed in meters. You need to provide a decent<br>    projection able to approximate distances by Euclide formula.<br>    By default, EuroPP() projection is considered, but a non explicit<br>    argument will raise a warning.<br><br>    - round_t: an additional column will be added in the DataFrame to group<br>    trajectories by relevant time frames. Distance computations will be<br>    considered only between trajectories flown in the same time frame.<br>    By default, the 'd' pandas freq parameter is considered, to group<br>    trajectories by day, but other ways of splitting ('h') may be more<br>    relevant and impact performance.<br><br>    - max_workers: distance computations are spread over a given number of<br>    processors.<br><br>    """<br><br>    if projection is None:<br>        logging.warn("Defaulting to projection EuroPP()")<br>        projection = crs.EuroPP()<br><br>    if isinstance(projection, crs.Projection):<br>        projection = pyproj.Proj(projection.proj4_init)<br><br>    def yield_pairs(t_chunk: Traffic):<br>        """<br>        This function yields all pairs of possible candidates for a CPA<br>        calculation.<br>        """<br><br>        # combinations types Iterator[Tuple[T, ...]]<br>        for first, second in cast(<br>            Iterator[Tuple[Flight, Flight]], combinations(t_chunk, 2)<br>        ):<br>            # cast are necessary because of the lru_cache × property bug<br>            if (<br>                cast(pd.Timestamp, first.start)<br>                 cast(pd.Timestamp, second.stop)<br>            ) or (<br>                cast(pd.Timestamp, second.start)<br>                 cast(pd.Timestamp, first.stop)<br>            ):<br>                # Flights must fly at the same time<br>                continue<br>            if (<br>                first.min("altitude")<br>                 second.max("altitude") + vertical_separation<br>            ):<br>                # Bounding boxes in altitude must cross<br>                continue<br>            if (<br>                second.min("altitude")<br>                 first.max("altitude") + vertical_separation<br>            ):<br>                # Bounding boxes in altitude must cross<br>                continue<br>            if first.min("x")  second.max("x") + lateral_separation:<br>                # Bounding boxes in x must cross<br>                continue<br>            if second.min("x")  first.max("x") + lateral_separation:<br>                # Bounding boxes in x must cross<br>                continue<br>            if first.min("y")  second.max("y") + lateral_separation:<br>                # Bounding boxes in y must cross<br>                continue<br>            if second.min("y")  first.max("y") + lateral_separation:<br>                # Bounding boxes in y must cross<br>                continue<br><br>            # Next step is to check the 2D footprint of the trajectories<br>            # intersect. Before computing the intersection we bufferize the<br>            # trajectories by half the requested separation.<br><br>            first_shape = first.project_shape(projection)<br>            second_shape = second.project_shape(projection)<br>            if first_shape is None or second_shape is None:<br>                continue<br><br>            first_shape = first_shape.simplify(1e3).buffer(<br>                lateral_separation / 2<br>            )<br>            second_shape = first_shape.simplify(1e3).buffer(<br>                lateral_separation / 2<br>            )<br><br>            if first_shape.intersects(second_shape):<br>                yield first, second<br><br>    t_xyt = (<br>        traffic.airborne()<br>        .compute_xy(projection)<br>        .assign(round_t=lambda df: df.timestamp.dt.round(round_t))<br>    )<br><br>    cumul = list()<br><br>    # Multiprocessing is implemented on each timerange slot only.<br>    # TODO: it would probably be more efficient to multiprocess over each<br>    # t_chunk rather than multiprocess the distance computation.<br><br>    for _, t_chunk in tqdm(<br>        t_xyt.groupby("round_t"), total=len(set(t_xyt.data.round_t))<br>    ):<br>        with ProcessPoolExecutor(max_workers=max_workers) as executor:<br>            tasks = {<br>                # TODO submit(Flight.distance, first, second)<br>                executor.submit(first.distance, second): (<br>                    first.flight_id,<br>                    second.flight_id,<br>                )<br>                for (first, second) in yield_pairs(Traffic(t_chunk))<br>            }<br><br>            for future in as_completed(tasks):<br>                cumul.append(future.result())<br><br>    return CPA(pd.concat(cumul, sort=False))

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_5702.txt
> def beam_search(symbols_to_logits_fn,<br>                initial_ids,<br>                beam_size,<br>                decode_length,<br>                vocab_size,<br>                alpha,<br>                states=None,<br>                eos_id=EOS_ID,<br>                stop_early=True,<br>                use_tpu=False,<br>                use_top_k_with_unique=True):<br>  """Beam search with length penalties.<br><br>  Requires a function that can take the currently decoded symbols and return<br>  the logits for the next symbol. The implementation is inspired by<br>  https://arxiv.org/abs/1609.08144.<br><br>  When running, the beam search steps can be visualized by using tfdbg to watch<br>  the operations generating the output ids for each beam step.  These operations<br>  have the pattern:<br>    (alive|finished)_topk_(seq,scores)<br><br>  Operations marked `alive` represent the new beam sequences that will be<br>  processed in the next step.  Operations marked `finished` represent the<br>  completed beam sequences, which may be padded with 0s if no beams finished.<br><br>  Operations marked `seq` store the full beam sequence for the time step.<br>  Operations marked `scores` store the sequence's final log scores.<br><br>  The beam search steps will be processed sequentially in order, so when<br>  capturing observed from these operations, tensors, clients can make<br>  assumptions about which step is being recorded.<br><br>  WARNING: Assumes 2nd dimension of tensors in `states` and not invariant, this<br>  means that the shape of the 2nd dimension of these tensors will not be<br>  available (i.e. set to None) inside symbols_to_logits_fn.<br><br>  Args:<br>    symbols_to_logits_fn: Interface to the model, to provide logits.<br>        Shoud take [batch_size, decoded_ids] and return [batch_size, vocab_size]<br>    initial_ids: Ids to start off the decoding, this will be the first thing<br>        handed to symbols_to_logits_fn (after expanding to beam size)<br>        [batch_size]<br>    beam_size: Size of the beam.<br>    decode_length: Number of steps to decode for.<br>    vocab_size: Size of the vocab, must equal the size of the logits returned by<br>        symbols_to_logits_fn<br>    alpha: alpha for length penalty.<br>    states: dict (possibly nested) of decoding states.<br>    eos_id: ID for end of sentence.<br>    stop_early: a boolean - stop once best sequence is provably determined.<br>    use_tpu: A bool, whether to do beam search on TPU.<br>    use_top_k_with_unique: bool, whether to use a fast (but decreased precision)<br>      top_k during TPU beam search.<br><br>  Returns:<br>    Tuple of<br>    (decoded beams [batch_size, beam_size, decode_length]<br>     decoding probabilities [batch_size, beam_size])<br>  """<br>  batch_size = common_layers.shape_list(initial_ids)[0]<br><br>  # Assume initial_ids are prob 1.0<br>  initial_log_probs = tf.constant([[0.] + [-INF]  (beam_size - 1)])<br>  # Expand to beam_size (batch_size, beam_size)<br>  alive_log_probs = tf.tile(initial_log_probs, [batch_size, 1])<br><br>  # Expand each batch and state to beam_size<br>  alive_seq = _expand_to_beam_size(initial_ids, beam_size)<br>  alive_seq = tf.expand_dims(alive_seq, axis=2)  # (batch_size, beam_size, 1)<br>  if use_tpu:<br>    alive_seq = tf.tile(alive_seq, [1, 1, decode_length + 1])<br>  if states:<br>    states = nest.map_structure(<br>        lambda state: _expand_to_beam_size(state, beam_size), states)<br>  else:<br>    states = {}<br><br>  # Finished will keep track of all the sequences that have finished so far<br>  # Finished log probs will be negative infinity in the beginning<br>  # finished_flags will keep track of booleans<br>  finished_seq = tf.zeros(common_layers.shape_list(alive_seq), tf.int32)<br>  # Setting the scores of the initial to negative infinity.<br>  finished_scores = tf.ones([batch_size, beam_size])  -INF<br>  finished_flags = tf.zeros([batch_size, beam_size], tf.bool)<br><br>  def grow_finished(finished_seq, finished_scores, finished_flags, curr_seq,<br>                    curr_scores, curr_finished):<br>    """Given sequences and scores, will gather the top k=beam size sequences.<br><br>    Args:<br>      finished_seq: Current finished sequences.<br>        [batch_size, beam_size, current_decoded_length]<br>      finished_scores: scores for each of these sequences.<br>        [batch_size, beam_size]<br>      finished_flags: finished bools for each of these sequences.<br>        [batch_size, beam_size]<br>      curr_seq: current topk sequence that has been grown by one position.<br>        [batch_size, beam_size, current_decoded_length]<br>      curr_scores: scores for each of these sequences. [batch_size, beam_size]<br>      curr_finished: Finished flags for each of these sequences.<br>        [batch_size, beam_size]<br>    Returns:<br>      Tuple of<br>        (Topk sequences based on scores,<br>         log probs of these sequences,<br>         Finished flags of these sequences)<br>    """<br>    if not use_tpu:<br>      # First append a column of 0'ids to finished to make the same length with<br>      # finished scores<br>      finished_seq = tf.concat(<br>          [finished_seq,<br>           tf.zeros([batch_size, beam_size, 1], tf.int32)], axis=2)<br><br>    # Set the scores of the unfinished seq in curr_seq to large negative<br>    # values<br>    curr_scores += (1. - tf.to_float(curr_finished))  -INF<br>    # concatenating the sequences and scores along beam axis<br>    curr_finished_seq = tf.concat([finished_seq, curr_seq], axis=1)<br>    curr_finished_scores = tf.concat([finished_scores, curr_scores], axis=1)<br>    curr_finished_flags = tf.concat([finished_flags, curr_finished], axis=1)<br>    return compute_topk_scores_and_seq(<br>        curr_finished_seq,<br>        curr_finished_scores,<br>        curr_finished_scores,<br>        curr_finished_flags,<br>        beam_size,<br>        batch_size,<br>        "grow_finished",<br>        use_tpu=use_tpu,<br>        use_top_k_with_unique=use_top_k_with_unique)<br><br>  def grow_alive(curr_seq, curr_scores, curr_log_probs, curr_finished, states):<br>    """Given sequences and scores, will gather the top k=beam size sequences.<br><br>    Args:<br>      curr_seq: current topk sequence that has been grown by one position.<br>        [batch_size, beam_size, i+1]<br>      curr_scores: scores for each of these sequences. [batch_size, beam_size]<br>      curr_log_probs: log probs for each of these sequences.<br>        [batch_size, beam_size]<br>      curr_finished: Finished flags for each of these sequences.<br>        [batch_size, beam_size]<br>      states: dict (possibly nested) of decoding states.<br>    Returns:<br>      Tuple of<br>        (Topk sequences based on scores,<br>         log probs of these sequences,<br>         Finished flags of these sequences)<br>    """<br>    # Set the scores of the finished seq in curr_seq to large negative<br>    # values<br>    curr_scores += tf.to_float(curr_finished)  -INF<br>    return compute_topk_scores_and_seq(curr_seq, curr_scores, curr_log_probs,<br>                                       curr_finished, beam_size, batch_size,<br>                                       "grow_alive", states, use_tpu=use_tpu)<br><br>  def grow_topk(i, alive_seq, alive_log_probs, states):<br>    r"""Inner beam search loop.<br><br>    This function takes the current alive sequences, and grows them to topk<br>    sequences where k = 2beam. We use 2beam because, we could have beam_size<br>    number of sequences that might hit <EOS and there will be no alive<br>    sequences to continue. With 2beam_size, this will not happen. This relies<br>    on the assumption the vocab size is  beam size. If this is true, we'll<br>    have at least beam_size non <EOS extensions if we extract the next top<br>    2beam words.<br>    Length penalty is given by = (5+len(decode)/6) ^ -\alpha. Pls refer to<br>    https://arxiv.org/abs/1609.08144.<br><br>    Args:<br>      i: loop index<br>      alive_seq: Topk sequences decoded so far [batch_size, beam_size, i+1]<br>      alive_log_probs: probabilities of these sequences. [batch_size, beam_size]<br>      states: dict (possibly nested) of decoding states.<br>    Returns:<br>      Tuple of<br>        (Topk sequences extended by the next word,<br>         The log probs of these sequences,<br>         The scores with length penalty of these sequences,<br>         Flags indicating which of these sequences have finished decoding,<br>         dict of transformed decoding states)<br>    """<br>    # Get the logits for all the possible next symbols<br>    if use_tpu and states:<br>      flat_ids = tf.reshape(<br>          tf.slice(alive_seq, [0, 0, i], [batch_size, beam_size, 1]),<br>          [batch_size  beam_size, -1])<br>    else:<br>      flat_ids = tf.reshape(alive_seq, [batch_size  beam_size, -1])<br><br>    # (batch_size  beam_size, decoded_length)<br>    if states:<br>      flat_states = nest.map_structure(_merge_beam_dim, states)<br>      flat_logits, flat_states = symbols_to_logits_fn(flat_ids, i, flat_states)<br>      states = nest.map_structure(<br>          lambda t: _unmerge_beam_dim(t, batch_size, beam_size), flat_states)<br>    elif use_tpu:<br>      flat_logits = symbols_to_logits_fn(flat_ids, i)<br>    else:<br>      flat_logits = symbols_to_logits_fn(flat_ids)<br><br>    logits = tf.reshape(flat_logits, [batch_size, beam_size, -1])<br><br>    # Convert logits to normalized log probs<br>    candidate_log_probs = common_layers.log_prob_from_logits(logits)<br><br>    # Multiply the probabilities by the current probabilities of the beam.<br>    # (batch_size, beam_size, vocab_size) + (batch_size, beam_size, 1)<br>    log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)<br><br>    length_penalty = tf.pow(((5. + tf.to_float(i + 1)) / 6.), alpha)<br><br>    curr_scores = log_probs / length_penalty<br>    # Flatten out (beam_size, vocab_size) probs in to a list of possibilities<br>    flat_curr_scores = tf.reshape(curr_scores, [-1, beam_size  vocab_size])<br><br>    if use_tpu and use_top_k_with_unique:<br>      topk_scores, topk_ids = top_k_with_unique(<br>          flat_curr_scores, k=beam_size  2)<br>    else:<br>      topk_scores, topk_ids = tf.nn.top_k(flat_curr_scores, k=beam_size  2)<br><br>    # Recovering the log probs because we will need to send them back<br>    topk_log_probs = topk_scores  length_penalty<br><br>    # Work out what beam the top probs are in.<br>    topk_beam_index = topk_ids // vocab_size<br>    topk_ids %= vocab_size  # Unflatten the ids<br><br>    if not use_tpu:<br>      # The next three steps are to create coordinates for tf.gather_nd to pull<br>      # out the correct sequences from id's that we need to grow.<br>      # We will also use the coordinates to gather the booleans of the beam<br>      # items that survived.<br>      batch_pos = compute_batch_indices(batch_size, beam_size  2)<br><br>      # top beams will give us the actual coordinates to do the gather.<br>      # stacking will create a tensor of dimension batch  beam  2, where the<br>      # last dimension contains the i,j gathering coordinates.<br>      topk_coordinates = tf.stack([batch_pos, topk_beam_index], axis=2)<br><br>      # Gather up the most probable 2beams both for the ids and<br>      # finished_in_alive bools<br>      topk_seq = tf.gather_nd(alive_seq, topk_coordinates)<br>      if states:<br>        states = nest.map_structure(<br>            lambda state: tf.gather_nd(state, topk_coordinates), states)<br><br>      # Append the most probable alive<br>      topk_seq = tf.concat([topk_seq, tf.expand_dims(topk_ids, axis=2)], axis=2)<br>    else:<br>      # Gather up the most probable 2beams both for the ids and<br>      # finished_in_alive bools<br>      topk_seq = fast_tpu_gather(alive_seq, topk_beam_index)<br><br>      if states:<br>        states = nest.map_structure(<br>            lambda state: fast_tpu_gather(state, topk_beam_index), states)<br><br>      # Update the most probable alive<br>      topk_seq = tf.transpose(topk_seq, perm=[2, 0, 1])<br>      topk_seq = inplace_ops.alias_inplace_update(topk_seq, i + 1, topk_ids)<br>      topk_seq = tf.transpose(topk_seq, perm=[1, 2, 0])<br><br>    topk_finished = tf.equal(topk_ids, eos_id)<br><br>    return topk_seq, topk_log_probs, topk_scores, topk_finished, states<br><br>  def inner_loop(i, alive_seq, alive_log_probs, finished_seq, finished_scores,<br>                 finished_flags, states):<br>    """Inner beam search loop.<br><br>    There are three groups of tensors, alive, finished, and topk.<br>    The alive group contains information about the current alive sequences<br>    The topk group contains information about alive + topk current decoded words<br>    the finished group contains information about finished sentences, that is,<br>    the ones that have decoded to <EOS. These are what we return.<br>    The general beam search algorithm is as follows:<br>    While we haven't terminated (pls look at termination condition)<br>      1. Grow the current alive to get beam2 topk sequences<br>      2. Among the topk, keep the top beam_size ones that haven't reached EOS<br>      into alive<br>      3. Among the topk, keep the top beam_size ones have reached EOS into<br>      finished<br>    Repeat<br>    To make things simple with using fixed size tensors, we will end<br>    up inserting unfinished sequences into finished in the beginning. To stop<br>    that we add -ve INF to the score of the unfinished sequence so that when a<br>    true finished sequence does appear, it will have a higher score than all the<br>    unfinished ones.<br><br>    Args:<br>      i: loop index<br>      alive_seq: Topk sequences decoded so far [batch_size, beam_size, i+1]<br>      alive_log_probs: probabilities of the beams. [batch_size, beam_size]<br>      finished_seq: Current finished sequences.<br>        [batch_size, beam_size, i+1]<br>      finished_scores: scores for each of these sequences.<br>        [batch_size, beam_size]<br>      finished_flags: finished bools for each of these sequences.<br>        [batch_size, beam_size]<br>      states: dict (possibly nested) of decoding states.<br><br>    Returns:<br>      Tuple of<br>        (Incremented loop index<br>         New alive sequences,<br>         Log probs of the alive sequences,<br>         New finished sequences,<br>         Scores of the new finished sequences,<br>         Flags indicating which sequence in finished as reached EOS,<br>         dict of final decoding states)<br>    """<br><br>    # Each inner loop, we carry out three steps:<br>    # 1. Get the current topk items.<br>    # 2. Extract the ones that have finished and haven't finished<br>    # 3. Recompute the contents of finished based on scores.<br>    topk_seq, topk_log_probs, topk_scores, topk_finished, states = grow_topk(<br>        i, alive_seq, alive_log_probs, states)<br>    alive_seq, alive_log_probs, _, states = grow_alive(<br>        topk_seq, topk_scores, topk_log_probs, topk_finished, states)<br>    finished_seq, finished_scores, finished_flags, _ = grow_finished(<br>        finished_seq, finished_scores, finished_flags, topk_seq, topk_scores,<br>        topk_finished)<br><br>    return (i + 1, alive_seq, alive_log_probs, finished_seq, finished_scores,<br>            finished_flags, states)<br><br>  def _is_finished(i, unused_alive_seq, alive_log_probs, unused_finished_seq,<br>                   finished_scores, unused_finished_in_finished, unused_states):<br>    """Checking termination condition.<br><br>    We terminate when we decoded up to decode_length or the lowest scoring item<br>    in finished has a greater score that the highest prob item in alive divided<br>    by the max length penalty<br><br>    Args:<br>      i: loop index<br>      alive_log_probs: probabilities of the beams. [batch_size, beam_size]<br>      finished_scores: scores for each of these sequences.<br>        [batch_size, beam_size]<br><br>    Returns:<br>      Bool.<br>    """<br>    max_length_penalty = tf.pow(((5. + tf.to_float(decode_length)) / 6.), alpha)<br>    # The best possible score of the most likely alive sequence.<br>    lower_bound_alive_scores = alive_log_probs[:, 0] / max_length_penalty<br><br>    if not stop_early:<br>      # by considering the min score (in the top N beams) we ensure that<br>      # the decoder will keep decoding until there is at least one beam<br>      # (in the top N) that can be improved (w.r.t. the alive beams).<br>      # any unfinished beam will have score -INF - thus the min<br>      # will always be -INF if there is at least one unfinished beam -<br>      # which means the bound_is_met condition cannot be true in this case.<br>      lowest_score_of_finished_in_finished = tf.reduce_min(finished_scores)<br>    else:<br>      # by taking the max score we only care about the first beam;<br>      # as soon as this first beam cannot be beaten from the alive beams<br>      # the beam decoder can stop.<br>      # similarly to the above, if the top beam is not completed, its<br>      # finished_score is -INF, thus it will not activate the<br>      # bound_is_met condition. (i.e., decoder will keep going on).<br>      # note we need to find the max for every sequence eparately - so, we need<br>      # to keep the batch dimension (see axis=1)<br>      lowest_score_of_finished_in_finished = tf.reduce_max(finished_scores,<br>                                                           axis=1)<br><br>    bound_is_met = tf.reduce_all(<br>        tf.greater(lowest_score_of_finished_in_finished,<br>                   lower_bound_alive_scores))<br><br>    return tf.logical_and(<br>        tf.less(i, decode_length), tf.logical_not(bound_is_met))<br><br>  inner_shape = tf.TensorShape([None, None, None])<br>  if use_tpu:<br>    inner_shape = tf.TensorShape([batch_size, beam_size, decode_length + 1])<br>  if use_tpu:<br>    state_struc = nest.map_structure(lambda state: state.get_shape(), states)<br>  else:<br>    state_struc = nest.map_structure(get_state_shape_invariants, states)<br>  (_, alive_seq, alive_log_probs, finished_seq, finished_scores,<br>   finished_flags, states) = tf.while_loop(<br>       _is_finished,<br>       inner_loop, [<br>           tf.constant(0), alive_seq, alive_log_probs, finished_seq,<br>           finished_scores, finished_flags, states<br>       ],<br>       shape_invariants=[<br>           tf.TensorShape([]),<br>           inner_shape,<br>           alive_log_probs.get_shape(),<br>           inner_shape,<br>           finished_scores.get_shape(),<br>           finished_flags.get_shape(),<br>           state_struc<br>       ],<br>       parallel_iterations=1,<br>       back_prop=False)<br><br>  alive_seq.set_shape((None, beam_size, None))<br>  finished_seq.set_shape((None, beam_size, None))<br><br>  # Accounting for corner case: It's possible that no sequence in alive for a<br>  # particular batch item ever reached EOS. In that case, we should just copy<br>  # the contents of alive for that batch item. tf.reduce_any(finished_flags, 1)<br>  # if 0, means that no sequence for that batch index had reached EOS. We need<br>  # to do the same for the scores as well.<br>  finished_seq = tf.where(<br>      tf.reduce_any(finished_flags, 1), finished_seq, alive_seq)<br>  finished_scores = tf.where(<br>      tf.reduce_any(finished_flags, 1), finished_scores, alive_log_probs)<br>  return finished_seq, finished_scores, states

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_256844.txt
> def fit_image(self, sma0=None, minsma=0., maxsma=None, step=0.1,<br>                  conver=DEFAULT_CONVERGENCE, minit=DEFAULT_MINIT,<br>                  maxit=DEFAULT_MAXIT, fflag=DEFAULT_FFLAG,<br>                  maxgerr=DEFAULT_MAXGERR, sclip=3., nclip=0,<br>                  integrmode=BILINEAR, linear=False, maxrit=None):<br>        # This parameter list is quite large and should in principle be<br>        # simplified by re-distributing these controls to somewhere else.<br>        # We keep this design though because it better mimics the flat<br>        # architecture used in the original STSDAS task `ellipse`.<br>        """<br>        Fit multiple isophotes to the image array.<br><br>        This method loops over each value of the semimajor axis (sma)<br>        length (constructed from the input parameters), fitting a single<br>        isophote at each sma.  The entire set of isophotes is returned<br>        in an `~photutils.isophote.IsophoteList` instance.<br><br>        Parameters<br>        ----------<br>        sma0 : float, optional<br>            The starting value for the semimajor axis length (pixels).<br>            This value must not be the minimum or maximum semimajor axis<br>            length, but something in between. The algorithm can't start<br>            from the very center of the galaxy image because the<br>            modelling of elliptical isophotes on that region is poor and<br>            it will diverge very easily if not tied to other previously<br>            fit isophotes. It can't start from the maximum value either<br>            because the maximum is not known beforehand, depending on<br>            signal-to-noise. The ``sma0`` value should be selected such<br>            that the corresponding isophote has a good signal-to-noise<br>            ratio and a clearly defined geometry. If set to `None` (the<br>            default), one of two actions will be taken:  if a<br>            `~photutils.isophote.EllipseGeometry` instance was input to<br>            the `~photutils.isophote.Ellipse` constructor, its ``sma``<br>            value will be used.  Otherwise, a default value of 10. will<br>            be used.<br>        minsma : float, optional<br>            The minimum value for the semimajor axis length (pixels).<br>            The default is 0.<br>        maxsma : float or `None`, optional<br>            The maximum value for the semimajor axis length (pixels).<br>            When set to `None` (default), the algorithm will increase<br>            the semimajor axis until one of several conditions will<br>            cause it to stop and revert to fit ellipses with sma <<br>            ``sma0``.<br>        step : float, optional<br>            The step value used to grow/shrink the semimajor axis length<br>            (pixels if ``linear=True``, or a relative value if<br>            ``linear=False``). See the ``linear`` parameter.  The<br>            default is 0.1.<br>        conver : float, optional<br>            The main convergence criterion. Iterations stop when the<br>            largest harmonic amplitude becomes smaller (in absolute<br>            value) than ``conver`` times the harmonic fit rms.  The<br>            default is 0.05.<br>        minit : int, optional<br>            The minimum number of iterations to perform. A minimum of 10<br>            (the default) iterations guarantees that, on average, 2<br>            iterations will be available for fitting each independent<br>            parameter (the four harmonic amplitudes and the intensity<br>            level). For the first isophote, the minimum number of<br>            iterations is 2  ``minit`` to ensure that, even departing<br>            from not-so-good initial values, the algorithm has a better<br>            chance to converge to a sensible solution.<br>        maxit : int, optional<br>            The maximum number of iterations to perform.  The default is<br>            50.<br>        fflag : float, optional<br>            The acceptable fraction of flagged data points in the<br>            sample.  If the actual fraction of valid data points is<br>            smaller than this, the iterations will stop and the current<br>            `~photutils.isophote.Isophote` will be returned.  Flagged<br>            data points are points that either lie outside the image<br>            frame, are masked, or were rejected by sigma-clipping.  The<br>            default is 0.7.<br>        maxgerr : float, optional<br>            The maximum acceptable relative error in the local radial<br>            intensity gradient. This is the main control for preventing<br>            ellipses to grow to regions of too low signal-to-noise<br>            ratio.  It specifies the maximum acceptable relative error<br>            in the local radial intensity gradient.  `Busko (1996; ASPC<br>            101, 139)<br>            <http://adsabs.harvard.edu/abs/1996ASPC..101..139B`_ showed<br>            that the fitting precision relates to that relative error.<br>            The usual behavior of the gradient relative error is to<br>            increase with semimajor axis, being larger in outer, fainter<br>            regions of a galaxy image.  In the current implementation,<br>            the ``maxgerr`` criterion is triggered only when two<br>            consecutive isophotes exceed the value specified by the<br>            parameter. This prevents premature stopping caused by<br>            contamination such as stars and HII regions.<br><br>            A number of actions may happen when the gradient error<br>            exceeds ``maxgerr`` (or becomes non-significant and is set<br>            to `None`).  If the maximum semimajor axis specified by<br>            ``maxsma`` is set to `None`, semimajor axis growth is<br>            stopped and the algorithm proceeds inwards to the galaxy<br>            center. If ``maxsma`` is set to some finite value, and this<br>            value is larger than the current semimajor axis length, the<br>            algorithm enters non-iterative mode and proceeds outwards<br>            until reaching ``maxsma``.  The default is 0.5.<br>        sclip : float, optional<br>            The sigma-clip sigma value.  The default is 3.0.<br>        nclip : int, optional<br>            The number of sigma-clip interations.  The default is 0,<br>            which means sigma-clipping is skipped.<br>        integrmode : {'bilinear', 'nearest_neighbor', 'mean', 'median'}, optional<br>            The area integration mode.  The default is 'bilinear'.<br>        linear : bool, optional<br>            The semimajor axis growing/shrinking mode. If `False`<br>            (default), the geometric growing mode is chosen, thus the<br>            semimajor axis length is increased by a factor of (1. +<br>            ``step``), and the process is repeated until either the<br>            semimajor axis value reaches the value of parameter<br>            ``maxsma``, or the last fitted ellipse has more than a given<br>            fraction of its sampled points flagged out (see ``fflag``).<br>            The process then resumes from the first fitted ellipse (at<br>            ``sma0``) inwards, in steps of (1./(1. + ``step``)), until<br>            the semimajor axis length reaches the value ``minsma``. In<br>            case of linear growing, the increment or decrement value is<br>            given directly by ``step`` in pixels.  If ``maxsma`` is set<br>            to `None`, the semimajor axis will grow until a low<br>            signal-to-noise criterion is met. See ``maxgerr``.<br>        maxrit : float or `None`, optional<br>            The maximum value of semimajor axis to perform an actual<br>            fit.  Whenever the current semimajor axis length is larger<br>            than ``maxrit``, the isophotes will be extracted using the<br>            current geometry, without being fitted.  This non-iterative<br>            mode may be useful for sampling regions of very low surface<br>            brightness, where the algorithm may become unstable and<br>            unable to recover reliable geometry information.<br>            Non-iterative mode can also be entered automatically<br>            whenever the ellipticity exceeds 1.0 or the ellipse center<br>            crosses the image boundaries.  If `None` (default), then no<br>            maximum value is used.<br><br>        Returns<br>        -------<br>        result : `~photutils.isophote.IsophoteList` instance<br>            A list-like object of `~photutils.isophote.Isophote`<br>            instances, sorted by increasing semimajor axis length.<br>        """<br><br>        # multiple fitted isophotes will be stored here<br>        isophote_list = []<br><br>        # get starting sma from appropriate source: keyword parameter,<br>        # internal EllipseGeometry instance, or fixed default value.<br>        if not sma0:<br>            if self._geometry:<br>                sma = self._geometry.sma<br>            else:<br>                sma = 10.<br>        else:<br>            sma = sma0<br><br>        # first, go from initial sma outwards until<br>        # hitting one of several stopping criteria.<br>        noiter = False<br>        first_isophote = True<br>        while True:<br>            # first isophote runs longer<br>            minit_a = 2  minit if first_isophote else minit<br>            first_isophote = False<br><br>            isophote = self.fit_isophote(sma, step, conver, minit_a, maxit,<br>                                         fflag, maxgerr, sclip, nclip,<br>                                         integrmode, linear, maxrit,<br>                                         noniterate=noiter,<br>                                         isophote_list=isophote_list)<br><br>            # check for failed fit.<br>            if (isophote.stop_code < 0 or isophote.stop_code == 1):<br>                # in case the fit failed right at the outset, return an<br>                # empty list. This is the usual case when the user<br>                # provides initial guesses that are too way off to enable<br>                # the fitting algorithm to find any meaningful solution.<br><br>                if len(isophote_list) == 1:<br>                    warnings.warn('No meaningful fit was possible.',<br>                                  AstropyUserWarning)<br>                    return IsophoteList([])<br><br>                self._fix_last_isophote(isophote_list, -1)<br><br>                # get last isophote from the actual list, since the last<br>                # `isophote` instance in this context may no longer be OK.<br>                isophote = isophote_list[-1]<br><br>                # if two consecutive isophotes failed to fit,<br>                # shut off iterative mode. Or, bail out and<br>                # change to go inwards.<br>                if len(isophote_list)  2:<br>                    if ((isophote.stop_code == 5 and<br>                         isophote_list[-2].stop_code == 5)<br>                            or isophote.stop_code == 1):<br>                        if maxsma and maxsma  isophote.sma:<br>                            # if a maximum sma value was provided by<br>                            # user, and the current sma is smaller than<br>                            # maxsma, keep growing sma in non-iterative<br>                            # mode until reaching it.<br>                            noiter = True<br>                        else:<br>                            # if no maximum sma, stop growing and change<br>                            # to go inwards.<br>                            break<br><br>            # reset variable from the actual list, since the last<br>            # `isophote` instance may no longer be OK.<br>            isophote = isophote_list[-1]<br><br>            # update sma. If exceeded user-defined<br>            # maximum, bail out from this loop.<br>            sma = isophote.sample.geometry.update_sma(step)<br>            if maxsma and sma = maxsma:<br>                break<br><br>        # reset sma so as to go inwards.<br>        first_isophote = isophote_list[0]<br>        sma, step = first_isophote.sample.geometry.reset_sma(step)<br><br>        # now, go from initial sma inwards towards center.<br>        while True:<br>            isophote = self.fit_isophote(sma, step, conver, minit, maxit,<br>                                         fflag, maxgerr, sclip, nclip,<br>                                         integrmode, linear, maxrit,<br>                                         going_inwards=True,<br>                                         isophote_list=isophote_list)<br><br>            # if abnormal condition, fix isophote but keep going.<br>            if isophote.stop_code < 0:<br>                self._fix_last_isophote(isophote_list, 0)<br><br>            # reset variable from the actual list, since the last<br>            # `isophote` instance may no longer be OK.<br>            isophote = isophote_list[-1]<br><br>            # figure out next sma; if exceeded user-defined<br>            # minimum, or too small, bail out from this loop<br>            sma = isophote.sample.geometry.update_sma(step)<br>            if sma <= max(minsma, 0.5):<br>                break<br><br>        # if user asked for minsma=0, extract special isophote there<br>        if minsma == 0.0:<br>            isophote = self.fit_isophote(0.0, isophote_list=isophote_list)<br><br>        # sort list of isophotes according to sma<br>        isophote_list.sort()<br><br>        return IsophoteList(isophote_list)


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_2332.txt
> def last_passenger_time(buses, passengers, capacity):<br>    """You are given a 0-indexed integer array `buses` of length `n`, where `buses[i]` represents the departure time of the `ith` bus. You are also given a 0-indexed integer array `passengers` of length `m`, where `passengers[j]` represents the arrival time of the `jth` passenger. All bus departure times are unique. All passenger arrival times are unique.<br><br>You are given an integer `capacity`, which represents the maximum number of passengers that can get on each bus.<br><br>When a passenger arrives, they will wait in line for the next available bus. You can get on a bus that departs at `x` minutes if you arrive at `y` minutes where `y <= x`, and the bus is not full. Passengers with the earliest arrival times get on the bus first.<br><br>More formally when a bus arrives, either:<br><br>   If `capacity` or fewer passengers are waiting for a bus, they will all get on the bus, or<br>   The `capacity` passengers with the earliest arrival times will get on the bus.<br><br>Return _the latest time you may arrive at the bus station to catch a bus_. You cannot arrive at the same time as another passenger.<br><br>Note: The arrays `buses` and `passengers` are not necessarily sorted.<br><br>Example 1:<br><br>Input: buses = \[10,20\], passengers = \[2,17,18,19\], capacity = 2<br>Output: 16<br>Explanation: Suppose you arrive at time 16.<br>At time 10, the first bus departs with the 0th passenger. <br>At time 20, the second bus departs with you and the 1st passenger.<br>Note that you may not arrive at the same time as another passenger, which is why you must arrive before the 1st passenger to catch the bus.<br><br>Example 2:<br><br>Input: buses = \[20,30,10\], passengers = \[19,13,26,4,25,11,21\], capacity = 2<br>Output: 20<br>Explanation: Suppose you arrive at time 20.<br>At time 10, the first bus departs with the 3rd passenger. <br>At time 20, the second bus departs with the 5th and 1st passengers.<br>At time 30, the third bus departs with the 0th passenger and you.<br>Notice if you had arrived any later, then the 6th passenger would have taken your seat on the third bus.<br><br>Constraints:<br><br>   `n == buses.length`<br>   `m == passengers.length`<br>   `1 <= n, m, capacity <= 105`<br>   `2 <= buses[i], passengers[i] <= 109`<br>   Each element in `buses` is unique.<br>   Each element in `passengers` is unique."""<br><br>    buses.sort()<br>    passengers.sort()<br><br>    last_index = 0<br>    for bus in buses:<br>        cnt = 0<br>        while last_index < len(passengers) and passengers[last_index] <= bus and cnt < capacity:<br>            cnt += 1<br>            last_index += 1<br><br>    return buses[0] - 1 if last_index == 0 else passengers[last_index - 1] - 1

### GROUND TRUTH 1, ranked 23, leetcode/leetcode_2462.txt
> def hireWorkers(costs, k, candidates):<br>    """You are given a 0-indexed integer array `costs` where `costs[i]` is the cost of hiring the `ith` worker.<br><br>You are also given two integers `k` and `candidates`. We want to hire exactly `k` workers according to the following rules:<br><br>   You will run `k` sessions and hire exactly one worker in each session.<br>   In each hiring session, choose the worker with the lowest cost from either the first `candidates` workers or the last `candidates` workers. Break the tie by the smallest index.<br>       For example, if `costs = [3,2,7,7,1,2]` and `candidates = 2`, then in the first hiring session, we will choose the `4th` worker because they have the lowest cost `[3,2,7,7,1,2]`.<br>       In the second hiring session, we will choose `1st` worker because they have the same lowest cost as `4th` worker but they have the smallest index `[3,2,7,7,2]`. Please note that the indexing may be changed in the process.<br>   If there are fewer than candidates workers remaining, choose the worker with the lowest cost among them. Break the tie by the smallest index.<br>   A worker can only be chosen once.<br><br>Return _the total cost to hire exactly_ `k` _workers._<br><br>Example 1:<br><br>Input: costs = \[17,12,10,2,7,2,11,20,8\], k = 3, candidates = 4<br>Output: 11<br>Explanation: We hire 3 workers in total. The total cost is initially 0.<br>- In the first hiring round we choose the worker from \[17,12,10,2,7,2,11,20,8\]. The lowest cost is 2, and we break the tie by the smallest index, which is 3. The total cost = 0 + 2 = 2.<br>- In the second hiring round we choose the worker from \[17,12,10,7,2,11,20,8\]. The lowest cost is 2 (index 4). The total cost = 2 + 2 = 4.<br>- In the third hiring round we choose the worker from \[17,12,10,7,11,20,8\]. The lowest cost is 7 (index 3). The total cost = 4 + 7 = 11. Notice that the worker with index 3 was common in the first and last four workers.<br>The total hiring cost is 11.<br><br>Example 2:<br><br>Input: costs = \[1,2,4,1\], k = 3, candidates = 3<br>Output: 4<br>Explanation: We hire 3 workers in total. The total cost is initially 0.<br>- In the first hiring round we choose the worker from \[1,2,4,1\]. The lowest cost is 1, and we break the tie by the smallest index, which is 0. The total cost = 0 + 1 = 1. Notice that workers with index 1 and 2 are common in the first and last 3 workers.<br>- In the second hiring round we choose the worker from \[2,4,1\]. The lowest cost is 1 (index 2). The total cost = 1 + 1 = 2.<br>- In the third hiring round there are less than three candidates. We choose the worker from the remaining workers \[2,4\]. The lowest cost is 2 (index 0). The total cost = 2 + 2 = 4.<br>The total hiring cost is 4.<br><br>Constraints:<br><br>   `1 <= costs.length <= 105`<br>   `1 <= costs[i] <= 105`<br>   `1 <= k, candidates <= costs.length`"""<br><br>    n = len(costs)<br>    workers = sorted([(cost, i) for i, cost in enumerate(costs)])<br><br>    cost = 0<br>    for i in range(k):<br>        if min(workers[i][1], n - workers[i][1] - 1) < candidates:<br>            cost += workers[i][0]<br><br>    return cost
