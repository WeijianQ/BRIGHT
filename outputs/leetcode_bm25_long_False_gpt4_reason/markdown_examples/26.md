# Query `26`

## Original Question

*The Fibonacci numbers, commonly denoted `F(n)` form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from `0` and `1`. That is,

F(0) = 0, F(1) = 1
F(n) = F(n - 1) + F(n - 2), for n > 1.

Given `n`, calculate `F(n)`.

Example 1:

Input: n = 2
Output: 1
Explanation: F(2) = F(1) + F(0) = 1 + 0 = 1.

Example 2:

Input: n = 3
Output: 2
Explanation: F(3) = F(2) + F(1) = 1 + 1 = 2.

Example 3:

Input: n = 4
Output: 3
Explanation: F(4) = F(3) + F(2) = 2 + 1 = 3.

Constraints:

   `0 <= n <= 30`*

## gpt4 Reasoning

```markdown
The essential problem here is to calculate the nth Fibonacci number, where the sequence starts with `F(0) = 0` and `F(1) = 1`, and each subsequent number is the sum of the two preceding ones. This is a classic problem in computer science and mathematics, often used to teach recursion, dynamic programming, and memoization techniques due to its clear example of overlapping subproblems and optimal substructure.

### Step-by-Step Reasoning & Relevant Information

1. **Base Case Identification**: The first step is to recognize the base cases, which are explicitly given: `F(0) = 0` and `F(1) = 1`. These are the starting points for building up the sequence.

2. **Recursive Formula**: The problem statement gives us the recursive formula `F(n) = F(n - 1) + F(n - 2)`, for `n > 1`. This means to find `F(n)`, we need to first find `F(n-1)` and `F(n-2)`, and then add them together.

3. **Recursive Approach**: A straightforward approach to solving this problem is to use recursion directly based on the formula given. However, this approach is highly inefficient for larger `n` because it recalculates the same values multiple times.

4. **Dynamic Programming (Tabulation)**: To avoid the inefficiency of the recursive approach, we can use dynamic programming. We start from the base cases and iteratively calculate and store the values of `F(n)` up to our desired `n`, thus avoiding redundant calculations.

5. **Memoization**: This is another dynamic programming approach where we still use recursion but save (memoize) the results of each `F(n)` as we calculate them. If the function is called again with the same `n`, we simply return the stored result rather than recalculating it.

6. **Space Optimization**: Noticing that to calculate `F(n)`, we only need the last two numbers in the sequence (`F(n-1)` and `F(n-2)`), we can further optimize the space complexity of our dynamic programming approach by keeping track of just these two numbers and updating them as we iterate through `n`.

### Detailed Solution

Given the constraints (`0 <= n <= 30`), any of the above methods would work efficiently, but for the sake of both efficiency and learning, let's detail the space-optimized dynamic programming approach:

- Initialize two variables, say `a` and `b`, with `0` and `1`, representing `F(0)` and `F(1)` respectively.
- Iterate from `2` to `n`, and for each iteration:
  - Calculate the next Fibonacci number as `c = a + b`.
  - Update `a` to `b` and `b` to `c`.
- After the loop, `b` will hold the value of `F(n)`.

### Example Walkthrough

Let's apply this method to the example where `n = 4`:

- Start with `a = 0`, `b = 1`.
- For `n = 2`, calculate `c = a + b = 1`. Update `a` to `1` (old `b`), and `b` to `1` (new `c`).
- For `n = 3`, calculate `c = a + b = 2`. Update `a` to `1` (old `b`), and `b` to `2` (new `c`).
- For `n = 4`, calculate `c = a + b = 3`. Update `a` to `2` (old `b`), and `b` to `3` (new `c`).
- We find `F(4) = 3`.

This method ensures we calculate each Fibonacci number only once and use minimal space, making it highly efficient for the given problem.
```

### Metrics

```
recip_rank: 0.0333
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0200
P_100: 0.0100
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.2500
recall_100: 0.2500
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0788
ndcg_cut_100: 0.0788
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0083
map_cut_100: 0.0083
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_191868.txt
> def NTU_from_P_E(P1, R1, Ntp, optimal=True):<br>    r'''Returns the number of transfer units of a TEMA E type heat exchanger<br>    with a specified (for side 1) thermal effectiveness `P1`, heat capacity <br>    ratio `R1`, the number of tube passes `Ntp`, and for the two-pass case<br>    whether or not the inlets are arranged optimally. The supported cases are <br>    as follows:<br>        <br>     1-1 TEMA E, shell fluid mixed<br>     1-2 TEMA E, shell fluid mixed (this configuration is symmetric)<br>     1-2 TEMA E, shell fluid split into two steams individually mixed<br>     1-3 TEMA E, shell and tube fluids mixed, one parallel pass and two <br>      counterflow passes (efficient)<br>     1-3 TEMA E, shell and tube fluids mixed, two parallel passes and one <br>      counteflow pass (inefficient)<br>     1-N TEMA E, shall and tube fluids mixed, efficient counterflow <br>      orientation, N an even number<br>      <br>    Two of these cases have analytical solutions; the rest use numerical <br>    solvers of varying quality.<br>    <br>    The analytical solution to 1-1 TEMA E, shell fluid mixed (the same as pure<br>    counterflow):<br>        <br>    .. math::<br>        NTU_1 = - \frac{1}{R_{1} - 1} \log{\left (\frac{P_{1} R_{1} - 1}{P_{1} <br>        - 1} \right )}<br>    <br>    1-2 TEMA E, shell fluid mixed:<br>        <br>    .. math::<br>        NTU_1 = \frac{2}{\sqrt{R_{1}^{2} + 1}} \log{\left (\sqrt{\frac{P_{1} <br>        R_{1} - P_{1} \sqrt{R_{1}^{2} + 1} + P_{1} - 2}{P_{1} R_{1} + P_{1} <br>        \sqrt{R_{1}^{2} + 1} + P_{1} - 2}} \right )}<br>        <br>    Parameters<br>    ----------<br>    P1 : float<br>        Thermal effectiveness of the heat exchanger in the P-NTU method,<br>        calculated with respect to stream 1 [-]<br>    R1 : float<br>        Heat capacity ratio of the heat exchanger in the P-NTU method,<br>        calculated with respect to stream 1 (shell side = 1, tube side = 2) [-]<br>    Ntp : int<br>        Number of tube passes, 1, 2, 3, 4, or an even number [-]<br>    optimal : bool, optional<br>        Whether or not the arrangement is configured to give more of a<br>        countercurrent and efficient (True) case or an inefficient parallel<br>        case, [-]<br><br>    Returns<br>    -------<br>    NTU1 : float<br>        Thermal number of transfer units of the heat exchanger in the P-NTU <br>        method, calculated with respect to stream 1 (shell side = 1, tube side<br>        = 2) [-]<br><br>    Notes<br>    -----<br>    For odd numbers of tube passes greater than 3, an exception is raised. <br>    <br>    For the 2 pass, unoptimal case, a bounded solver is used with NTU1 between<br>    1E-11 and 100; the solution to any feasible P1 was found to lie in there.<br>    For the 4 or a higher even number of pass case, the upper limit on NTU1<br>    is 1000; this solver works pretty well, but as NTU1 reaches its limit the<br>    change in P1 is so small a smaller but also correct solution is often <br>    returned.<br>    <br>    For both the optimal and unoptimal 3 tube pass case, a solution is only<br>    returned if NTU1 is between 1E-11 and 10. These functions are extremely<br>    mathematically frustrating, and as NTU1 rises above 10 catastrophic <br>    cancellation quickly results in this expression finding a ZeroDivisionError.<br>    The use of arbitrary prevision helps little - quickly 1000 digits are needed,<br>    and then 1000000 digits, and so one. Using SymPy's rational number support<br>    works better but is extremely slow for these complicated solutions.<br>    Nevertheless, so long as a solution is between 1E-11 and 10, the solver is<br>    quite robust.<br><br>    Examples<br>    --------<br>     NTU_from_P_E(P1=.58, R1=1/3., Ntp=2)<br>    1.0381979240816719<br><br>    '''<br>    NTU_min = 1E-11<br>    function = temperature_effectiveness_TEMA_E<br>    if Ntp == 1:<br>        return NTU_from_P_basic(P1, R1, subtype='counterflow')<br>    elif Ntp == 2 and optimal:<br>        # Nice analytical solution is available<br>        # There are actualy two roots but one of them is complex<br>        x1 = R1R1 + 1.<br>        return 2.log(((P1R1 - P1x10.5 + P1 - 2.)/(P1R1 + P1x10.5 + P1 - 2.))0.5)(x1)-.5<br>    elif Ntp == 2 and not optimal:<br>        NTU_max = 1E2 <br>        # Can't find anywhere it needs to go above 70 to reach the maximum<br>    elif Ntp == 3 and optimal:<br>        # no pade could be found, just about the worst-conditioned problem<br>        # I've ever found<br>        # Higher starting values result in errors<br>        NTU_max = 10<br>    elif Ntp == 3 and not optimal:<br>        # no pade could be found, just about the worst-conditioned problem<br>        # I've ever found<br>        NTU_max = 10<br>    elif Ntp == 4 or Ntp %2 == 0:<br>        NTU_max = 1E3<br>    else:<br>        raise Exception('For TEMA E shells with an odd number of tube passes more than 3, no solution is implemented.')<br>    return _NTU_from_P_solver(P1, R1, NTU_min, NTU_max, function, Ntp=Ntp, optimal=optimal)

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_321793.txt
> def evolution_strength_of_connection(A, B=None, epsilon=4.0, k=2,<br>                                     proj_type="l2", block_flag=False,<br>                                     symmetrize_measure=True):<br>    """Evolution Strength Measure.<br><br>    Construct strength of connection matrix using an Evolution-based measure<br><br>    Parameters<br>    ----------<br>    A : csr_matrix, bsr_matrix<br>        Sparse NxN matrix<br>    B : string, array<br>        If B=None, then the near nullspace vector used is all ones.  If B is<br>        an (NxK) array, then B is taken to be the near nullspace vectors.<br>    epsilon : scalar<br>        Drop tolerance<br>    k : integer<br>        ODE num time steps, step size is assumed to be 1/rho(DinvA)<br>    proj_type : {'l2','D_A'}<br>        Define norm for constrained min prob, i.e. define projection<br>    block_flag : boolean<br>        If True, use a block D inverse as preconditioner for A during<br>        weighted-Jacobi<br><br>    Returns<br>    -------<br>    Atilde : csr_matrix<br>        Sparse matrix of strength values<br><br>    See [2008OlScTu]_ for more details.<br><br>    References<br>    ----------<br>    .. [2008OlScTu] Olson, L. N., Schroder, J., Tuminaro, R. S.,<br>       "A New Perspective on Strength Measures in Algebraic Multigrid",<br>       submitted, June, 2008.<br><br>    Examples<br>    --------<br>     import numpy as np<br>     from pyamg.gallery import stencil_grid<br>     from pyamg.strength import evolution_strength_of_connection<br>     n=3<br>     stencil =  np.array([[-1.0,-1.0,-1.0],<br>    ...                        [-1.0, 8.0,-1.0],<br>    ...                        [-1.0,-1.0,-1.0]])<br>     A = stencil_grid(stencil, (n,n), format='csr')<br>     S = evolution_strength_of_connection(A,  np.ones((A.shape[0],1)))<br><br>    """<br>    # local imports for evolution_strength_of_connection<br>    from pyamg.util.utils import scale_rows, get_block_diag, scale_columns<br>    from pyamg.util.linalg import approximate_spectral_radius<br><br>    # ====================================================================<br>    # Check inputs<br>    if epsilon < 1.0:<br>        raise ValueError("expected epsilon  1.0")<br>    if k <= 0:<br>        raise ValueError("number of time steps must be  0")<br>    if proj_type not in ['l2', 'D_A']:<br>        raise ValueError("proj_type must be 'l2' or 'D_A'")<br>    if (not sparse.isspmatrix_csr(A)) and (not sparse.isspmatrix_bsr(A)):<br>        raise TypeError("expected csr_matrix or bsr_matrix")<br><br>    # ====================================================================<br>    # Format A and B correctly.<br>    # B must be in mat format, this isn't a deep copy<br>    if B is None:<br>        Bmat = np.mat(np.ones((A.shape[0], 1), dtype=A.dtype))<br>    else:<br>        Bmat = np.mat(B)<br><br>    # Pre-process A.  We need A in CSR, to be devoid of explicit 0's and have<br>    # sorted indices<br>    if (not sparse.isspmatrix_csr(A)):<br>        csrflag = False<br>        numPDEs = A.blocksize[0]<br>        D = A.diagonal()<br>        # Calculate DinvA<br>        if block_flag:<br>            Dinv = get_block_diag(A, blocksize=numPDEs, inv_flag=True)<br>            Dinv = sparse.bsr_matrix((Dinv, np.arange(Dinv.shape[0]),<br>                                      np.arange(Dinv.shape[0] + 1)),<br>                                     shape=A.shape)<br>            Dinv_A = (Dinv  A).tocsr()<br>        else:<br>            Dinv = np.zeros_like(D)<br>            mask = (D != 0.0)<br>            Dinv[mask] = 1.0 / D[mask]<br>            Dinv[D == 0] = 1.0<br>            Dinv_A = scale_rows(A, Dinv, copy=True)<br>        A = A.tocsr()<br>    else:<br>        csrflag = True<br>        numPDEs = 1<br>        D = A.diagonal()<br>        Dinv = np.zeros_like(D)<br>        mask = (D != 0.0)<br>        Dinv[mask] = 1.0 / D[mask]<br>        Dinv[D == 0] = 1.0<br>        Dinv_A = scale_rows(A, Dinv, copy=True)<br><br>    A.eliminate_zeros()<br>    A.sort_indices()<br><br>    # Handle preliminaries for the algorithm<br>    dimen = A.shape[1]<br>    NullDim = Bmat.shape[1]<br><br>    # Get spectral radius of DinvA, this will be used to scale the time step<br>    # size for the ODE<br>    rho_DinvA = approximate_spectral_radius(Dinv_A)<br><br>    # Calculate D_A for later use in the minimization problem<br>    if proj_type == "D_A":<br>        D_A = sparse.spdiags([D], [0], dimen, dimen, format='csr')<br>    else:<br>        D_A = sparse.eye(dimen, dimen, format="csr", dtype=A.dtype)<br><br>    # Calculate (I - delta_t Dinv A)^k<br>    #      In order to later access columns, we calculate the transpose in<br>    #      CSR format so that columns will be accessed efficiently<br>    # Calculate the number of time steps that can be done by squaring, and<br>    # the number of time steps that must be done incrementally<br>    nsquare = int(np.log2(k))<br>    ninc = k - 2nsquare<br><br>    # Calculate one time step<br>    Id = sparse.eye(dimen, dimen, format="csr", dtype=A.dtype)<br>    Atilde = (Id - (1.0 / rho_DinvA)  Dinv_A)<br>    Atilde = Atilde.T.tocsr()<br><br>    # Construct a sparsity mask for Atilde that will restrict Atilde^T to the<br>    # nonzero pattern of A, with the added constraint that row i of Atilde^T<br>    # retains only the nonzeros that are also in the same PDE as i.<br>    mask = A.copy()<br><br>    # Restrict to same PDE<br>    if numPDEs  1:<br>        row_length = np.diff(mask.indptr)<br>        my_pde = np.mod(np.arange(dimen), numPDEs)<br>        my_pde = np.repeat(my_pde, row_length)<br>        mask.data[np.mod(mask.indices, numPDEs) != my_pde] = 0.0<br>        del row_length, my_pde<br>        mask.eliminate_zeros()<br><br>    # If the total number of time steps is a power of two, then there is<br>    # a very efficient computational short-cut.  Otherwise, we support<br>    # other numbers of time steps, through an inefficient algorithm.<br>    if ninc  0:<br>        warn("The most efficient time stepping for the Evolution Strength\<br>             Method is done in powers of two.\nYou have chosen " + str(k) +<br>             " time steps.")<br><br>        # Calculate (Atilde^nsquare)^T = (Atilde^T)^nsquare<br>        for i in range(nsquare):<br>            Atilde = Atilde  Atilde<br><br>        JacobiStep = (Id - (1.0 / rho_DinvA)  Dinv_A).T.tocsr()<br>        for i in range(ninc):<br>            Atilde = Atilde  JacobiStep<br>        del JacobiStep<br><br>        # Apply mask to Atilde, zeros in mask have already been eliminated at<br>        # start of routine.<br>        mask.data[:] = 1.0<br>        Atilde = Atilde.multiply(mask)<br>        Atilde.eliminate_zeros()<br>        Atilde.sort_indices()<br><br>    elif nsquare == 0:<br>        if numPDEs  1:<br>            # Apply mask to Atilde, zeros in mask have already been eliminated<br>            # at start of routine.<br>            mask.data[:] = 1.0<br>            Atilde = Atilde.multiply(mask)<br>            Atilde.eliminate_zeros()<br>            Atilde.sort_indices()<br><br>    else:<br>        # Use computational short-cut for case (ninc == 0) and (nsquare  0)<br>        # Calculate Atilde^k only at the sparsity pattern of mask.<br>        for i in range(nsquare - 1):<br>            Atilde = Atilde  Atilde<br><br>        # Call incomplete mat-mat mult<br>        AtildeCSC = Atilde.tocsc()<br>        AtildeCSC.sort_indices()<br>        mask.sort_indices()<br>        Atilde.sort_indices()<br>        amg_core.incomplete_mat_mult_csr(Atilde.indptr, Atilde.indices,<br>                                         Atilde.data, AtildeCSC.indptr,<br>                                         AtildeCSC.indices, AtildeCSC.data,<br>                                         mask.indptr, mask.indices, mask.data,<br>                                         dimen)<br><br>        del AtildeCSC, Atilde<br>        Atilde = mask<br>        Atilde.eliminate_zeros()<br>        Atilde.sort_indices()<br><br>    del Dinv, Dinv_A, mask<br><br>    # Calculate strength based on constrained min problem of<br>    # min( z - Bx ), such that<br>    # (Bx)|_i = z|_i, i.e. they are equal at point i<br>    # z = (I - (t/k) Dinv A)^k delta_i<br>    #<br>    # Strength is defined as the relative point-wise approx. error between<br>    # Bx and z.  We don't use the full z in this problem, only that part of<br>    # z that is in the sparsity pattern of A.<br>    #<br>    # Can use either the D-norm, and inner product, or l2-norm and inner-prod<br>    # to solve the constrained min problem.  Using D gives scale invariance.<br>    #<br>    # This is a quadratic minimization problem with a linear constraint, so<br>    # we can build a linear system and solve it to find the critical point,<br>    # i.e. minimum.<br>    #<br>    # We exploit a known shortcut for the case of NullDim = 1.  The shortcut is<br>    # mathematically equivalent to the longer constrained min. problem<br><br>    if NullDim == 1:<br>        # Use shortcut to solve constrained min problem if B is only a vector<br>        # Strength(i,j) = | 1 - (z(i)/b(j))/(z(j)/b(i)) |<br>        # These ratios can be calculated by diagonal row and column scalings<br><br>        # Create necessary vectors for scaling Atilde<br>        #   Its not clear what to do where B == 0.  This is an<br>        #   an easy programming solution, that may make sense.<br>        Bmat_forscaling = np.ravel(Bmat)<br>        Bmat_forscaling[Bmat_forscaling == 0] = 1.0<br>        DAtilde = Atilde.diagonal()<br>        DAtildeDivB = np.ravel(DAtilde) / Bmat_forscaling<br><br>        # Calculate best approximation, z_tilde, in span(B)<br>        #   Importantly, scale_rows and scale_columns leave zero entries<br>        #   in the matrix.  For previous implementations this was useful<br>        #   because we assume data and Atilde.data are the same length below<br>        data = Atilde.data.copy()<br>        Atilde.data[:] = 1.0<br>        Atilde = scale_rows(Atilde, DAtildeDivB)<br>        Atilde = scale_columns(Atilde, np.ravel(Bmat_forscaling))<br><br>        # If angle in the complex plane between z and z_tilde is<br>        # greater than 90 degrees, then weak.  We can just look at the<br>        # dot product to determine if angle is greater than 90 degrees.<br>        angle = np.multiply(np.real(Atilde.data), np.real(data)) +\<br>            np.multiply(np.imag(Atilde.data), np.imag(data))<br>        angle = angle < 0.0<br>        angle = np.array(angle, dtype=bool)<br><br>        # Calculate Approximation ratio<br>        Atilde.data = Atilde.data / data<br><br>        # If approximation ratio is less than tol, then weak connection<br>        weak_ratio = (np.abs(Atilde.data) < 1e-4)<br><br>        # Calculate Approximation error<br>        Atilde.data = abs(1.0 - Atilde.data)<br><br>        # Set small ratios and large angles to weak<br>        Atilde.data[weak_ratio] = 0.0<br>        Atilde.data[angle] = 0.0<br><br>        # Set near perfect connections to 1e-4<br>        Atilde.eliminate_zeros()<br>        Atilde.data[Atilde.data < np.sqrt(np.finfo(float).eps)] = 1e-4<br><br>        del data, weak_ratio, angle<br><br>    else:<br>        # For use in computing local B_i^HB, precompute the element-wise<br>        # multiply of each column of B with each other column.  We also scale<br>        # by 2.0 to account for BDB's eventual use in a constrained<br>        # minimization problem<br>        BDBCols = int(np.sum(np.arange(NullDim + 1)))<br>        BDB = np.zeros((dimen, BDBCols), dtype=A.dtype)<br>        counter = 0<br>        for i in range(NullDim):<br>            for j in range(i, NullDim):<br>                BDB[:, counter] = 2.0 \<br>                    (np.conjugate(np.ravel(np.asarray(B[:, i]))) <br>                        np.ravel(np.asarray(D_A  B[:, j])))<br>                counter = counter + 1<br><br>        # Choose tolerance for dropping "numerically zero" values later<br>        t = Atilde.dtype.char<br>        eps = np.finfo(np.float).eps<br>        feps = np.finfo(np.single).eps<br>        geps = np.finfo(np.longfloat).eps<br>        _array_precision = {'f': 0, 'd': 1, 'g': 2, 'F': 0, 'D': 1, 'G': 2}<br>        tol = {0: feps  1e3, 1: eps  1e6, 2: geps  1e6}[_array_precision[t]]<br><br>        # Use constrained min problem to define strength<br>        amg_core.evolution_strength_helper(Atilde.data,<br>                                           Atilde.indptr,<br>                                           Atilde.indices,<br>                                           Atilde.shape[0],<br>                                           np.ravel(np.asarray(B)),<br>                                           np.ravel(np.asarray(<br>                                               (D_A  np.conjugate(B)).T)),<br>                                           np.ravel(np.asarray(BDB)),<br>                                           BDBCols, NullDim, tol)<br><br>        Atilde.eliminate_zeros()<br><br>    # All of the strength values are real by this point, so ditch the complex<br>    # part<br>    Atilde.data = np.array(np.real(Atilde.data), dtype=float)<br><br>    # Apply drop tolerance<br>    if epsilon != np.inf:<br>        amg_core.apply_distance_filter(dimen, epsilon, Atilde.indptr,<br>                                       Atilde.indices, Atilde.data)<br>        Atilde.eliminate_zeros()<br><br>    # Symmetrize<br>    if symmetrize_measure:<br>        Atilde = 0.5  (Atilde + Atilde.T)<br><br>    # Set diagonal to 1.0, as each point is strongly connected to itself.<br>    Id = sparse.eye(dimen, dimen, format="csr")<br>    Id.data -= Atilde.diagonal()<br>    Atilde = Atilde + Id<br><br>    # If converted BSR to CSR, convert back and return amalgamated matrix,<br>    #   i.e. the sparsity structure of the blocks of Atilde<br>    if not csrflag:<br>        Atilde = Atilde.tobsr(blocksize=(numPDEs, numPDEs))<br><br>        n_blocks = Atilde.indices.shape[0]<br>        blocksize = Atilde.blocksize[0]  Atilde.blocksize[1]<br>        CSRdata = np.zeros((n_blocks,))<br>        amg_core.min_blocks(n_blocks, blocksize,<br>                            np.ravel(np.asarray(Atilde.data)), CSRdata)<br>        # Atilde = sparse.csr_matrix((data, row, col), shape=(,))<br>        Atilde = sparse.csr_matrix((CSRdata, Atilde.indices, Atilde.indptr),<br>                                   shape=(int(Atilde.shape[0] / numPDEs),<br>                                          int(Atilde.shape[1] / numPDEs)))<br><br>    # Standardized strength values require small values be weak and large<br>    # values be strong.  So, we invert the algebraic distances computed here<br>    Atilde.data = 1.0 / Atilde.data<br><br>    # Scale C by the largest magnitude entry in each row<br>    Atilde = scale_rows_by_largest_entry(Atilde)<br><br>    return Atilde

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_326876.txt
> def hurst_rs(data, nvals=None, fit="RANSAC", debug_plot=False,<br>             debug_data=False, plot_file=None, corrected=True, unbiased=True):<br>  """<br>  Calculates the Hurst exponent by a standard rescaled range (R/S) approach.<br><br>  Explanation of Hurst exponent:<br>    The Hurst exponent is a measure for the "long-term memory" of a<br>    time series, meaning the long statistical dependencies in the data that do<br>    not originate from cycles.<br><br>    It originates from H.E. Hursts observations of the problem of long-term<br>    storage in water reservoirs. If x_i is the discharge of a river in year i<br>    and we observe this discharge for N years, we can calculate the storage<br>    capacity that would be required to keep the discharge steady at its mean<br>    value.<br><br>    To do so, we first substract the mean over all x_i from the individual<br>    x_i to obtain the departures x'_i from the mean for each year i. As the<br>    excess or deficit in discharge always carrys over from year i to year i+1,<br>    we need to examine the cumulative sum of x'_i, denoted by y_i. This<br>    cumulative sum represents the filling of our hypothetical storage. If the<br>    sum is above 0, we are storing excess discharge from the river, if it is<br>    below zero we have compensated a deficit in discharge by releasing<br>    water from the storage. The range (maximum - minimum) R of y_i therefore<br>    represents the total capacity required for the storage.<br><br>    Hurst showed that this value follows a steady trend for varying N if it<br>    is normalized by the standard deviation sigma over the x_i. Namely he<br>    obtained the following formula:<br><br>    R/sigma = (N/2)^K<br><br>    In this equation, K is called the Hurst exponent. Its value is 0.5 for<br>    white noise, but becomes greater for time series that exhibit some positive<br>    dependency on previous values. For negative dependencies it becomes less<br>    than 0.5.<br><br>  Explanation of the algorithm:<br>    The rescaled range (R/S) approach is directly derived from Hurst's<br>    definition. The time series of length N is split into non-overlapping<br>    subseries of length n. Then, R and S (S = sigma) are calculated for each<br>    subseries and the mean is taken over all subseries yielding (R/S)_n. This<br>    process is repeated for several lengths n. Finally, the exponent K is<br>    obtained by fitting a straight line to the plot of log((R/S)_n) vs log(n).<br><br>    There seems to be no consensus how to chose the subseries lenghts n.<br>    This function therefore leaves the choice to the user. The module provides<br>    some utility functions for "typical" values:<br><br>       binary_n: N/2, N/4, N/8, ...<br>       logarithmic_n: min_n, min_n  f, min_n  f^2, ...<br><br>  References:<br>    .. [h_1] H. E. Hurst, “The problem of long-term storage in reservoirs,”<br>       International Association of Scientific Hydrology. Bulletin, vol. 1,<br>       no. 3, pp. 13–27, 1956.<br>    .. [h_2] H. E. Hurst, “A suggested statistical model of some time series<br>       which occur in nature,” Nature, vol. 180, p. 494, 1957.<br>    .. [h_3] R. Weron, “Estimating long-range dependence: finite sample<br>       properties and confidence intervals,” Physica A: Statistical Mechanics<br>       and its Applications, vol. 312, no. 1, pp. 285–299, 2002.<br><br>  Reference Code:<br>    .. [h_a] "hurst" function in R-package "pracma",<br>             url: https://cran.r-project.org/web/packages/pracma/pracma.pdf<br><br>             Note: Pracma yields several estimates of the Hurst exponent, which<br>             are listed below. Unless otherwise stated they use the divisors<br>             of the length of the sequence as n. The length is reduced by at<br>             most 1% to find the value that has the most divisors.<br><br>              The "Simple R/S" estimate is just log((R/S)_n) / log(n) for <br>               n = N.<br>              The "theoretical Hurst exponent" is the value that would be<br>               expected of an uncorrected rescaled range approach for random<br>               noise of the size of the input data.<br>              The "empirical Hurst exponent" is the uncorrected Hurst exponent<br>               obtained by the rescaled range approach.<br>              The "corrected empirical Hurst exponent" is the Anis-Lloyd-Peters<br>               corrected Hurst exponent, but with sqrt(1/2  pi  n) added to<br>               the (R/S)_n before the log.<br>              The "corrected R over S Hurst exponent" uses the R-function "lm"<br>               instead of pracmas own "polyfit" and uses n = N/2, N/4, N/8, ...<br>               by successively halving the subsequences (which means that some<br>               subsequences may be one element longer than others). In contrast<br>               to its name it does not use the Anis-Lloyd-Peters correction<br>               factor.<br><br>             If you want to compare the output of pracma to the output of<br>             nolds, the "empirical hurst exponent" is the only measure that<br>             exactly corresponds to the Hurst measure implemented in nolds<br>             (by choosing corrected=False, fit="poly" and employing the same<br>             strategy for choosing n as the divisors of the (reduced)<br>             sequence length).<br>    .. [h_b] Rafael Weron, "HURST: MATLAB function to compute the Hurst<br>             exponent using R/S Analysis",<br>             url: https://ideas.repec.org/c/wuu/hscode/m11003.html<br><br>             Note: When the same values for nvals are used and fit is set to<br>             "poly", nolds yields exactly the same results as this<br>             implementation.<br>    .. [h_c] Bill Davidson, "Hurst exponent",<br>             url: http://www.mathworks.com/matlabcentral/fileexchange/9842-hurst-exponent<br>    .. [h_d] Tomaso Aste, "Generalized Hurst exponent",<br>             url: http://de.mathworks.com/matlabcentral/fileexchange/30076-generalized-hurst-exponent<br><br>  Args:<br>    data (array-like of float):<br>      time series<br>  Kwargs:<br>    nvals (iterable of int):<br>      sizes of subseries to use<br>      (default: logmid_n(total_N, ratio=1/4.0, nsteps=15) , that is 15<br>      logarithmically spaced values in the medium 25% of the logarithmic range)<br><br>      Generally, the choice for n is a trade-off between the length and the<br>      number of the subsequences that are used for the calculation of the<br>      (R/S)_n. Very low values of n lead to high variance in the ``r`` and ``s``<br>      while very high values may leave too few subsequences that the mean along<br>      them is still meaningful. Logarithmic spacing makes sense, because it <br>      translates to even spacing in the log-log-plot.<br>    fit (str):<br>      the fitting method to use for the line fit, either 'poly' for normal<br>      least squares polynomial fitting or 'RANSAC' for RANSAC-fitting which<br>      is more robust to outliers<br>    debug_plot (boolean):<br>      if True, a simple plot of the final line-fitting step will be shown<br>    debug_data (boolean):<br>      if True, debugging data will be returned alongside the result<br>    plot_file (str):<br>      if debug_plot is True and plot_file is not None, the plot will be saved<br>      under the given file name instead of directly showing it through<br>      ``plt.show()``<br>    corrected (boolean):<br>      if True, the Anis-Lloyd-Peters correction factor will be applied to the<br>      output according to the expected value for the individual (R/S)_n<br>      (see [h_3]_)<br>    unbiased (boolean):<br>      if True, the standard deviation based on the unbiased variance<br>      (1/(N-1) instead of 1/N) will be used. This should be the default choice,<br>      since the true mean of the sequences is not known. This parameter should<br>      only be changed to recreate results of other implementations.<br><br>  Returns:<br>    float:<br>      estimated Hurst exponent K using a rescaled range approach (if K = 0.5<br>      there are no long-range correlations in the data, if K < 0.5 there are<br>      negative long-range correlations, if K  0.5 there are positive<br>      long-range correlations)<br>    (1d-vector, 1d-vector, list):<br>      only present if debug_data is True: debug data of the form<br>      ``(nvals, rsvals, poly)`` where ``nvals`` are the values used for log(n), <br>      ``rsvals`` are the corresponding log((R/S)_n) and ``poly`` are the line <br>      coefficients (``[slope, intercept]``)<br>  """<br>  data = np.asarray(data)<br>  total_N = len(data)<br>  if nvals is None:<br>    # chooses a default value for nvals that will give 15 logarithmically<br>    # spaced datapoints leaning towards the middle of the logarithmic range<br>    # (since both too small and too large n introduce too much variance)<br>    nvals = logmid_n(total_N, ratio=1/4.0, nsteps=15)<br>  # get individual values for (R/S)_n<br>  rsvals = np.array([rs(data, n, unbiased=unbiased) for n in nvals])<br>  # filter NaNs (zeros should not be possible, because if R is 0 then<br>  # S is also zero)<br>  not_nan = np.logical_not(np.isnan(rsvals))<br>  rsvals = rsvals[not_nan]<br>  nvals = np.asarray(nvals)[not_nan]<br>  # it may happen that no rsvals are left (if all values of data are the same)<br>  if len(rsvals) == 0:<br>    poly = [np.nan, np.nan]<br>    if debug_plot:<br>      warnings.warn("Cannot display debug plot, all (R/S)_n are NaN")<br>  else:<br>    # fit a line to the logarithm of the obtained (R/S)_n<br>    xvals = np.log(nvals)<br>    yvals = np.log(rsvals)<br>    if corrected:<br>      yvals -= np.log([expected_rs(n) for n in nvals])<br>    poly = poly_fit(xvals, yvals, 1, fit=fit)<br>    if debug_plot:<br>      plot_reg(xvals, yvals, poly, "log(n)", "log((R/S)_n)",<br>               fname=plot_file)<br>  # account for correction if necessary<br>  h = poly[0] + 0.5 if corrected else poly[0]<br>  # return line slope (+ correction) as hurst exponent<br>  if debug_data:<br>    return (h, (np.log(nvals), np.log(rsvals), poly))<br>  else:<br>    return h

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_387402.txt
> def dca(x, f, g, niter, callback=None):<br>    r"""Subgradient DCA of Tao and An.<br><br>    This algorithm solves a problem of the form ::<br><br>        min_x f(x) - g(x),<br><br>    where ``f`` and ``g`` are proper, convex and lower semicontinuous<br>    functions.<br><br>    Parameters<br>    ----------<br>    x : `LinearSpaceElement`<br>        Initial point, updated in-place.<br>    f : `Functional`<br>        Convex functional. Needs to implement ``f.convex_conj.gradient``.<br>    g : `Functional`<br>        Convex functional. Needs to implement ``g.gradient``.<br>    niter : int<br>        Number of iterations.<br>    callback : callable, optional<br>        Function called with the current iterate after each iteration.<br><br>    Notes<br>    -----<br>    The algorithm is described in Section 3 and in particular in Theorem 3 of<br>    `[TA1997] <http://journals.math.ac.vn/acta/pdf/9701289.pdf`_. The problem<br><br>    .. math::<br>        \min f(x) - g(x)<br><br>    has the first-order optimality condition :math:`0 \in \partial f(x) -<br>    \partial g(x)`, i.e., aims at finding an :math:`x` so that there exists a<br>    common element<br><br>    .. math::<br>        y \in \partial f(x) \cap \partial g(x).<br><br>    The element :math:`y` can be seen as a solution of the Toland dual problem<br><br>    .. math::<br>        \min g^(y) - f^(y)<br><br>    and the iteration is given by<br><br>    .. math::<br>        y_n \in \partial g(x_n), \qquad x_{n+1} \in \partial f^(y_n),<br><br>    for :math:`n\geq 0`. Here, a subgradient is found by evaluating the<br>    gradient method of the respective functionals.<br><br>    References<br>    ----------<br>    [TA1997] Tao, P D, and An, L T H. Convex analysis approach to d.c.<br>    programming: Theory, algorithms and applications. Acta Mathematica<br>    Vietnamica, 22.1 (1997), pp 289--355.<br><br>    See also<br>    --------<br>    prox_dca :<br>        Solver with a proximal step for ``f`` and a subgradient step for ``g``.<br>    doubleprox_dc :<br>        Solver with proximal steps for all the nonsmooth convex functionals<br>        and a gradient step for a smooth functional.<br>    """<br>    space = f.domain<br>    if g.domain != space:<br>        raise ValueError('`f.domain` and `g.domain` need to be equal, but '<br>                         '{} != {}'.format(space, g.domain))<br>    f_convex_conj = f.convex_conj<br>    for _ in range(niter):<br>        f_convex_conj.gradient(g.gradient(x), out=x)<br><br>        if callback is not None:<br>            callback(x)

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_191866.txt
> def NTU_from_P_G(P1, R1, Ntp, optimal=True):<br>    r'''Returns the number of transfer units of a TEMA G type heat exchanger<br>    with a specified (for side 1) thermal effectiveness `P1`, heat capacity <br>    ratio `R1`, the number of tube passes `Ntp`, and for the two-pass case<br>    whether or not the inlets are arranged optimally. The supported cases are <br>    as follows:<br>        <br>     One tube pass (tube fluid split into two streams individually mixed,  <br>      shell fluid mixed)<br>     Two tube passes (shell and tube exchanger with shell and tube fluids  <br>      mixed in each pass at the cross section), counterflow arrangement<br>     Two tube passes (shell and tube exchanger with shell and tube fluids  <br>      mixed in each pass at the cross section), parallelflow arrangement<br>                <br>    Parameters<br>    ----------<br>    P1 : float<br>        Thermal effectiveness of the heat exchanger in the P-NTU method,<br>        calculated with respect to stream 1 [-]<br>    R1 : float<br>        Heat capacity ratio of the heat exchanger in the P-NTU method,<br>        calculated with respect to stream 1 (shell side = 1, tube side = 2) [-]<br>    Ntp : int<br>        Number of tube passes, 1 or 2 [-]<br>    optimal : bool, optional<br>        Whether or not the arrangement is configured to give more of a<br>        countercurrent and efficient (True) case or an inefficient parallel<br>        case (only applies for two passes), [-]<br><br>    Returns<br>    -------<br>    NTU1 : float<br>        Thermal number of transfer units of the heat exchanger in the P-NTU <br>        method, calculated with respect to stream 1 (shell side = 1, tube side<br>        = 2) [-]<br><br>    Notes<br>    -----<br>    For numbers of tube passes greater than 1 or 2, an exception is raised.<br>    <br>    Although this function allows the thermal effectiveness desired to be<br>    specified, it does not mean such a high value can be obtained. An exception<br>    is raised which shows the maximum possible effectiveness obtainable at the<br>    specified `R1` and configuration.<br>    <br>     NTU_from_P_G(P1=1, R1=1/3., Ntp=2)<br>    Traceback (most recent call last):<br>    ValueError: No solution possible gives such a high P1; maximum P1=0.954545 at NTU1=10000.000000<br>    <br>    Of the three configurations, 1 pass and the optimal 2 pass have monotonic <br>    functions which allow for a bounded solver to work smoothly. In both cases<br>    a solution is searched for between NTU1 values of 1E-11 and 1E-4.<br>    <br>    For the 2 pass unoptimal solution, a bounded solver is first use, but<br>    the upper bound on P1 and the upper NTU1 limit is calculated from a pade<br>    approximation performed with mpmath. <br><br>    Examples<br>    --------<br>     NTU_from_P_G(P1=.573, R1=1/3., Ntp=1)<br>    0.9999513707769526<br>    '''<br>    NTU_min = 1E-11<br>    function = temperature_effectiveness_TEMA_G<br>    if Ntp == 1 or (Ntp == 2 and optimal):<br>        NTU_max = 1E4<br>        # We could fit a curve to determine the NTU where the floating point<br>        # does not allow NTU to increase though, but that would be another<br>        # binary bisection process, different from the current pipeline<br>    elif Ntp == 2 and not optimal:<br>        NTU_max = _NTU_max_for_P_solver(NTU_from_G_2_unoptimal, R1)<br>    else:<br>        raise Exception('Supported numbers of tube passes are 1 or 2.')<br>    return _NTU_from_P_solver(P1, R1, NTU_min, NTU_max, function, Ntp=Ntp, optimal=optimal)


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_70.txt
> def climbStairs(n):<br>    """You are climbing a staircase. It takes `n` steps to reach the top.<br><br>Each time you can either climb `1` or `2` steps. In how many distinct ways can you climb to the top?<br><br>Example 1:<br><br>Input: n = 2<br>Output: 2<br>Explanation: There are two ways to climb to the top.<br>1. 1 step + 1 step<br>2. 2 steps<br><br>Example 2:<br><br>Input: n = 3<br>Output: 3<br>Explanation: There are three ways to climb to the top.<br>1. 1 step + 1 step + 1 step<br>2. 1 step + 2 steps<br>3. 2 steps + 1 step<br><br>Constraints:<br><br>   `1 <= n <= 45`"""<br><br>    if n <= 2:<br>        return n<br>    a, b = 1, 2<br>    for _ in range(3, n+1):<br>        a, b = b, a + b<br>    return b

### GROUND TRUTH 1, ranked not in top 100, leetcode/leetcode_842.txt
> def smallest_good_integer(fronts, backs):<br>    """You are given a string of digits `num`, such as `"123456579 "`. We can split it into a Fibonacci-like sequence `[123, 456, 579]`.<br><br>Formally, a Fibonacci-like sequence is a list `f` of non-negative integers such that:<br><br>   `0 <= f[i] < 231`, (that is, each integer fits in a 32-bit signed integer type),<br>   `f.length = 3`, and<br>   `f[i] + f[i + 1] == f[i + 2]` for all `0 <= i < f.length - 2`.<br><br>Note that when splitting the string into pieces, each piece must not have extra leading zeroes, except if the piece is the number `0` itself.<br><br>Return any Fibonacci-like sequence split from `num`, or return `[]` if it cannot be done.<br><br>Example 1:<br><br>Input: num =  "1101111 "<br>Output: \[11,0,11,11\]<br>Explanation: The output \[110, 1, 111\] would also be accepted.<br><br>Example 2:<br><br>Input: num =  "112358130 "<br>Output: \[\]<br>Explanation: The task is impossible.<br><br>Example 3:<br><br>Input: num =  "0123 "<br>Output: \[\]<br>Explanation: Leading zeroes are not allowed, so  "01 ",  "2 ",  "3 " is not valid.<br><br>Constraints:<br><br>   `1 <= num.length <= 200`<br>   `num` contains only digits."""<br><br>    not_good = {fronts[i] for i in range(len(fronts)) if fronts[i] == backs[i]}<br><br>    result = float('inf')<br>    for i in range(len(fronts)):<br>        if fronts[i] not in not_good:<br>            result = min(result, fronts[i])<br>        if backs[i] not in not_good:<br>            result = min(result, backs[i])<br><br>    return 0 if result == float('inf') else result

### GROUND TRUTH 2, ranked 29, leetcode/leetcode_873.txt
> import random<br>    """A sequence `x1, x2, ..., xn` is _Fibonacci-like_ if:<br><br>   `n = 3`<br>   `xi + xi+1 == xi+2` for all `i + 2 <= n`<br><br>Given a strictly increasing array `arr` of positive integers forming a sequence, return _the length of the longest Fibonacci-like subsequence of_ `arr`. If one does not exist, return `0`.<br><br>A subsequence is derived from another sequence `arr` by deleting any number of elements (including none) from `arr`, without changing the order of the remaining elements. For example, `[3, 5, 8]` is a subsequence of `[3, 4, 5, 6, 7, 8]`.<br><br>Example 1:<br><br>Input: arr = \[1,2,3,4,5,6,7,8\]<br>Output: 5<br>Explanation: The longest subsequence that is fibonacci-like: \[1,2,3,5,8\].<br><br>Example 2:<br><br>Input: arr = \[1,3,7,11,12,14,18\]<br>Output: 3<br>Explanation: The longest subsequence that is fibonacci-like: \[1,11,12\], \[3,11,14\] or \[7,11,18\].<br><br>Constraints:<br><br>   `3 <= arr.length <= 1000`<br>   `1 <= arr[i] < arr[i + 1] <= 109`"""<br><br><br>def match(a, b):<br>    return sum(a[i] == b[i] for i in range(6))<br><br>def find_secret_word(wordlist, master):<br>    for _ in range(10):<br>        guess = random.choice(wordlist)<br>        match_cnt = master.guess(guess)<br>        if match_cnt == 6:<br>            return<br>        wordlist = [word for word in wordlist if match(word, guess) == match_cnt]

### GROUND TRUTH 3, ranked not in top 100, leetcode/leetcode_1137.txt
> def height_checker(heights):<br>    """The Tribonacci sequence Tn is defined as follows:<br><br>T0 = 0, T1 = 1, T2 = 1, and Tn+3 = Tn + Tn+1 + Tn+2 for n = 0.<br><br>Given `n`, return the value of Tn.<br><br>Example 1:<br><br>Input: n = 4<br>Output: 4<br>Explanation:<br>T\_3 = 0 + 1 + 1 = 2<br>T\_4 = 1 + 1 + 2 = 4<br><br>Example 2:<br><br>Input: n = 25<br>Output: 1389537<br><br>Constraints:<br><br>   `0 <= n <= 37`<br>   The answer is guaranteed to fit within a 32-bit integer, ie. `answer <= 2^31 - 1`."""<br><br>    expected = sorted(heights)<br>    count = 0<br>    for i in range(len(heights)):<br>        if heights[i] != expected[i]:<br>            count += 1<br>    return count
