# Query `13`

## Original Question

*A tree is an undirected graph in which any two vertices are connected by _exactly_ one path. In other words, any connected graph without simple cycles is a tree.

Given a tree of `n` nodes labelled from `0` to `n - 1`, and an array of `n - 1` `edges` where `edges[i] = [ai, bi]` indicates that there is an undirected edge between the two nodes `ai` and `bi` in the tree, you can choose any node of the tree as the root. When you select a node `x` as the root, the result tree has height `h`. Among all possible rooted trees, those with minimum height (i.e. `min(h)`) are called minimum height trees (MHTs).

Return _a list of all MHTs' root labels_. You can return the answer in any order.

The height of a rooted tree is the number of edges on the longest downward path between the root and a leaf.

Example 1:

Input: n = 4, edges = \[\[1,0\],\[1,2\],\[1,3\]\]
Output: \[1\]
Explanation: As shown, the height of the tree is 1 when the root is the node with label 1 which is the only MHT.

Example 2:

Input: n = 6, edges = \[\[3,0\],\[3,1\],\[3,2\],\[3,4\],\[5,4\]\]
Output: \[3,4\]

Constraints:

   `1 <= n <= 2  104`
   `edges.length == n - 1`
   `0 <= ai, bi < n`
   `ai != bi`
   All the pairs `(ai, bi)` are distinct.
   The given input is guaranteed to be a tree and there will be no repeated edges.*

## gpt4 Reasoning

```markdown
The essential problem here is to find all the nodes that, when chosen as the root, yield the Minimum Height Trees (MHTs) for a given undirected tree graph. This involves understanding the structure of the tree and how the choice of root affects the height of the tree.

### Step-by-step Reasoning & Relevant Information

1. **Understanding Tree Height**: The height of a tree is determined by the longest path from the root to a leaf node. In a balanced tree, this height is minimized because the nodes are evenly distributed on all sides of the root. Therefore, finding an MHT is akin to finding a root that balances the tree as much as possible.

2. **Characteristics of MHTs**: In any tree, the nodes that are candidates for producing an MHT are generally towards the center of the tree. This is because choosing a node at the periphery (leaf node) as the root would result in a very tall tree (unbalanced), as all other nodes would be on one side of the root.

3. **Leaf Removal Process**: One efficient way to find the MHT roots is to iteratively remove the leaf nodes (nodes with only one connection) until one or two nodes remain. The reasoning behind this is that leaf nodes cannot be part of the MHT root since they are at the periphery. By removing them, we gradually move towards the center of the tree. The last one or two nodes remaining after this process are the roots of the MHTs because:
   - If one node remains, it's the center of the tree and choosing it as the root gives the minimum height.
   - If two nodes remain, they are both central and choosing either gives trees of the same minimum height. This happens in trees with an even number of nodes, where the "center" is between two nodes.

4. **Implementing the Solution**:
   - Initialize a graph to represent the tree using adjacency lists.
   - Count the degree (number of connections) for each node. Initially, leaf nodes will have a degree of 1.
   - Use a queue to perform a breadth-first search (BFS) from the leaf nodes towards the center. Enqueue all initial leaf nodes.
   - Iteratively remove leaf nodes from the graph (decrement the degree of connected nodes and enqueue the new leaf nodes formed by this removal) until only one or two nodes remain.
   - The nodes remaining are the roots of the MHTs.

5. **Edge Cases and Constraints**:
   - If `n == 1`, meaning the tree consists of a single node, that node is trivially the root of an MHT since the height is 0.
   - The algorithm needs to efficiently handle up to `2 * 10^4` nodes, requiring careful management of space and time complexity, particularly in how the graph is constructed and traversed.

### Conclusion

By focusing on the central concept that the roots of MHTs must be as centrally located within the tree as possible, we can employ a leaf removal strategy to iteratively narrow down potential candidates. This approach not only adheres to the constraints given but also ensures that we can handle large trees efficiently. The key is understanding that the process gradually "peels away" the outer layers of the tree, moving inward until the most central nodes are identified. These central nodes, being the last ones standing in the leaf removal process, offer the balanced structure necessary for minimizing tree height, thus identifying them as the roots of the Minimum Height Trees.
```

### Metrics

```
recip_rank: 0.0625
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0400
P_50: 0.0200
P_100: 0.0100
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.3333
recall_50: 0.3333
recall_100: 0.3333
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.1148
ndcg_cut_50: 0.1148
ndcg_cut_100: 0.1148
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0208
map_cut_50: 0.0208
map_cut_100: 0.0208
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_162313.txt
> def extended_stats(G, connectivity=False, anc=False, ecc=False, bc=False, cc=False):<br>    """<br>    Calculate extended topological stats and metrics for a graph.<br><br>    Many of these algorithms have an inherently high time complexity. Global<br>    topological analysis of large complex networks is extremely time consuming<br>    and may exhaust computer memory. Consider using function arguments to not<br>    run metrics that require computation of a full matrix of paths if they<br>    will not be needed.<br><br>    Parameters<br>    ----------<br>    G : networkx multidigraph<br>    connectivity : bool<br>        if True, calculate node and edge connectivity<br>    anc : bool<br>        if True, calculate average node connectivity<br>    ecc : bool<br>        if True, calculate shortest paths, eccentricity, and topological metrics<br>        that use eccentricity<br>    bc : bool<br>        if True, calculate node betweenness centrality<br>    cc : bool<br>        if True, calculate node closeness centrality<br><br>    Returns<br>    -------<br>    stats : dict<br>        dictionary of network measures containing the following elements (some<br>        only calculated/returned optionally, based on passed parameters):<br><br>          - avg_neighbor_degree<br>          - avg_neighbor_degree_avg<br>          - avg_weighted_neighbor_degree<br>          - avg_weighted_neighbor_degree_avg<br>          - degree_centrality<br>          - degree_centrality_avg<br>          - clustering_coefficient<br>          - clustering_coefficient_avg<br>          - clustering_coefficient_weighted<br>          - clustering_coefficient_weighted_avg<br>          - pagerank<br>          - pagerank_max_node<br>          - pagerank_max<br>          - pagerank_min_node<br>          - pagerank_min<br>          - node_connectivity<br>          - node_connectivity_avg<br>          - edge_connectivity<br>          - eccentricity<br>          - diameter<br>          - radius<br>          - center<br>          - periphery<br>          - closeness_centrality<br>          - closeness_centrality_avg<br>          - betweenness_centrality<br>          - betweenness_centrality_avg<br><br>    """<br><br>    stats = {}<br>    full_start_time = time.time()<br><br>    # create a DiGraph from the MultiDiGraph, for those metrics that require it<br>    G_dir = nx.DiGraph(G)<br><br>    # create an undirected Graph from the MultiDiGraph, for those metrics that<br>    # require it<br>    G_undir = nx.Graph(G)<br><br>    # get the largest strongly connected component, for those metrics that<br>    # require strongly connected graphs<br>    G_strong = get_largest_component(G, strongly=True)<br><br>    # average degree of the neighborhood of each node, and average for the graph<br>    avg_neighbor_degree = nx.average_neighbor_degree(G)<br>    stats['avg_neighbor_degree'] = avg_neighbor_degree<br>    stats['avg_neighbor_degree_avg'] = sum(avg_neighbor_degree.values())/len(avg_neighbor_degree)<br><br>    # average weighted degree of the neighborhood of each node, and average for<br>    # the graph<br>    avg_weighted_neighbor_degree = nx.average_neighbor_degree(G, weight='length')<br>    stats['avg_weighted_neighbor_degree'] = avg_weighted_neighbor_degree<br>    stats['avg_weighted_neighbor_degree_avg'] = sum(avg_weighted_neighbor_degree.values())/len(avg_weighted_neighbor_degree)<br><br>    # degree centrality for a node is the fraction of nodes it is connected to<br>    degree_centrality = nx.degree_centrality(G)<br>    stats['degree_centrality'] = degree_centrality<br>    stats['degree_centrality_avg'] = sum(degree_centrality.values())/len(degree_centrality)<br><br>    # calculate clustering coefficient for the nodes<br>    stats['clustering_coefficient'] = nx.clustering(G_undir)<br><br>    # average clustering coefficient for the graph<br>    stats['clustering_coefficient_avg'] = nx.average_clustering(G_undir)<br><br>    # calculate weighted clustering coefficient for the nodes<br>    stats['clustering_coefficient_weighted'] = nx.clustering(G_undir, weight='length')<br><br>    # average clustering coefficient (weighted) for the graph<br>    stats['clustering_coefficient_weighted_avg'] = nx.average_clustering(G_undir, weight='length')<br><br>    # pagerank: a ranking of the nodes in the graph based on the structure of<br>    # the incoming links<br>    pagerank = nx.pagerank(G_dir, weight='length')<br>    stats['pagerank'] = pagerank<br><br>    # node with the highest page rank, and its value<br>    pagerank_max_node = max(pagerank, key=lambda x: pagerank[x])<br>    stats['pagerank_max_node'] = pagerank_max_node<br>    stats['pagerank_max'] = pagerank[pagerank_max_node]<br><br>    # node with the lowest page rank, and its value<br>    pagerank_min_node = min(pagerank, key=lambda x: pagerank[x])<br>    stats['pagerank_min_node'] = pagerank_min_node<br>    stats['pagerank_min'] = pagerank[pagerank_min_node]<br><br>    # if True, calculate node and edge connectivity<br>    if connectivity:<br>        start_time = time.time()<br><br>        # node connectivity is the minimum number of nodes that must be removed<br>        # to disconnect G or render it trivial<br>        stats['node_connectivity'] = nx.node_connectivity(G_strong)<br><br>        # edge connectivity is equal to the minimum number of edges that must be<br>        # removed to disconnect G or render it trivial<br>        stats['edge_connectivity'] = nx.edge_connectivity(G_strong)<br>        log('Calculated node and edge connectivity in {:,.2f} seconds'.format(time.time() - start_time))<br><br>    # if True, calculate average node connectivity<br>    if anc:<br>        # mean number of internally node-disjoint paths between each pair of<br>        # nodes in G, i.e., the expected number of nodes that must be removed to<br>        # disconnect a randomly selected pair of non-adjacent nodes<br>        start_time = time.time()<br>        stats['node_connectivity_avg'] = nx.average_node_connectivity(G)<br>        log('Calculated average node connectivity in {:,.2f} seconds'.format(time.time() - start_time))<br><br>    # if True, calculate shortest paths, eccentricity, and topological metrics<br>    # that use eccentricity<br>    if ecc:<br>        # precompute shortest paths between all nodes for eccentricity-based<br>        # stats<br>        start_time = time.time()<br>        sp = {source:dict(nx.single_source_dijkstra_path_length(G_strong, source, weight='length')) for source in G_strong.nodes()}<br><br>        log('Calculated shortest path lengths in {:,.2f} seconds'.format(time.time() - start_time))<br><br>        # eccentricity of a node v is the maximum distance from v to all other<br>        # nodes in G<br>        eccentricity = nx.eccentricity(G_strong, sp=sp)<br>        stats['eccentricity'] = eccentricity<br><br>        # diameter is the maximum eccentricity<br>        diameter = nx.diameter(G_strong, e=eccentricity)<br>        stats['diameter'] = diameter<br><br>        # radius is the minimum eccentricity<br>        radius = nx.radius(G_strong, e=eccentricity)<br>        stats['radius'] = radius<br><br>        # center is the set of nodes with eccentricity equal to radius<br>        center = nx.center(G_strong, e=eccentricity)<br>        stats['center'] = center<br><br>        # periphery is the set of nodes with eccentricity equal to the diameter<br>        periphery = nx.periphery(G_strong, e=eccentricity)<br>        stats['periphery'] = periphery<br><br>    # if True, calculate node closeness centrality<br>    if cc:<br>        # closeness centrality of a node is the reciprocal of the sum of the<br>        # shortest path distances from u to all other nodes<br>        start_time = time.time()<br>        closeness_centrality = nx.closeness_centrality(G, distance='length')<br>        stats['closeness_centrality'] = closeness_centrality<br>        stats['closeness_centrality_avg'] = sum(closeness_centrality.values())/len(closeness_centrality)<br>        log('Calculated closeness centrality in {:,.2f} seconds'.format(time.time() - start_time))<br><br>    # if True, calculate node betweenness centrality<br>    if bc:<br>        # betweenness centrality of a node is the sum of the fraction of<br>        # all-pairs shortest paths that pass through node<br>        start_time = time.time()<br>        betweenness_centrality = nx.betweenness_centrality(G, weight='length')<br>        stats['betweenness_centrality'] = betweenness_centrality<br>        stats['betweenness_centrality_avg'] = sum(betweenness_centrality.values())/len(betweenness_centrality)<br>        log('Calculated betweenness centrality in {:,.2f} seconds'.format(time.time() - start_time))<br><br>    log('Calculated extended stats in {:,.2f} seconds'.format(time.time()-full_start_time))<br>    return stats

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_256844.txt
> def fit_image(self, sma0=None, minsma=0., maxsma=None, step=0.1,<br>                  conver=DEFAULT_CONVERGENCE, minit=DEFAULT_MINIT,<br>                  maxit=DEFAULT_MAXIT, fflag=DEFAULT_FFLAG,<br>                  maxgerr=DEFAULT_MAXGERR, sclip=3., nclip=0,<br>                  integrmode=BILINEAR, linear=False, maxrit=None):<br>        # This parameter list is quite large and should in principle be<br>        # simplified by re-distributing these controls to somewhere else.<br>        # We keep this design though because it better mimics the flat<br>        # architecture used in the original STSDAS task `ellipse`.<br>        """<br>        Fit multiple isophotes to the image array.<br><br>        This method loops over each value of the semimajor axis (sma)<br>        length (constructed from the input parameters), fitting a single<br>        isophote at each sma.  The entire set of isophotes is returned<br>        in an `~photutils.isophote.IsophoteList` instance.<br><br>        Parameters<br>        ----------<br>        sma0 : float, optional<br>            The starting value for the semimajor axis length (pixels).<br>            This value must not be the minimum or maximum semimajor axis<br>            length, but something in between. The algorithm can't start<br>            from the very center of the galaxy image because the<br>            modelling of elliptical isophotes on that region is poor and<br>            it will diverge very easily if not tied to other previously<br>            fit isophotes. It can't start from the maximum value either<br>            because the maximum is not known beforehand, depending on<br>            signal-to-noise. The ``sma0`` value should be selected such<br>            that the corresponding isophote has a good signal-to-noise<br>            ratio and a clearly defined geometry. If set to `None` (the<br>            default), one of two actions will be taken:  if a<br>            `~photutils.isophote.EllipseGeometry` instance was input to<br>            the `~photutils.isophote.Ellipse` constructor, its ``sma``<br>            value will be used.  Otherwise, a default value of 10. will<br>            be used.<br>        minsma : float, optional<br>            The minimum value for the semimajor axis length (pixels).<br>            The default is 0.<br>        maxsma : float or `None`, optional<br>            The maximum value for the semimajor axis length (pixels).<br>            When set to `None` (default), the algorithm will increase<br>            the semimajor axis until one of several conditions will<br>            cause it to stop and revert to fit ellipses with sma <<br>            ``sma0``.<br>        step : float, optional<br>            The step value used to grow/shrink the semimajor axis length<br>            (pixels if ``linear=True``, or a relative value if<br>            ``linear=False``). See the ``linear`` parameter.  The<br>            default is 0.1.<br>        conver : float, optional<br>            The main convergence criterion. Iterations stop when the<br>            largest harmonic amplitude becomes smaller (in absolute<br>            value) than ``conver`` times the harmonic fit rms.  The<br>            default is 0.05.<br>        minit : int, optional<br>            The minimum number of iterations to perform. A minimum of 10<br>            (the default) iterations guarantees that, on average, 2<br>            iterations will be available for fitting each independent<br>            parameter (the four harmonic amplitudes and the intensity<br>            level). For the first isophote, the minimum number of<br>            iterations is 2  ``minit`` to ensure that, even departing<br>            from not-so-good initial values, the algorithm has a better<br>            chance to converge to a sensible solution.<br>        maxit : int, optional<br>            The maximum number of iterations to perform.  The default is<br>            50.<br>        fflag : float, optional<br>            The acceptable fraction of flagged data points in the<br>            sample.  If the actual fraction of valid data points is<br>            smaller than this, the iterations will stop and the current<br>            `~photutils.isophote.Isophote` will be returned.  Flagged<br>            data points are points that either lie outside the image<br>            frame, are masked, or were rejected by sigma-clipping.  The<br>            default is 0.7.<br>        maxgerr : float, optional<br>            The maximum acceptable relative error in the local radial<br>            intensity gradient. This is the main control for preventing<br>            ellipses to grow to regions of too low signal-to-noise<br>            ratio.  It specifies the maximum acceptable relative error<br>            in the local radial intensity gradient.  `Busko (1996; ASPC<br>            101, 139)<br>            <http://adsabs.harvard.edu/abs/1996ASPC..101..139B`_ showed<br>            that the fitting precision relates to that relative error.<br>            The usual behavior of the gradient relative error is to<br>            increase with semimajor axis, being larger in outer, fainter<br>            regions of a galaxy image.  In the current implementation,<br>            the ``maxgerr`` criterion is triggered only when two<br>            consecutive isophotes exceed the value specified by the<br>            parameter. This prevents premature stopping caused by<br>            contamination such as stars and HII regions.<br><br>            A number of actions may happen when the gradient error<br>            exceeds ``maxgerr`` (or becomes non-significant and is set<br>            to `None`).  If the maximum semimajor axis specified by<br>            ``maxsma`` is set to `None`, semimajor axis growth is<br>            stopped and the algorithm proceeds inwards to the galaxy<br>            center. If ``maxsma`` is set to some finite value, and this<br>            value is larger than the current semimajor axis length, the<br>            algorithm enters non-iterative mode and proceeds outwards<br>            until reaching ``maxsma``.  The default is 0.5.<br>        sclip : float, optional<br>            The sigma-clip sigma value.  The default is 3.0.<br>        nclip : int, optional<br>            The number of sigma-clip interations.  The default is 0,<br>            which means sigma-clipping is skipped.<br>        integrmode : {'bilinear', 'nearest_neighbor', 'mean', 'median'}, optional<br>            The area integration mode.  The default is 'bilinear'.<br>        linear : bool, optional<br>            The semimajor axis growing/shrinking mode. If `False`<br>            (default), the geometric growing mode is chosen, thus the<br>            semimajor axis length is increased by a factor of (1. +<br>            ``step``), and the process is repeated until either the<br>            semimajor axis value reaches the value of parameter<br>            ``maxsma``, or the last fitted ellipse has more than a given<br>            fraction of its sampled points flagged out (see ``fflag``).<br>            The process then resumes from the first fitted ellipse (at<br>            ``sma0``) inwards, in steps of (1./(1. + ``step``)), until<br>            the semimajor axis length reaches the value ``minsma``. In<br>            case of linear growing, the increment or decrement value is<br>            given directly by ``step`` in pixels.  If ``maxsma`` is set<br>            to `None`, the semimajor axis will grow until a low<br>            signal-to-noise criterion is met. See ``maxgerr``.<br>        maxrit : float or `None`, optional<br>            The maximum value of semimajor axis to perform an actual<br>            fit.  Whenever the current semimajor axis length is larger<br>            than ``maxrit``, the isophotes will be extracted using the<br>            current geometry, without being fitted.  This non-iterative<br>            mode may be useful for sampling regions of very low surface<br>            brightness, where the algorithm may become unstable and<br>            unable to recover reliable geometry information.<br>            Non-iterative mode can also be entered automatically<br>            whenever the ellipticity exceeds 1.0 or the ellipse center<br>            crosses the image boundaries.  If `None` (default), then no<br>            maximum value is used.<br><br>        Returns<br>        -------<br>        result : `~photutils.isophote.IsophoteList` instance<br>            A list-like object of `~photutils.isophote.Isophote`<br>            instances, sorted by increasing semimajor axis length.<br>        """<br><br>        # multiple fitted isophotes will be stored here<br>        isophote_list = []<br><br>        # get starting sma from appropriate source: keyword parameter,<br>        # internal EllipseGeometry instance, or fixed default value.<br>        if not sma0:<br>            if self._geometry:<br>                sma = self._geometry.sma<br>            else:<br>                sma = 10.<br>        else:<br>            sma = sma0<br><br>        # first, go from initial sma outwards until<br>        # hitting one of several stopping criteria.<br>        noiter = False<br>        first_isophote = True<br>        while True:<br>            # first isophote runs longer<br>            minit_a = 2  minit if first_isophote else minit<br>            first_isophote = False<br><br>            isophote = self.fit_isophote(sma, step, conver, minit_a, maxit,<br>                                         fflag, maxgerr, sclip, nclip,<br>                                         integrmode, linear, maxrit,<br>                                         noniterate=noiter,<br>                                         isophote_list=isophote_list)<br><br>            # check for failed fit.<br>            if (isophote.stop_code < 0 or isophote.stop_code == 1):<br>                # in case the fit failed right at the outset, return an<br>                # empty list. This is the usual case when the user<br>                # provides initial guesses that are too way off to enable<br>                # the fitting algorithm to find any meaningful solution.<br><br>                if len(isophote_list) == 1:<br>                    warnings.warn('No meaningful fit was possible.',<br>                                  AstropyUserWarning)<br>                    return IsophoteList([])<br><br>                self._fix_last_isophote(isophote_list, -1)<br><br>                # get last isophote from the actual list, since the last<br>                # `isophote` instance in this context may no longer be OK.<br>                isophote = isophote_list[-1]<br><br>                # if two consecutive isophotes failed to fit,<br>                # shut off iterative mode. Or, bail out and<br>                # change to go inwards.<br>                if len(isophote_list)  2:<br>                    if ((isophote.stop_code == 5 and<br>                         isophote_list[-2].stop_code == 5)<br>                            or isophote.stop_code == 1):<br>                        if maxsma and maxsma  isophote.sma:<br>                            # if a maximum sma value was provided by<br>                            # user, and the current sma is smaller than<br>                            # maxsma, keep growing sma in non-iterative<br>                            # mode until reaching it.<br>                            noiter = True<br>                        else:<br>                            # if no maximum sma, stop growing and change<br>                            # to go inwards.<br>                            break<br><br>            # reset variable from the actual list, since the last<br>            # `isophote` instance may no longer be OK.<br>            isophote = isophote_list[-1]<br><br>            # update sma. If exceeded user-defined<br>            # maximum, bail out from this loop.<br>            sma = isophote.sample.geometry.update_sma(step)<br>            if maxsma and sma = maxsma:<br>                break<br><br>        # reset sma so as to go inwards.<br>        first_isophote = isophote_list[0]<br>        sma, step = first_isophote.sample.geometry.reset_sma(step)<br><br>        # now, go from initial sma inwards towards center.<br>        while True:<br>            isophote = self.fit_isophote(sma, step, conver, minit, maxit,<br>                                         fflag, maxgerr, sclip, nclip,<br>                                         integrmode, linear, maxrit,<br>                                         going_inwards=True,<br>                                         isophote_list=isophote_list)<br><br>            # if abnormal condition, fix isophote but keep going.<br>            if isophote.stop_code < 0:<br>                self._fix_last_isophote(isophote_list, 0)<br><br>            # reset variable from the actual list, since the last<br>            # `isophote` instance may no longer be OK.<br>            isophote = isophote_list[-1]<br><br>            # figure out next sma; if exceeded user-defined<br>            # minimum, or too small, bail out from this loop<br>            sma = isophote.sample.geometry.update_sma(step)<br>            if sma <= max(minsma, 0.5):<br>                break<br><br>        # if user asked for minsma=0, extract special isophote there<br>        if minsma == 0.0:<br>            isophote = self.fit_isophote(0.0, isophote_list=isophote_list)<br><br>        # sort list of isophotes according to sma<br>        isophote_list.sort()<br><br>        return IsophoteList(isophote_list)

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_144450.txt
> def ner_net(source, destinations, width, height, wrap_around=False, radius=10):<br>    """Produce a shortest path tree for a given net using NER.<br><br>    This is the kernel of the NER algorithm.<br><br>    Parameters<br>    ----------<br>    source : (x, y)<br>        The coordinate of the source vertex.<br>    destinations : iterable([(x, y), ...])<br>        The coordinates of destination vertices.<br>    width : int<br>        Width of the system (nodes)<br>    height : int<br>        Height of the system (nodes)<br>    wrap_around : bool<br>        True if wrap-around links should be used, false if they should be<br>        avoided.<br>    radius : int<br>        Radius of area to search from each node. 20 is arbitrarily selected in<br>        the paper and shown to be acceptable in practice.<br><br>    Returns<br>    -------<br>    (:py:class:`~.rig.place_and_route.routing_tree.RoutingTree`,<br>     {(x,y): :py:class:`~.rig.place_and_route.routing_tree.RoutingTree`, ...})<br>        A RoutingTree is produced rooted at the source and visiting all<br>        destinations but which does not contain any vertices etc. For<br>        convenience, a dictionarry mapping from destination (x, y) coordinates<br>        to the associated RoutingTree is provided to allow the caller to insert<br>        these items.<br>    """<br>    # Map from (x, y) to RoutingTree objects<br>    route = {source: RoutingTree(source)}<br><br>    # Handle each destination, sorted by distance from the source, closest<br>    # first.<br>    for destination in sorted(destinations,<br>                              key=(lambda destination:<br>                                   shortest_mesh_path_length(<br>                                       to_xyz(source), to_xyz(destination))<br>                                   if not wrap_around else<br>                                   shortest_torus_path_length(<br>                                       to_xyz(source), to_xyz(destination),<br>                                       width, height))):<br>        # We shall attempt to find our nearest neighbouring placed node.<br>        neighbour = None<br><br>        # Try to find a nearby (within radius hops) node in the routing tree<br>        # that we can route to (falling back on just routing to the source).<br>        #<br>        # In an implementation according to the algorithm's original<br>        # specification looks for nodes at each point in a growing set of rings<br>        # of concentric hexagons. If it doesn't find any destinations this<br>        # means an awful lot of checks: 1261 for the default radius of 20.<br>        #<br>        # An alternative (but behaviourally identical) implementation scans the<br>        # list of all route nodes created so far and finds the closest node<br>        # which is < radius hops (falling back on the origin if no node is<br>        # closer than radius hops).  This implementation requires one check per<br>        # existing route node. In most routes this is probably a lot less than<br>        # 1261 since most routes will probably have at most a few hundred route<br>        # nodes by the time the last destination is being routed.<br>        #<br>        # Which implementation is best is a difficult question to answer:<br>        #  In principle nets with quite localised connections (e.g.<br>        #   nearest-neighbour or centroids traffic) may route slightly more<br>        #   quickly with the original algorithm since it may very quickly find<br>        #   a neighbour.<br>        #  In nets which connect very spaced-out destinations the second<br>        #   implementation may be quicker since in such a scenario it is<br>        #   unlikely that a neighbour will be found.<br>        #  In extremely high-fan-out nets (e.g. broadcasts), the original<br>        #   method is very likely to perform far better than the alternative<br>        #   method since most iterations will complete immediately while the<br>        #   alternative method must scan all the route vertices.<br>        # As such, it should be clear that neither method alone is 'best' and<br>        # both have degenerate performance in certain completely reasonable<br>        # styles of net. As a result, a simple heuristic is used to decide<br>        # which technique to use.<br>        #<br>        # The following micro-benchmarks are crude estimate of the<br>        # runtime-per-iteration of each approach (at least in the case of a<br>        # torus topology)::<br>        #<br>        #     $ # Original approach<br>        #     $ python -m timeit --setup 'x, y, w, h, r = 1, 2, 5, 10, \<br>        #                                     {x:None for x in range(10)}' \<br>        #                        'x += 1; y += 1; x %= w; y %= h; (x, y) in r'<br>        #     1000000 loops, best of 3: 0.207 usec per loop<br>        #     $ # Alternative approach<br>        #     $ python -m timeit --setup 'from rig.geometry import \<br>        #                                 shortest_torus_path_length' \<br>        #                        'shortest_torus_path_length( \<br>        #                             (0, 1, 2), (3, 2, 1), 10, 10)'<br>        #     1000000 loops, best of 3: 0.666 usec per loop<br>        #<br>        # From this we can approximately suggest that the alternative approach<br>        # is 3x more expensive per iteration. A very crude heuristic is to use<br>        # the original approach when the number of route nodes is more than<br>        # 1/3rd of the number of routes checked by the original method.<br>        concentric_hexagons = memoized_concentric_hexagons(radius)<br>        if len(concentric_hexagons) < len(route) / 3:<br>            # Original approach: Start looking for route nodes in a concentric<br>            # spiral pattern out from the destination node.<br>            for x, y in concentric_hexagons:<br>                x += destination[0]<br>                y += destination[1]<br>                if wrap_around:<br>                    x %= width<br>                    y %= height<br>                if (x, y) in route:<br>                    neighbour = (x, y)<br>                    break<br>        else:<br>            # Alternative approach: Scan over every route node and check to see<br>            # if any are < radius, picking the closest one if so.<br>            neighbour = None<br>            neighbour_distance = None<br>            for candidate_neighbour in route:<br>                if wrap_around:<br>                    distance = shortest_torus_path_length(<br>                        to_xyz(candidate_neighbour), to_xyz(destination),<br>                        width, height)<br>                else:<br>                    distance = shortest_mesh_path_length(<br>                        to_xyz(candidate_neighbour), to_xyz(destination))<br><br>                if distance <= radius and (neighbour is None or<br>                                           distance < neighbour_distance):<br>                    neighbour = candidate_neighbour<br>                    neighbour_distance = distance<br><br>        # Fall back on routing directly to the source if no nodes within radius<br>        # hops of the destination was found.<br>        if neighbour is None:<br>            neighbour = source<br><br>        # Find the shortest vector from the neighbour to this destination<br>        if wrap_around:<br>            vector = shortest_torus_path(to_xyz(neighbour),<br>                                         to_xyz(destination),<br>                                         width, height)<br>        else:<br>            vector = shortest_mesh_path(to_xyz(neighbour), to_xyz(destination))<br><br>        # The longest-dimension-first route may inadvertently pass through an<br>        # already connected node. If the route is allowed to pass through that<br>        # node it would create a cycle in the route which would be VeryBad(TM).<br>        # As a result, we work backward through the route and truncate it at<br>        # the first point where the route intersects with a connected node.<br>        ldf = longest_dimension_first(vector, neighbour, width, height)<br>        i = len(ldf)<br>        for direction, (x, y) in reversed(ldf):<br>            i -= 1<br>            if (x, y) in route:<br>                # We've just bumped into a node which is already part of the<br>                # route, this becomes our new neighbour and we truncate the LDF<br>                # route. (Note ldf list is truncated just after the current<br>                # position since it gives (direction, destination) pairs).<br>                neighbour = (x, y)<br>                ldf = ldf[i + 1:]<br>                break<br><br>        # Take the longest dimension first route.<br>        last_node = route[neighbour]<br>        for direction, (x, y) in ldf:<br>            this_node = RoutingTree((x, y))<br>            route[(x, y)] = this_node<br><br>            last_node.children.append((Routes(direction), this_node))<br>            last_node = this_node<br><br>    return (route[source], route)

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_144360.txt
> def place(vertices_resources, nets, machine, constraints,<br>          effort=1.0, random=default_random, on_temperature_change=None,<br>          kernel=default_kernel, kernel_kwargs={}):<br>    """A flat Simulated Annealing based placement algorithm.<br><br>    This placement algorithm uses simulated annealing directly on the supplied<br>    problem graph with the objective of reducing wire lengths (and thus,<br>    indirectly, the potential for congestion). Though computationally<br>    expensive, this placer produces relatively good placement solutions.<br><br>    The annealing temperature schedule used by this algorithm is taken from<br>    "VPR: A New Packing, Placement and Routing Tool for FPGA Research" by<br>    Vaughn Betz and Jonathan Rose from the "1997 International Workshop on<br>    Field Programmable Logic and Applications".<br><br>    Two implementations of the algorithm's kernel are available:<br><br>     :py:class:`~rig.place_and_route.place.sa.python_kernel.PythonKernel` A<br>      pure Python implementation which is available on all platforms supported<br>      by Rig.<br>     :py:class:`~rig.place_and_route.place.sa.c_kernel.CKernel` A C<br>      implementation which is typically 50-150x faster than the basic Python<br>      kernel. Since this implementation requires a C compiler during<br>      installation, it is an optional feature of Rig. See the<br>      :py:class:`CKernel's documentation<br>      <rig.place_and_route.place.sa.c_kernel.CKernel` for details.<br><br>    The fastest kernel installed is used by default and can be manually chosen<br>    using the ``kernel`` argument.<br><br>    This algorithm produces INFO level logging information describing the<br>    progress made by the algorithm.<br><br>    .. warning:<br>        This algorithm does not attempt to produce good solutions to the<br>        bin-packing problem of optimally fitting vertices into chips and it may<br>        fail if a good placement requires good bin packing.<br><br>    Parameters<br>    ----------<br>    effort : float<br>        A scaling factor for the number of iterations the algorithm should run<br>        for. 1.0 is probably about as low as you'll want to go in practice and<br>        runtime increases linearly as you increase this parameter.<br>    random : :py:class:`random.Random`<br>        A Python random number generator. Defaults to ``import random`` but can<br>        be set to your own instance of :py:class:`random.Random` to allow you<br>        to control the seed and produce deterministic results. For results to<br>        be deterministic, vertices_resources must be supplied as an<br>        :py:class:`collections.OrderedDict`.<br>    on_temperature_change : callback_function or None<br>        An (optional) callback function which is called every time the<br>        temperature is changed. This callback can be used to provide status<br>        updates<br><br>        The callback function is passed the following arguments:<br><br>         ``iteration_count``: the number of iterations the placer has<br>          attempted (integer)<br>         ``placements``: The current placement solution.<br>         ``cost``: the weighted sum over all nets of bounding-box size.<br>          (float)<br>         ``acceptance_rate``: the proportion of iterations which have resulted<br>          in an accepted change since the last callback call. (float between<br>          0.0 and 1.0)<br>         ``temperature``: The current annealing temperature. (float)<br>         ``distance_limit``: The maximum distance any swap may be made over.<br>          (integer)<br><br>        If the callback returns False, the anneal is terminated immediately and<br>        the current solution is returned.<br>    kernel : :py:class:`~rig.place_and_route.place.sa.kernel.Kernel`<br>        A simulated annealing placement kernel. A sensible default will be<br>        chosen based on the available kernels on this machine. The kernel may<br>        not be used if the placement problem has a trivial solution.<br>    kernel_kwargs : dict<br>        Optional kernel-specific keyword arguments to pass to the kernel<br>        constructor.<br>    """<br>    # Special case: just return immediately when there's nothing to place<br>    if len(vertices_resources) == 0:<br>        return {}<br><br>    # Within the algorithm we modify the resource availability values in the<br>    # machine to account for the effects of the current placement. As a result,<br>    # an internal copy of the structure must be made.<br>    machine = machine.copy()<br><br>    # {vertex: (x, y), ...} gives the location of all vertices whose position<br>    # is fixed by a LocationConstraint.<br>    fixed_vertices = {}<br><br>    # Handle constraints<br>    vertices_resources, nets, constraints, substitutions = \<br>        apply_same_chip_constraints(vertices_resources, nets, constraints)<br>    for constraint in constraints:<br>        if isinstance(constraint, LocationConstraint):<br>            # Location constraints are handled by recording the set of fixed<br>            # vertex locations and subtracting their resources from the chips<br>            # they're allocated to. These vertices will then not be added to<br>            # the internal placement data structure to prevent annealing from<br>            # moving them. They will be re-introduced at the last possible<br>            # moment.<br>            location = constraint.location<br>            if location not in machine:<br>                raise InvalidConstraintError(<br>                    "Chip requested by {} unavailable".format(machine))<br>            vertex = constraint.vertex<br><br>            # Record the constrained vertex's location<br>            fixed_vertices[vertex] = location<br><br>            # Make sure the vertex fits at the requested location (updating the<br>            # resource availability after placement)<br>            resources = vertices_resources[vertex]<br>            machine[location] = subtract_resources(machine[location],<br>                                                   resources)<br>            if overallocated(machine[location]):<br>                raise InsufficientResourceError(<br>                    "Cannot meet {}".format(constraint))<br>        elif isinstance(constraint,  # pragma: no branch<br>                        ReserveResourceConstraint):<br>            apply_reserve_resource_constraint(machine, constraint)<br><br>    # Initially randomly place the movable vertices<br>    movable_vertices = {v for v in vertices_resources<br>                        if v not in fixed_vertices}<br>    initial_placements = _initial_placement(movable_vertices,<br>                                            vertices_resources,<br>                                            machine, random)<br><br>    # Include the fixed vertices in initial placement<br>    initial_placements.update(fixed_vertices)<br><br>    # Filter out empty or singleton nets and those weighted as zero since they<br>    # cannot influence placement.<br>    nets = [n for n in nets if len(set(n))  1 and n.weight  0.0]<br><br>    # Special cases where no placement effort is required:<br>    #  There is only one chip<br>    #  There are no resource types to be consumed<br>    #  No effort is to be made<br>    #  No movable vertices<br>    #  There are no nets (and moving things has no effect)<br>    trivial = ((machine.width, machine.height) == (1, 1) or<br>               len(machine.chip_resources) == 0 or<br>               effort == 0.0 or<br>               len(movable_vertices) == 0 or<br>               len(nets) == 0)<br>    if trivial:<br>        logger.info("Placement has trivial solution. SA not used.")<br>        finalise_same_chip_constraints(substitutions, initial_placements)<br>        return initial_placements<br><br>    # Intialise the algorithm kernel<br>    k = kernel(vertices_resources, movable_vertices, set(fixed_vertices),<br>               initial_placements, nets, machine, random, kernel_kwargs)<br><br>    logger.info("SA placement kernel: %s", kernel.__name__)<br><br>    # Specifies the maximum distance any swap can span. Initially consider<br>    # swaps that span the entire machine.<br>    distance_limit = max(machine.width, machine.height)<br><br>    # Determine initial temperature according to the heuristic used by VPR: 20<br>    # times the standard deviation of len(movable_vertices) random swap costs.<br>    # The arbitrary very-high temperature is used to cause "all" swaps to be<br>    # accepted.<br>    _0, _1, cost_delta_sd = k.run_steps(len(movable_vertices),<br>                                        distance_limit,<br>                                        1e100)<br>    temperature = 20.0  cost_delta_sd<br><br>    # The number of swap-attempts between temperature changes is selected by<br>    # the heuristic used by VPR. This value is scaled linearly by the effort<br>    # parameter.<br>    num_steps = max(1, int(effort  len(vertices_resources)1.33))<br><br>    logger.info("Initial placement temperature: %0.1f", temperature)<br><br>    # Counter for the number of swap attempts made (used for diagnostic<br>    # purposes)<br>    iteration_count = 0<br><br>    # Holds the total cost of the current placement. This default value chosen<br>    # to ensure the loop below iterates at least once.<br>    current_cost = 0.0<br><br>    # The annealing algorithm runs until a heuristic termination condition<br>    # (taken from VPR) is hit. The heuristic waits until the temperature falls<br>    # below a small fraction of the average net cost.<br>    while temperature  (0.005  current_cost) / len(nets):<br>        # Run an iteration at the current temperature<br>        num_accepted, current_cost, _ = k.run_steps(<br>            num_steps, int(math.ceil(distance_limit)), temperature)<br><br>        # The ratio of accepted-to-not-accepted changes<br>        r_accept = num_accepted / float(num_steps)<br><br>        # Special case: Can't do better than 0 cost! This is a special case<br>        # since the normal termination condition will not terminate if the cost<br>        # doesn't drop below 0.<br>        if current_cost == 0:<br>            break<br><br>        # The temperature is reduced by a factor heuristically based on the<br>        # acceptance rate. The schedule below attempts to maximise the time<br>        # spent at temperatures where a large portion (but not all) of changes<br>        # are being accepted. If lots of changes are being accepted (e.g.<br>        # during high-temperature periods) then most of them are likely not to<br>        # be beneficial. If few changes are being accepted, we're probably<br>        # pretty close to the optimal placement.<br>        if r_accept  0.96:<br>            alpha = 0.5<br>        elif r_accept  0.8:<br>            alpha = 0.9<br>        elif r_accept  0.15:<br>            alpha = 0.95<br>        else:<br>            alpha = 0.8<br>        temperature = alpha  temperature<br><br>        # According to:<br>        #  M. Huang, F. Romeo, and A. Sangiovanni-Vincentelli, "An Efficient<br>        #   General Cooling Schedule for Simulated Annealing" ICCAD, 1986, pp.<br>        #   381 - 384 and J. Lam<br>        #  J. Delosme, "Performance of a New Annealing Schedule" DAC, 1988,<br>        #   pp. 306 - 311.<br>        # It is desirable to keep the acceptance ratio as close to 0.44 for as<br>        # long as possible. As a result, when r_accept falls below this we can<br>        # help increase the acceptance rate by reducing the set of possible<br>        # swap candidates based on the observation that near the end of<br>        # placement, most things are near their optimal location and thus long<br>        # distance swaps are unlikely to be useful.<br>        distance_limit = 1.0 - 0.44 + r_accept<br>        distance_limit = min(max(distance_limit, 1.0),<br>                             max(machine.width, machine.height))<br><br>        iteration_count += num_steps<br>        logger.debug("Iteration: %d, "<br>                     "Cost: %0.1f, "<br>                     "Kept: %0.1f%%, "<br>                     "Temp: %0.3f, "<br>                     "Dist: %d.",<br>                     iteration_count, current_cost,<br>                     r_accept100, temperature, math.ceil(distance_limit))<br><br>        # Call the user callback before the next iteration, terminating if<br>        # requested.<br>        if on_temperature_change is not None:<br>            placements = k.get_placements().copy()<br>            finalise_same_chip_constraints(substitutions, placements)<br>            ret_val = on_temperature_change(iteration_count,<br>                                            placements,<br>                                            current_cost,<br>                                            r_accept,<br>                                            temperature,<br>                                            distance_limit)<br>            if ret_val is False:<br>                break<br><br>    logger.info("Anneal terminated after %d iterations.", iteration_count)<br><br>    placements = k.get_placements()<br>    finalise_same_chip_constraints(substitutions, placements)<br><br>    return placements

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_333992.txt
> def canonical_order(self):<br>        """The vertices in a canonical or normalized order.<br><br>           This routine will return a list of vertices in an order that does not<br>           depend on the initial order, but only depends on the connectivity and<br>           the return values of the function self.get_vertex_string.<br><br>           Only the vertices that are involved in edges will be included. The<br>           result can be given as first argument to self.get_subgraph, with<br>           reduce=True as second argument. This will return a complete canonical<br>           graph.<br><br>           The routine is designed not to use symmetry relations that are<br>           obtained with the GraphSearch routine. We also tried to create an<br>           ordering that feels like natural, i.e. starting in the center and<br>           pushing vertices with few equivalents to the front. If necessary, the<br>           nature of the vertices and  their bonds to atoms closer to the center<br>           will also play a role, but only as a last resort.<br>        """<br>        # A) find an appropriate starting vertex.<br>        # Here we take a central vertex that has a minimal number of symmetrical<br>        # equivalents, 'the highest atom number', and the highest fingerprint.<br>        # Note that the symmetrical equivalents are computed from the vertex<br>        # fingerprints, i.e. without the GraphSearch.<br>        starting_vertex = max(<br>            (<br>                -len(self.equivalent_vertices[vertex]),<br>                self.get_vertex_string(vertex),<br>                self.vertex_fingerprints[vertex].tobytes(),<br>                vertex<br>            ) for vertex in self.central_vertices<br>        )[-1]<br><br>        # B) sort all vertices based on<br>        #      1) distance from central vertex<br>        #      2) number of equivalent vertices<br>        #      3) vertex string, (higher atom numbers come first)<br>        #      4) fingerprint<br>        #      5) vertex index<br>        # The last field is only included to collect the result of the sort.<br>        # The fingerprint on itself would be sufficient, but the three first are<br>        # there to have a naturally appealing result.<br>        l = [<br>            [<br>                -distance,<br>                -len(self.equivalent_vertices[vertex]),<br>                self.get_vertex_string(vertex),<br>                self.vertex_fingerprints[vertex].tobytes(),<br>                vertex<br>            ] for vertex, distance in self.iter_breadth_first(starting_vertex)<br>            if len(self.neighbors[vertex])  0<br>        ]<br>        l.sort(reverse=True)<br><br>        # C) The order of some vertices is still not completely set. e.g.<br>        # consider the case of allene. The four hydrogen atoms are equivalent,<br>        # but one can have two different orders: make geminiles consecutive or<br>        # don't. It is more trikcy than one would think at first sight. In the<br>        # case of allene, geminility could easily solve the problem. Consider a<br>        # big flat rotationally symmetric molecule (order 2). The first five<br>        # shells are order 4 and one would just give a random order to four<br>        # segemnts in the first shell. Only when one reaches the outer part that<br>        # has order two, it turns out that the arbitrary choices in the inner<br>        # shell play a role. So it does not help to look at relations with<br>        # vertices at inner or current shells only. One has to consider the<br>        # whole picture. (unit testing reveals troubles like these)<br><br>        # I need some sleep now. The code below checks for potential fuzz and<br>        # will raise an error if the ordering is not fully determined yet. One<br>        # day, I'll need this code more than I do now, and I'll fix things up.<br>        # I know how to do this, but I don't care enough right now.<br>        # -- Toon<br>        for i in range(1, len(l)):<br>            if l[i][:-1] == l[i-1][:-1]:<br>                raise NotImplementedError<br><br>        # D) Return only the vertex indexes.<br>        return [record[-1] for record in l]


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_207.txt
> from collections import defaultdict, deque<br>    """There are a total of `numCourses` courses you have to take, labeled from `0` to `numCourses - 1`. You are given an array `prerequisites` where `prerequisites[i] = [ai, bi]` indicates that you must take course `bi` first if you want to take course `ai`.<br><br>   For example, the pair `[0, 1]`, indicates that to take course `0` you have to first take course `1`.<br><br>Return `true` if you can finish all courses. Otherwise, return `false`.<br><br>Example 1:<br><br>Input: numCourses = 2, prerequisites = \[\[1,0\]\]<br>Output: true<br>Explanation: There are a total of 2 courses to take. <br>To take course 1 you should have finished course 0. So it is possible.<br><br>Example 2:<br><br>Input: numCourses = 2, prerequisites = \[\[1,0\],\[0,1\]\]<br>Output: false<br>Explanation: There are a total of 2 courses to take. <br>To take course 1 you should have finished course 0, and to take course 0 you should also have finished course 1. So it is impossible.<br><br>Constraints:<br><br>   `1 <= numCourses <= 2000`<br>   `0 <= prerequisites.length <= 5000`<br>   `prerequisites[i].length == 2`<br>   `0 <= ai, bi < numCourses`<br>   All the pairs prerequisites\[i\] are unique."""<br><br><br>def canFinish(numCourses, prerequisites):<br>    adj = defaultdict(list)<br>    inDegree = [0]  numCourses<br>    <br>    for a, b in prerequisites:<br>        adj[b].append(a)<br>        inDegree[a] += 1<br>    <br>    q = deque([i for i in range(numCourses) if inDegree[i]==0])<br>    <br>    while q:<br>        currCourse = q.popleft()<br>        numCourses -= 1<br>        <br>        for nextCourse in adj[currCourse]:<br>            inDegree[nextCourse] -= 1<br>            if inDegree[nextCourse] == 0:<br>                q.append(nextCourse)<br>    <br>    return numCourses == 0

### GROUND TRUTH 1, ranked not in top 100, leetcode/leetcode_210.txt
> from collections import deque<br>    """There are a total of `numCourses` courses you have to take, labeled from `0` to `numCourses - 1`. You are given an array `prerequisites` where `prerequisites[i] = [ai, bi]` indicates that you must take course `bi` first if you want to take course `ai`.<br><br>   For example, the pair `[0, 1]`, indicates that to take course `0` you have to first take course `1`.<br><br>Return _the ordering of courses you should take to finish all courses_. If there are many valid answers, return any of them. If it is impossible to finish all courses, return an empty array.<br><br>Example 1:<br><br>Input: numCourses = 2, prerequisites = \[\[1,0\]\]<br>Output: \[0,1\]<br>Explanation: There are a total of 2 courses to take. To take course 1 you should have finished course 0. So the correct course order is \[0,1\].<br><br>Example 2:<br><br>Input: numCourses = 4, prerequisites = \[\[1,0\],\[2,0\],\[3,1\],\[3,2\]\]<br>Output: \[0,2,1,3\]<br>Explanation: There are a total of 4 courses to take. To take course 3 you should have finished both courses 1 and 2. Both courses 1 and 2 should be taken after you finished course 0.<br>So one correct course order is \[0,1,2,3\]. Another correct ordering is \[0,2,1,3\].<br><br>Example 3:<br><br>Input: numCourses = 1, prerequisites = \[\]<br>Output: \[0\]<br><br>Constraints:<br><br>   `1 <= numCourses <= 2000`<br>   `0 <= prerequisites.length <= numCourses  (numCourses - 1)`<br>   `prerequisites[i].length == 2`<br>   `0 <= ai, bi < numCourses`<br>   `ai != bi`<br>   All the pairs `[ai, bi]` are distinct."""<br><br><br>def findOrder(numCourses, prerequisites):<br>    in_degree = [0]  numCourses<br>    adj_list = [[] for _ in range(numCourses)]<br>    <br>    for p in prerequisites:<br>        adj_list[p[1]].append(p[0])<br>        in_degree[p[0]] += 1<br>    <br>    q = deque([i for i in range(numCourses) if in_degree[i] == 0])<br>    order = []<br>    <br>    while q:<br>        curr = q.popleft()<br>        order.append(curr)<br>        for next in adj_list[curr]:<br>            in_degree[next] -= 1<br>            if in_degree[next] == 0:<br>                q.append(next)<br>    <br>    return order if len(order) == numCourses else []

### GROUND TRUTH 2, ranked 15, leetcode/leetcode_2603.txt
> from collections import defaultdict<br>    """There exists an undirected and unrooted tree with `n` nodes indexed from `0` to `n - 1`. You are given an integer `n` and a 2D integer array edges of length `n - 1`, where `edges[i] = [ai, bi]` indicates that there is an edge between nodes `ai` and `bi` in the tree. You are also given an array `coins` of size `n` where `coins[i]` can be either `0` or `1`, where `1` indicates the presence of a coin in the vertex `i`.<br><br>Initially, you choose to start at any vertex in the tree. Then, you can perform the following operations any number of times:<br><br>   Collect all the coins that are at a distance of at most `2` from the current vertex, or<br>   Move to any adjacent vertex in the tree.<br><br>Find _the minimum number of edges you need to go through to collect all the coins and go back to the initial vertex_.<br><br>Note that if you pass an edge several times, you need to count it into the answer several times.<br><br>Example 1:<br><br>Input: coins = \[1,0,0,0,0,1\], edges = \[\[0,1\],\[1,2\],\[2,3\],\[3,4\],\[4,5\]\]<br>Output: 2<br>Explanation: Start at vertex 2, collect the coin at vertex 0, move to vertex 3, collect the coin at vertex 5 then move back to vertex 2.<br><br>Example 2:<br><br>Input: coins = \[0,0,0,1,1,0,0,1\], edges = \[\[0,1\],\[0,2\],\[1,3\],\[1,4\],\[2,5\],\[5,6\],\[5,7\]\]<br>Output: 2<br>Explanation: Start at vertex 0, collect the coins at vertices 4 and 3, move to vertex 2,  collect the coin at vertex 7, then move back to vertex 0.<br><br>Constraints:<br><br>   `n == coins.length`<br>   `1 <= n <= 3  104`<br>   `0 <= coins[i] <= 1`<br>   `edges.length == n - 1`<br>   `edges[i].length == 2`<br>   `0 <= ai, bi < n`<br>   `ai != bi`<br>   `edges` represents a valid tree."""<br><br><br>def min_edges_to_collect_coins(coins, edges):<br>    adj = defaultdict(list)<br>    for a, b in edges:<br>        adj[a].append(b)<br>        adj[b].append(a)<br>    return dfs(adj, coins, 0, -1)[1] // 2<br><br>def dfs(adj, coins, node, parent):<br>    sum_coins = coins[node]<br>    distance = 0<br>    for neighbor in adj[node]:<br>        if neighbor != parent:<br>            result = dfs(adj, coins, neighbor, node)<br>            sum_coins += result[0]<br>            distance += 2  result[0] + result[1]<br>    return (sum_coins, distance)
