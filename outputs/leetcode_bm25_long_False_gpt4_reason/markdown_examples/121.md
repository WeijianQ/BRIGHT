# Query `121`

## Original Question

*Alice manages a company and has rented some floors of a building as office space. Alice has decided some of these floors should be special floors, used for relaxation only.

You are given two integers `bottom` and `top`, which denote that Alice has rented all the floors from `bottom` to `top` (inclusive). You are also given the integer array `special`, where `special[i]` denotes a special floor that Alice has designated for relaxation.

Return _the maximum number of consecutive floors without a special floor_.

Example 1:

Input: bottom = 2, top = 9, special = \[4,6\]
Output: 3
Explanation: The following are the ranges (inclusive) of consecutive floors without a special floor:
- (2, 3) with a total amount of 2 floors.
- (5, 5) with a total amount of 1 floor.
- (7, 9) with a total amount of 3 floors.
Therefore, we return the maximum number which is 3 floors.

Example 2:

Input: bottom = 6, top = 8, special = \[7,6,8\]
Output: 0
Explanation: Every floor rented is a special floor, so we return 0.

Constraints:

   `1 <= special.length <= 105`
   `1 <= bottom <= special[i] <= top <= 109`
   All the values of `special` are unique.*

## gpt4 Reasoning

```markdown
The essential problem here is to find the maximum number of consecutive floors within the range of floors rented by Alice (`bottom` to `top`, inclusive) that do not contain any of the floors designated as special (`special` array). To solve this problem, we need to consider the distribution of the special floors within the rented floors and calculate the gaps between these special floors as well as the gaps from the `bottom` floor to the first special floor and from the last special floor to the `top` floor.

Here's a step-by-step approach to solve the problem:

1. **Sort the Special Floors Array**: First, ensure that the `special` floors array is sorted in ascending order. This is crucial because it allows us to efficiently calculate the consecutive non-special floors between each pair of special floors.

2. **Initialize Variables**: Initialize a variable to keep track of the maximum number of consecutive non-special floors. This variable will be updated as we iterate through the `special` floors.

3. **Calculate Initial Gap**: Calculate the gap between the `bottom` floor and the first special floor, if any. This gap is equal to `special[0] - bottom`. If this is the maximum gap so far, update the maximum.

4. **Iterate Through Special Floors**: Loop through the `special` array starting from the first element. For each pair of consecutive special floors, calculate the gap between them, which is `special[i+1] - special[i] - 1` (subtracting 1 because both `special[i]` and `special[i+1]` are special floors and do not count towards the consecutive non-special floors).

5. **Update Maximum**: If the gap calculated in step 4 is greater than the current maximum, update the maximum.

6. **Calculate Final Gap**: After the loop, calculate the gap between the last special floor and the `top` floor, which is `top - special[special.length - 1]`. Again, if this is the maximum gap so far, update the maximum.

7. **Return the Maximum**: Finally, return the maximum number of consecutive non-special floors found.

Here's a quick example to illustrate these steps with the first input example:

- **Input:** `bottom = 2, top = 9, special = [4,6]`
- **Step 1:** `special` is already sorted.
- **Step 2:** Initialize `maxConsecutive = 0`.
- **Step 3:** Calculate initial gap: `4 - 2 = 2`. `maxConsecutive = 2`.
- **Step 4:** Iterate through `special`. The gap between `4` and `6` is `1`. No update to `maxConsecutive`.
- **Step 6:** Calculate final gap: `9 - 6 = 3`. Update `maxConsecutive = 3`.
- **Step 7:** Return `maxConsecutive = 3`.

For the second example:

- **Input:** `bottom = 6, top = 8, special = [7,6,8]`
- After sorting and calculating the gaps as per the steps, we find that every floor is a special floor, so the maximum number of consecutive non-special floors is `0`.

This problem requires understanding how to efficiently navigate and manipulate arrays and perform calculations based on their values. The key is to recognize that the solution involves comparing the differences between adjacent elements (special floors) and also considering the edges of the range (`bottom` and `top`).
```

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_244664.txt
> def lookup(self, form, df, only_built=True, pass_through=None):<br>        """<br>        This function does the developer model lookups for all the actual input data.<br><br>        Parameters<br>        ----------<br>        form : string<br>            One of the forms specified in the configuration file<br>        df: dataframe<br>            Pass in a single data frame which is indexed by parcel_id and has the<br>            following columns<br>        only_built : bool<br>            Whether to return only those buildings that are profitable and allowed<br>            by zoning, or whether to return as much information as possible, even if<br>            unlikely to be built (can be used when development might be subsidized<br>            or when debugging)<br>        pass_through : list of strings<br>            List of field names to take from the input parcel frame and pass<br>            to the output feasibility frame - is usually used for debugging<br>            purposes - these fields will be passed all the way through<br>            developer<br><br>        Input Dataframe Columns<br>        rent : dataframe<br>            A set of columns, one for each of the uses passed in the configuration.<br>            Values are yearly rents for that use.  Typical column names would be<br>            "residential", "retail", "industrial" and "office"<br>        land_cost : series<br>            A series representing the CURRENT yearly rent for each parcel.  Used to<br>            compute acquisition costs for the parcel.<br>        parcel_size : series<br>            A series representing the parcel size for each parcel.<br>        max_far : series<br>            A series representing the maximum far allowed by zoning.  Buildings<br>            will not be built above these fars.<br>        max_height : series<br>            A series representing the maxmium height allowed by zoning.  Buildings<br>            will not be built above these heights.  Will pick between the min of<br>            the far and height, will ignore on of them if one is nan, but will not<br>            build if both are nan.<br>        max_dua : series, optional<br>            A series representing the maximum dwelling units per acre allowed by<br>            zoning.  If max_dua is passed, the average unit size should be passed<br>            below to translate from dua to floor space.<br>        ave_unit_size : series, optional<br>            This is required if max_dua is passed above, otherwise it is optional.<br>            This is the same as the parameter to Developer.pick() (it should be the<br>            same series).<br><br>        Returns<br>        -------<br>        index : Series, int<br>            parcel identifiers<br>        building_sqft : Series, float<br>            The number of square feet for the building to build.  Keep in mind<br>            this includes parking and common space.  Will need a helpful function<br>            to convert from gross square feet to actual usable square feet in<br>            residential units.<br>        building_cost : Series, float<br>            The cost of constructing the building as given by the<br>            ave_cost_per_sqft from the cost model (for this FAR) and the number<br>            of square feet.<br>        total_cost : Series, float<br>            The cost of constructing the building plus the cost of acquisition of<br>            the current parcel/building.<br>        building_revenue : Series, float<br>            The NPV of the revenue for the building to be built, which is the<br>            number of square feet times the yearly rent divided by the cap<br>            rate (with a few adjustment factors including building efficiency).<br>        max_profit_far : Series, float<br>            The FAR of the maximum profit building (constrained by the max_far and<br>            max_height from the input dataframe).<br>        max_profit :<br>            The profit for the maximum profit building (constrained by the max_far<br>            and max_height from the input dataframe).<br><br>        """<br>        df = pd.concat(self._lookup_parking_cfg(form, parking_config, df, only_built,<br>                                                pass_through)<br>                       for parking_config in self.config.parking_configs)<br><br>        if len(df) == 0:<br>            return pd.DataFrame()<br><br>        max_profit_ind = df.pivot(<br>            columns="parking_config",<br>            values="max_profit").idxmax(axis=1).to_frame("parking_config")<br><br>        df.set_index(["parking_config"], append=True, inplace=True)<br>        max_profit_ind.set_index(["parking_config"], append=True, inplace=True)<br><br>        # get the max_profit idx<br>        return df.loc[max_profit_ind.index].reset_index(1)

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_256844.txt
> def fit_image(self, sma0=None, minsma=0., maxsma=None, step=0.1,<br>                  conver=DEFAULT_CONVERGENCE, minit=DEFAULT_MINIT,<br>                  maxit=DEFAULT_MAXIT, fflag=DEFAULT_FFLAG,<br>                  maxgerr=DEFAULT_MAXGERR, sclip=3., nclip=0,<br>                  integrmode=BILINEAR, linear=False, maxrit=None):<br>        # This parameter list is quite large and should in principle be<br>        # simplified by re-distributing these controls to somewhere else.<br>        # We keep this design though because it better mimics the flat<br>        # architecture used in the original STSDAS task `ellipse`.<br>        """<br>        Fit multiple isophotes to the image array.<br><br>        This method loops over each value of the semimajor axis (sma)<br>        length (constructed from the input parameters), fitting a single<br>        isophote at each sma.  The entire set of isophotes is returned<br>        in an `~photutils.isophote.IsophoteList` instance.<br><br>        Parameters<br>        ----------<br>        sma0 : float, optional<br>            The starting value for the semimajor axis length (pixels).<br>            This value must not be the minimum or maximum semimajor axis<br>            length, but something in between. The algorithm can't start<br>            from the very center of the galaxy image because the<br>            modelling of elliptical isophotes on that region is poor and<br>            it will diverge very easily if not tied to other previously<br>            fit isophotes. It can't start from the maximum value either<br>            because the maximum is not known beforehand, depending on<br>            signal-to-noise. The ``sma0`` value should be selected such<br>            that the corresponding isophote has a good signal-to-noise<br>            ratio and a clearly defined geometry. If set to `None` (the<br>            default), one of two actions will be taken:  if a<br>            `~photutils.isophote.EllipseGeometry` instance was input to<br>            the `~photutils.isophote.Ellipse` constructor, its ``sma``<br>            value will be used.  Otherwise, a default value of 10. will<br>            be used.<br>        minsma : float, optional<br>            The minimum value for the semimajor axis length (pixels).<br>            The default is 0.<br>        maxsma : float or `None`, optional<br>            The maximum value for the semimajor axis length (pixels).<br>            When set to `None` (default), the algorithm will increase<br>            the semimajor axis until one of several conditions will<br>            cause it to stop and revert to fit ellipses with sma <<br>            ``sma0``.<br>        step : float, optional<br>            The step value used to grow/shrink the semimajor axis length<br>            (pixels if ``linear=True``, or a relative value if<br>            ``linear=False``). See the ``linear`` parameter.  The<br>            default is 0.1.<br>        conver : float, optional<br>            The main convergence criterion. Iterations stop when the<br>            largest harmonic amplitude becomes smaller (in absolute<br>            value) than ``conver`` times the harmonic fit rms.  The<br>            default is 0.05.<br>        minit : int, optional<br>            The minimum number of iterations to perform. A minimum of 10<br>            (the default) iterations guarantees that, on average, 2<br>            iterations will be available for fitting each independent<br>            parameter (the four harmonic amplitudes and the intensity<br>            level). For the first isophote, the minimum number of<br>            iterations is 2  ``minit`` to ensure that, even departing<br>            from not-so-good initial values, the algorithm has a better<br>            chance to converge to a sensible solution.<br>        maxit : int, optional<br>            The maximum number of iterations to perform.  The default is<br>            50.<br>        fflag : float, optional<br>            The acceptable fraction of flagged data points in the<br>            sample.  If the actual fraction of valid data points is<br>            smaller than this, the iterations will stop and the current<br>            `~photutils.isophote.Isophote` will be returned.  Flagged<br>            data points are points that either lie outside the image<br>            frame, are masked, or were rejected by sigma-clipping.  The<br>            default is 0.7.<br>        maxgerr : float, optional<br>            The maximum acceptable relative error in the local radial<br>            intensity gradient. This is the main control for preventing<br>            ellipses to grow to regions of too low signal-to-noise<br>            ratio.  It specifies the maximum acceptable relative error<br>            in the local radial intensity gradient.  `Busko (1996; ASPC<br>            101, 139)<br>            <http://adsabs.harvard.edu/abs/1996ASPC..101..139B`_ showed<br>            that the fitting precision relates to that relative error.<br>            The usual behavior of the gradient relative error is to<br>            increase with semimajor axis, being larger in outer, fainter<br>            regions of a galaxy image.  In the current implementation,<br>            the ``maxgerr`` criterion is triggered only when two<br>            consecutive isophotes exceed the value specified by the<br>            parameter. This prevents premature stopping caused by<br>            contamination such as stars and HII regions.<br><br>            A number of actions may happen when the gradient error<br>            exceeds ``maxgerr`` (or becomes non-significant and is set<br>            to `None`).  If the maximum semimajor axis specified by<br>            ``maxsma`` is set to `None`, semimajor axis growth is<br>            stopped and the algorithm proceeds inwards to the galaxy<br>            center. If ``maxsma`` is set to some finite value, and this<br>            value is larger than the current semimajor axis length, the<br>            algorithm enters non-iterative mode and proceeds outwards<br>            until reaching ``maxsma``.  The default is 0.5.<br>        sclip : float, optional<br>            The sigma-clip sigma value.  The default is 3.0.<br>        nclip : int, optional<br>            The number of sigma-clip interations.  The default is 0,<br>            which means sigma-clipping is skipped.<br>        integrmode : {'bilinear', 'nearest_neighbor', 'mean', 'median'}, optional<br>            The area integration mode.  The default is 'bilinear'.<br>        linear : bool, optional<br>            The semimajor axis growing/shrinking mode. If `False`<br>            (default), the geometric growing mode is chosen, thus the<br>            semimajor axis length is increased by a factor of (1. +<br>            ``step``), and the process is repeated until either the<br>            semimajor axis value reaches the value of parameter<br>            ``maxsma``, or the last fitted ellipse has more than a given<br>            fraction of its sampled points flagged out (see ``fflag``).<br>            The process then resumes from the first fitted ellipse (at<br>            ``sma0``) inwards, in steps of (1./(1. + ``step``)), until<br>            the semimajor axis length reaches the value ``minsma``. In<br>            case of linear growing, the increment or decrement value is<br>            given directly by ``step`` in pixels.  If ``maxsma`` is set<br>            to `None`, the semimajor axis will grow until a low<br>            signal-to-noise criterion is met. See ``maxgerr``.<br>        maxrit : float or `None`, optional<br>            The maximum value of semimajor axis to perform an actual<br>            fit.  Whenever the current semimajor axis length is larger<br>            than ``maxrit``, the isophotes will be extracted using the<br>            current geometry, without being fitted.  This non-iterative<br>            mode may be useful for sampling regions of very low surface<br>            brightness, where the algorithm may become unstable and<br>            unable to recover reliable geometry information.<br>            Non-iterative mode can also be entered automatically<br>            whenever the ellipticity exceeds 1.0 or the ellipse center<br>            crosses the image boundaries.  If `None` (default), then no<br>            maximum value is used.<br><br>        Returns<br>        -------<br>        result : `~photutils.isophote.IsophoteList` instance<br>            A list-like object of `~photutils.isophote.Isophote`<br>            instances, sorted by increasing semimajor axis length.<br>        """<br><br>        # multiple fitted isophotes will be stored here<br>        isophote_list = []<br><br>        # get starting sma from appropriate source: keyword parameter,<br>        # internal EllipseGeometry instance, or fixed default value.<br>        if not sma0:<br>            if self._geometry:<br>                sma = self._geometry.sma<br>            else:<br>                sma = 10.<br>        else:<br>            sma = sma0<br><br>        # first, go from initial sma outwards until<br>        # hitting one of several stopping criteria.<br>        noiter = False<br>        first_isophote = True<br>        while True:<br>            # first isophote runs longer<br>            minit_a = 2  minit if first_isophote else minit<br>            first_isophote = False<br><br>            isophote = self.fit_isophote(sma, step, conver, minit_a, maxit,<br>                                         fflag, maxgerr, sclip, nclip,<br>                                         integrmode, linear, maxrit,<br>                                         noniterate=noiter,<br>                                         isophote_list=isophote_list)<br><br>            # check for failed fit.<br>            if (isophote.stop_code < 0 or isophote.stop_code == 1):<br>                # in case the fit failed right at the outset, return an<br>                # empty list. This is the usual case when the user<br>                # provides initial guesses that are too way off to enable<br>                # the fitting algorithm to find any meaningful solution.<br><br>                if len(isophote_list) == 1:<br>                    warnings.warn('No meaningful fit was possible.',<br>                                  AstropyUserWarning)<br>                    return IsophoteList([])<br><br>                self._fix_last_isophote(isophote_list, -1)<br><br>                # get last isophote from the actual list, since the last<br>                # `isophote` instance in this context may no longer be OK.<br>                isophote = isophote_list[-1]<br><br>                # if two consecutive isophotes failed to fit,<br>                # shut off iterative mode. Or, bail out and<br>                # change to go inwards.<br>                if len(isophote_list)  2:<br>                    if ((isophote.stop_code == 5 and<br>                         isophote_list[-2].stop_code == 5)<br>                            or isophote.stop_code == 1):<br>                        if maxsma and maxsma  isophote.sma:<br>                            # if a maximum sma value was provided by<br>                            # user, and the current sma is smaller than<br>                            # maxsma, keep growing sma in non-iterative<br>                            # mode until reaching it.<br>                            noiter = True<br>                        else:<br>                            # if no maximum sma, stop growing and change<br>                            # to go inwards.<br>                            break<br><br>            # reset variable from the actual list, since the last<br>            # `isophote` instance may no longer be OK.<br>            isophote = isophote_list[-1]<br><br>            # update sma. If exceeded user-defined<br>            # maximum, bail out from this loop.<br>            sma = isophote.sample.geometry.update_sma(step)<br>            if maxsma and sma = maxsma:<br>                break<br><br>        # reset sma so as to go inwards.<br>        first_isophote = isophote_list[0]<br>        sma, step = first_isophote.sample.geometry.reset_sma(step)<br><br>        # now, go from initial sma inwards towards center.<br>        while True:<br>            isophote = self.fit_isophote(sma, step, conver, minit, maxit,<br>                                         fflag, maxgerr, sclip, nclip,<br>                                         integrmode, linear, maxrit,<br>                                         going_inwards=True,<br>                                         isophote_list=isophote_list)<br><br>            # if abnormal condition, fix isophote but keep going.<br>            if isophote.stop_code < 0:<br>                self._fix_last_isophote(isophote_list, 0)<br><br>            # reset variable from the actual list, since the last<br>            # `isophote` instance may no longer be OK.<br>            isophote = isophote_list[-1]<br><br>            # figure out next sma; if exceeded user-defined<br>            # minimum, or too small, bail out from this loop<br>            sma = isophote.sample.geometry.update_sma(step)<br>            if sma <= max(minsma, 0.5):<br>                break<br><br>        # if user asked for minsma=0, extract special isophote there<br>        if minsma == 0.0:<br>            isophote = self.fit_isophote(0.0, isophote_list=isophote_list)<br><br>        # sort list of isophotes according to sma<br>        isophote_list.sort()<br><br>        return IsophoteList(isophote_list)

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_5201.txt
> def basic_params1():<br>  """A set of basic hyperparameters."""<br>  return hparam.HParams(<br>      # If the problem consists of variable-length sequences<br>      # (see problem.batch_size_means_tokens()), then this is the number<br>      # of tokens per batch per GPU or per TPU core.  Otherwise, this is<br>      # the number of examples per GPU or per TPU core.<br>      batch_size=4096,<br>      batch_shuffle_size=512,<br>      # If True, then if the features are of variable length, the batch_size is<br>      # used as the actual batch size (and not tokens per batch).<br>      use_fixed_batch_size=False,<br>      num_hidden_layers=4,<br>      kernel_height=3,<br>      kernel_width=1,<br>      hidden_size=64,<br>      compress_steps=0,<br>      # All hyperparameters ending in "dropout" are automatically set to 0.0<br>      # when not in training mode.<br>      dropout=0.2,<br>      clip_grad_norm=2.0,<br>      grad_noise_scale=0.0,<br>      summarize_grads=False,<br>      # Flag for whether mlperf mode is on<br>      mlperf_mode=False,<br>      # Whether to log the name and size of every variable<br>      summarize_vars=False,<br>      initializer="orthogonal",<br>      initializer_gain=1.5,<br>      label_smoothing=0.1,<br>      optimizer="adam",<br>      optimizer_adam_epsilon=1e-6,<br>      optimizer_adam_beta1=0.85,<br>      optimizer_adam_beta2=0.997,<br>      optimizer_momentum_momentum=0.9,<br>      optimizer_momentum_nesterov=False,<br>      optimizer_adafactor_beta1=0.0,<br>      optimizer_adafactor_beta2=0.999,<br>      optimizer_adafactor_factored=True,<br>      optimizer_adafactor_decay_type="pow",<br>      optimizer_adafactor_memory_exponent=0.8,<br>      optimizer_adafactor_clipping_threshold=1.0,<br>      optimizer_adafactor_multiply_by_parameter_scale=True,<br>      # Number of accumulating steps for multi step optimizers.<br>      optimizer_multistep_accumulate_steps=0,<br>      # Loss scaling used.<br>      # Generally only necessary with mixed precision training.<br>      # Mixed precision training only supports exponential scaling currently<br>      # To disable the scaler, see to 0/False<br>      mixed_precision_optimizer_loss_scaler="exponential",<br>      # Determines the initial loss scaling value for mixed precision<br>      mixed_precision_optimizer_init_loss_scale=215,<br>      # Whether to zero gradients that were not computed, so that the<br>      # appropriate slots are created. Useful for sharing checkpoints between<br>      # models with different sets of heads.<br>      optimizer_zero_grads=False,<br>      weight_decay=1e-6,<br>      weight_noise=0.0,<br>      # Defines the learning rate as a product of named functions.<br>      # Available functions are listed in learning_rate._LEARNING_RATE_FUNCTIONS<br>      # e.g. "constantlinear_warmuprsqrt_decayrsqrt_hidden_size"<br>      learning_rate_schedule="legacy",<br>      learning_rate_constant=1.0,<br>      # If learning_rate_schedule=="legacy",<br>      # then we specify decay scheme here.  Warmup is always exponential,<br>      # except with "noam" learning rate decay scheme.<br>      # see optimize.legacy_learning_rate_schedule()<br>      # TODO(noam): migrate everyone away from this.<br>      learning_rate_decay_scheme="none",<br>      # decay_steps and decay_staircase for learning_rate_decay_scheme=="exp"<br>      learning_rate_decay_steps=5000,<br>      learning_rate_decay_staircase=False,<br>      learning_rate_minimum=None,<br>      learning_rate_decay_rate=1.0,<br>      learning_rate_warmup_steps=100,<br>      learning_rate_cosine_cycle_steps=250000,<br>      learning_rate=0.1,<br>      sampling_method="argmax",  # "argmax" or "random"<br>      sampling_temp=1.0,  # temperature for sampling<br>      sampling_keep_top_k=-1,  # If 0, ignore all but the top k logits<br>      # expand the logits a piece at a time - saves memory.<br>      factored_logits=False,<br>      multiply_embedding_mode="sqrt_depth",<br>      # Parameters related to mixtures of experts.<br>      moe_hidden_sizes="2048",  # hidden layer sizes (comma-separated)<br>      moe_num_experts=64,  # number of experts per layer<br>      moe_k=2,  # how many experts to use for each batch element<br>      moe_loss_coef=1e-2,<br>      # Sequences of operations to perform on layer input and layer output.<br>      # Used by common_layers.layer_preprocess, common_layers.layer_postprocess<br>      # Each character represents an operation:<br>      # none: no preprocessing<br>      #    d: apply dropout<br>      #    n: apply normalization (see norm_type and norm_epsilon)<br>      #    a: add layer input (residual connection - only during postprocess)<br>      # The special string "none" is used instead of the empty string<br>      # to indicate no pre/postprocessing, since the empty string causes<br>      # trouble for hyperparameter tuning.<br>      # TODO(noam): The current settings ("", "dan") are the published version<br>      # of the transformer.  ("n", "da") seems better for harder-to-learn<br>      # models, so it should probably be the default.<br>      layer_preprocess_sequence="none",<br>      layer_postprocess_sequence="dan",<br>      # dropout rate to use during layer_preprocess and layer_postprocess<br>      layer_prepostprocess_dropout=0.1,<br>      # broadcast dimensions for layer_prepostprocess_dropout<br>      # a comma-separated list of integers.<br>      # see common_layers.dropout_with_broadcast_dims()<br>      # Change this to "1" to save memory.<br>      layer_prepostprocess_dropout_broadcast_dims="",<br>      # dropout some symbols (set them to 0) before embedding.<br>      symbol_dropout=0.0,<br>      # What type of normalization to use<br>      norm_type="layer",  # "batch", layer", "noam", "none".<br>      # epsilon parameter to normalization function<br>      norm_epsilon=1e-6,<br>      # pad vocabularies so that this value divides the vocabulary size.<br>      vocab_divisor=1,<br>      # During training, we drop sequences whose inputs and targets are shorter<br>      # than min_length<br>      min_length=0,<br>      # During training, we drop sequences whose inputs or targets are longer<br>      # than max_length.<br>      # If max_length==0, we use hparams.batch_size instead.<br>      max_length=0,<br>      # Pack examples on the fly.<br>      pack_dataset=False,<br>      # Use custom ops not included in standard tensorflow.<br>      use_custom_ops=True,<br>      # Split targets on the first axis into chunks of this length.<br>      split_targets_chunk_length=0,<br>      split_targets_max_chunks=100,<br>      split_targets_strided_training=False,<br>      # Maximum length in the smallest length bucket.  Setting this<br>      # flag too high will result in wasteful padding of short<br>      # sequences.  Due to some (hopefully) temporary hacks in the<br>      # data reading and batching code, setting this flag too low<br>      # results in a very long batch-shuffling queue.<br>      # TODO(noam): change this once the Datasets API changes.<br>      min_length_bucket=8,<br>      # This flag controls the number of length buckets in the data<br>      # reader.  The buckets have maximum lengths from<br>      # min_bucket_length to (max_length or batch_size), increasing<br>      # (approximately) by factors of length_bucket_step.<br>      length_bucket_step=1.1,<br>      # If set to True, drop sequences longer than max_length during eval.<br>      # This affects the validity of the evaluation metrics.<br>      eval_drop_long_sequences=False,<br>      # If True, run the model autoregressively instead of teacher-forcing<br>      # during eval<br>      eval_run_autoregressive=False,<br>      # (For features with symbol modality) If True, share all of the<br>      # input embeddings, target embeddings, and softmax weights.<br>      shared_embedding_and_softmax_weights=False,<br>      # (For features with symbol modality) If True, share the input embeddings<br>      # and target embeddings.<br>      shared_embedding=False,<br>      # (For features with symbol modality) Number to shard embeddings by.<br>      symbol_modality_num_shards=1,<br>      # Feature transformations are optional dictionaries comprising key-value<br>      # pairs of a feature name (str) and its transformation (function). If not<br>      # specified, T2TModel applies a default transformation according to the<br>      # feature's modality. Bottom is applicable to all features; loss, top, and<br>      # weights_fn are only applicable to target features.<br>      # TODO(trandustin): `name` is an optional hparam for legacy reasons,<br>      # defining variable scope names. Remove this hparam in the future.<br>      bottom={},<br>      loss={},<br>      name={},<br>      top={},<br>      weights_fn={},<br>      # The maximum length of "input" sequence.<br>      # Sequences longer than this value will be truncated. 0 or negative values<br>      # mean there is no maximum or truncation.<br>      # You can change this behavior by overriding preprocess_example() method<br>      # in your problem class.<br>      max_input_seq_length=0,<br>      # The maximum length of "target" sequence.<br>      # Sequences longer than this value will be truncated. 0 or negative values<br>      # mean there is no maximum or truncation.<br>      # You can change this behavior by overriding preprocess_example() method<br>      # in your problem class.<br>      max_target_seq_length=0,<br>      # if nonzero, we split the target sequences on example read.<br>      # This is for use with language modeling problems with fixed length<br>      # examples.  e.g.  The examples may be written with length 65536, but we<br>      # want to split each example into 64 examples of length 1024.<br>      split_to_length=0,<br>      # Video settings: how many frames to batch on input and targets.<br>      video_num_input_frames=1,<br>      video_num_target_frames=1,<br>      # This flag allows us to optionally treat a seq-to-seq problem<br>      # as a language model.  Legal values are:<br>      #<br>      # "none" - Do not prepend the inputs to the targets.<br>      # "prepend_inputs_masked_attention"<br>      #     replace "targets" in preprocessing with<br>      #     tf.concat([inputs, [0], targets], axis=1)<br>      #     i.e. we prepend the inputs to the targets with a single<br>      #     padding token in between.  Use masked self-attention on the<br>      #     entire resulting sequence.  During training, we compute losses on<br>      #     the combined sequence.  During eval, we compute the metrics<br>      #     on only the targets portion.<br>      # "prepend_inputs_full_attention"<br>      #     similar to the previous option except that each<br>      #     position in the inputs portion can see the<br>      #     entire inputs portion.  This removes the challenge of<br>      #     autoregressively predicting the inputs portion.<br>      prepend_mode="none",<br>      # Scheduled sampling is interesting for auto-regressive models.<br>      # It runs an additional step using the generated output as autoregressive<br>      # targets, which can improve the models inference results later. The<br>      # parameter scheduled_sampling_prob determines with what probability<br>      # will such additional step be run. It's turned off (0.0) by default.<br>      # This probability will exponentially warm up for the number of<br>      # steps determined by scheduled_sampling_warmup_steps.<br>      # The tensor used for the n-th pass will consist of outputs from<br>      # the (n-1)-th pass mixed with gold truth, with the proportion of gold<br>      # determined by scheduled_sampling_gold_mixin_prob. Control the number<br>      # of passes with scheduled_sampling_num_passes.<br>      scheduled_sampling_prob=0.0,<br>      scheduled_sampling_warmup_steps=50000,<br>      scheduled_sampling_gold_mixin_prob=0.5,<br>      # TODO(duckworthd): Uncomment when we can ascertain why adding an<br>      # extra field to HParam causes test failures.<br>      # scheduled_sampling_num_passes=1,<br><br>      # This setting controls whether to copy variables around in a daisy chain<br>      # (if true) or leave their placement to TensorFlow. It only affects multi<br>      # device training and mostly should be turned on for performance. One<br>      # exception are recurrent models: with dynamic loops it must be off.<br>      daisy_chain_variables=True,<br>      # If True in PREDICT mode, then last-position-only optimizations are not<br>      # used.<br>      force_full_predict=False,<br>      # Set this for pure model parallelism.  There is only one data shard.<br>      no_data_parallelism=False,<br>      # dtype used for activations. - "float32" or "bfloat16"<br>      # activation_dtype="bfloat16" currently only works on TPU.<br>      #    It lowers activation-memory usage<br>      #    and does not appear to affect quality.<br>      #    You can train on TPU with activation_dtype="bfloat16" and evaluate<br>      #    on CPU/GPU with activation_dtype="float32"<br>      activation_dtype="float32",<br>      # dtype used for parameters: "float32" or "bfloat16"<br>      # bfloat16 currently only works with optimizer="adafactor".<br>      #   The savings in memory allow for training larger models.<br>      #   Weights are encoded as (w128)^8, using pseudostochastic<br>      #   roundoff.  Initial experiments show that model quality is similar<br>      #   to baseline for about 3M training steps, but worse thereafter.<br>      weight_dtype="float32",<br>      # Directory containing a checkpoint for a pretrained model. This will only<br>      # be used if a new run is being started. Parameters not found in the<br>      # pretrained model will be randomly initialized. Superfluous parameters in<br>      # the pretrained model will be ignored.<br>      pretrained_model_dir="",<br>      # Threshold used for two cases: the primary task probability for the<br>      # constant mixing schedule, and the exponential schedule limit for when<br>      # mixing should stop (eg: 0.5 means stop at 50-50 mixing, 0.8 means stop<br>      # at 20-80 mixing for the primary-others mixing case.)<br>      multiproblem_schedule_threshold=0.5,<br>      # For more than 2 tasks, we may want to specify per-task thresholds here.<br>      # In that case, this needs to be a string with as many floating point<br>      # numbers as the number of tasks in the multi-problem. These numbers<br>      # are later normalized to add up to 1 and taken as probabilities for<br>      # each task. This enforces a constant mixing schedule and if this is<br>      # empty then the threshold from above is used for the first task and<br>      # the other tasks get the remaining probability split uniformly.<br>      multiproblem_per_task_threshold="",<br>      # The number of examples at which the proportion of the mixed in datasets<br>      # is multiproblem_schedule_threshold<br>      multiproblem_schedule_max_examples=1e7,<br>      # When training multiproblems, we can mix the data according to different<br>      # schedules. Example: a constant schedule mixing 20-80 between the primary<br>      # and other tasks.<br>      # A list of supported schedules can be found in<br>      # `data_generators.multi_problem.py`.<br>      multiproblem_mixing_schedule="constant",<br>      # A boolean that decides whether input sequence losses and target label<br>      # losses in classification problems should be reweighted.<br>      multiproblem_reweight_label_loss=False,<br>      # How much weight the targets in classification problems receive. Inputs<br>      # receive 1 minus this weight.<br>      multiproblem_label_weight=0.5,<br>      # Hyperparameters for relative attention.<br>      # The maximum relative positional distance to learn an embedding for.<br>      max_relative_position=0,<br>      # If heads share the same relative embedding.<br>      heads_share_relative_embedding=False,<br>      # If relative embedding terms are added to values too.<br>      add_relative_to_values=False,<br>      # If enable the host_call which is executed every training step.<br>      # There could be a performance drop if host_call function is slow and<br>      # cannot keep up with the TPU-side computation.<br>      tpu_enable_host_call=False,<br>      # Pad batch dim of inputs to nearest multiple of batch multiple.<br>      pad_batch=False,<br>      # When true, do not evaluate on the language model data when running the<br>      # multiproblem since it can take a while. If False, set eval_steps to<br>      # something large like 6000 or 10000.<br>      multiproblem_target_eval_only=False,<br>      # Max out the vocab size to a power of 2 for efficiency and to reserve<br>      # extra space in the vocabulary for new task ids and label classes.<br>      multiproblem_vocab_size=-1,<br>      # When using multiproblem with generation tasks, need to truncate the<br>      # inputs and targets manually before concatenating them.<br>      multiproblem_max_input_length=-1,<br>      multiproblem_max_target_length=-1,<br>      # If positive, makes training targets fixed-length in MultiProblem.<br>      multiproblem_fixed_train_length=-1,<br>      # Load weights from a second model. For instance, when using<br>      # pre-trained weights, you might want to initialize the encoder<br>      # and decoder by loading different models.<br>      warm_start_from_second="",<br>      # Area attention hyper parameters<br>      area_value_mode="none",<br>      area_key_mode="none",<br>      # Using area attention for the number of layers from the bottom<br>      num_area_layers=0,<br>      max_area_width=1,<br>      max_area_height=1,<br>      memory_height=1<br>  )

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_89416.txt
> def preprocess(lat,lon,sla,<br>               N_min=None,<br>               per_min=15.0,<br>               max_gap=None,<br>               leave_gaps=False,<br>               remove_edges=True,<br>               interp_over_continents=False,<br>               truncate_if_continents=True,<br>               discard_continental_gaps = True, #do not consider gaps on continents to discard time steps using max_gap.<br>               flag_interp=False,<br>               return_lonlat=False,<br>               return_interpolated=False,<br>               last=True,mid=None,first=None, #for get_segments<br>               verbose=1):<br>    '''   <br>    Preprocessing of the SLA data ::<br>        process positions :<br>           interpolate over gaps<br>           find continents (extend the positions over continents to get the discontinuity)<br>           find track edges<br>           find gap lengths<br><br>        clean SLA data::<br>           Remove gaps greater than maximum allowed length over which interpolate is OK.<br>           Remove time steps with not enough coverage<br>           get sub-segments of valid data of a given length<br>          <br>    :parameter lon: longitude<br>    :parameter lat: longitude<br>    :parameter sla: data<br>    <br>    :keyword N_min: Length of subsegments (cf :func:`altimetry.tools.spectrum.get_segments`)<br>    :keyword per_min: Minimum percentage of valid data to allow.<br>    :keyword max_gap: Maximum gap length to interpolate over (interpolation is done 1st, THEN long gaps are eliminated)<br>    :keyword leave_gaps: Leave gaps (equivalent to setting max_gap to number of points in track).<br>    <br>    :keyword remove_edges: discard data at track edges.<br>    :keyword truncate_if_continents: Force truncating data if a continent is found within a segment of data.<br>    :keyword last: Get segments of data sticked to the last point in track (cf :func:`altimetry.tools.spectrum.get_segments`)<br>    :keyword first: Get segments of data sticked to the first point in track (cf :func:`altimetry.tools.spectrum.get_segments`)<br>    :keyword mid: Get segments of data sticked to the middle point in track (cf :func:`altimetry.tools.spectrum.get_segments`)<br>    <br>    '''<br>    sh=sla.shape<br>    nt=sh[0]<br>    nx=sh[1]<br>    dumsla=sla.copy()<br>        <br>    #Remove profiles with less than 3 points<br>    ok=np.where(sla.mask.sum(axis=1) < (nx -3))[0]<br>    if (nt != len(ok)) : message(2, '%i time steps on %i removed: contain less than 3 valid data points' % (nt - len(ok),nt),verbose=verbose)<br>    dumsla=dumsla[ok,:]<br>    ntinit=nt<br>    nt=len(ok)<br>    <br>    # 1 - regrid tracks regularly<br>    #     get gap lengths<br>    #############################<br>    fg_dumlon=nx<br>    for i in np.arange(nt):<br>        <br>        #grid track regularly<br>        fg=~dumsla.mask[i,:]<br>        dst, dumlon, dumlat, dsla, lgaps, n, edges, inter = grid_track(lat[fg], lon[fg], dumsla[i,:][fg],remove_edges=False,backbone=[lon,lat],interp_over_continents=interp_over_continents)<br>        if isinstance(dumlon,np.ma.masked_array) : fg_dumlon_new = dumlon.mask.sum() <br>        else : fg_dumlon_new = np.isfinite(dumlon).sum()<br>        if fg_dumlon_new < fg_dumlon :<br>            fg_dumlon = fg_dumlon_new<br>            lonout = dumlon<br>            latout = dumlat<br>        <br>        #extend matrix width if track has gone over any land (ie. any points not found in the backbone)<br>        if (len(dumlon)  len(lon)) & (i == 0) :            <br>            lendiff = len(dumlon) - len(lon)<br>            print '[WARNING] : Pass goes over a land mass, changing the track size from {0} to {1}'.format(nx,nx+lendiff)<br>            nx+=lendiff<br><br>        #~ if i==0:<br>            #~ dumint=inter.reshape((1,len(dsla)))<br>            #~ dumslaout=dsla.reshape((1,len(dsla)))<br>        #~ else:<br>            #~ dumint=np.ma.array(np.concatenate([dumint.data,inter.data.reshape((1,len(dsla)))],axis=0),<br>                               #~ mask=np.concatenate([dumint.mask,inter.mask.reshape((1,len(dsla)))],axis=0))<br>            #~ dumslaout=np.ma.array(np.concatenate([dumslaout.data,dsla.data.reshape((1,len(dsla)))],axis=0),<br>                                  #~ mask=np.concatenate([dumslaout.data,dsla.data.reshape((1,len(dsla)))],axis=0))<br>        dumint=inter.reshape((1,len(dsla))) if i == 0 else np.ma.concatenate([dumint,inter.reshape((1,len(dsla)))],axis=0)<br>        dumslaout=dsla.reshape((1,len(dsla))) if i == 0 else np.ma.concatenate([dumslaout,dsla.reshape((1,len(dsla)))],axis=0)<br>    <br>        if i == 0 :<br>            gaplen = [lgaps]<br>            gapedges= [edges]<br>        else :<br>            gaplen.append(lgaps)<br>            gapedges.append(edges)<br>        ngaps = n if i == 0 else np.append(ngaps,n) <br>    dumsla=dumslaout.copy()<br>    <br>    #These points are the continental ones.<br>    dumint=dumint.astype(bool)<br>    continent=dumsla.mask & dumint<br>    flagged=dumint.astype(bool) & ~continent<br>    <br>    #check for continental gaps<br>    iscont=np.sum(continent,axis=0) == nt<br>    indcont=np.arange(nx)[iscont]<br>    <br>    #here we get the position of the gap from its egdes and intersect with the continental points<br>    # - if any point is over continent, cont_gap is set to True<br>    cont_gap = [[len(set(indcont).intersection(range(gapedges[j][0][jj],gapedges[j][1][jj])))  0 for jj in xrange(ngaps[j])] for j in xrange(nt)]<br><br>    if discard_continental_gaps :<br>        gaplen=[ np.array([g[jj] for jj in xrange(len(g)) if not cont_gap[j][jj]]) for j,g in enumerate(gaplen)]<br>    <br>    # 2 - remove/subsample using stats from previous loop and keyword arguments <br>    ###########################################################################<br>    if max_gap is not None:<br>        <br>        #Remove profiles with long gaps<br>        gapmax=np.array([np.max(g) if len(g)  0 else 0 for g in gaplen])<br>        id1 = np.where(gapmax <= max_gap)[0] if not leave_gaps else ok<br>        <br>        if len(id1) == 0 : raise Exception('[ERROR] : All gaps in current track are greater than the maximum specified gap')<br>        if (len(id1) != nt) : message(2, '%i time steps on %i removed: gaps  %i point' %(nt - len(id1), ntinit, int(max_gap)),verbose=verbose)<br>        <br>        dumsla=dumsla[id1,:]<br>        <br>        #Remove profiles with not enough coverage :<br>        # - based on points with interpolation flag from grid_track and not on continents<br>#        per=100  dumsla.mask.sum(axis=0) / np.float(nt)<br>        per = 100.  flagged[id1,:].sum(axis=1)/np.float(nx)<br>        if N_min is None :<br>#             N_min = np.round((0.01 (100- per_min))  nx)<br>            N_min = nx<br>#        if per_min is None : per_min = 100 (1 - N_min/np.float(nx))<br>        <br>        id2 = np.where( per <= per_min)[0]<br>        <br>        if len(id2) == 0 : raise Exception('[ERROR] : All time steps in current track have a percentage of invalid data  than the maximum allowed (%i)' % int(per_min))<br>        if (len(id2) != len(id1)) : message(2, '%i time steps on %i removed: exceed maximum allowed percentage of invalid data (%i)' %(len(id1) - len(id2), ntinit, int(per_min)),verbose=verbose)<br>        <br>        <br>        dumsla=dumsla[id2,:]<br>        <br>        <br>        <br>        #At this point track edges are removed<br>        dumsla,id3=get_segment(dumsla,N_min,remove_edges=remove_edges,truncate_if_continents=truncate_if_continents,last=last,mid=mid,first=first)<br>        <br>        if len(id3) == 0 : raise Exception('[ERROR] : Remaining time steps do not reach the minimum length of %i points' % int(N_min))<br>        if (len(id3) != len(id2)) : message(2, '%i time steps no reaching rhe minimum length of %i points have been removed)' %(len(id2) - len(id3), int(N_min)),verbose=verbose)<br>        <br>        res=(dumsla, ok[id1[id2[id3]]])<br>        <br>        <br>    else :<br>        res=(dumsla, ngaps, gaplen)<br>    <br>    nt=res[0].shape[0]<br>    if (nt != ntinit) : message(1, '%i time steps on %i removed by data pre-processing' %(ntinit - nt, ntinit),verbose=verbose)<br>    if return_lonlat : res+=(lonout, latout)<br>    if return_interpolated : res += (dumint,)<br>    return res

### DOC[5] (IRRELEVANT) leetcode/leetcode_1884.txt
> def min_operations_to_alternate(s: str) - int:<br>    """You are given two identical eggs and you have access to a building with `n` floors labeled from `1` to `n`.<br><br>You know that there exists a floor `f` where `0 <= f <= n` such that any egg dropped at a floor higher than `f` will break, and any egg dropped at or below floor `f` will not break.<br><br>In each move, you may take an unbroken egg and drop it from any floor `x` (where `1 <= x <= n`). If the egg breaks, you can no longer use it. However, if the egg does not break, you may reuse it in future moves.<br><br>Return _the minimum number of moves that you need to determine with certainty what the value of_ `f` is.<br><br>Example 1:<br><br>Input: n = 2<br>Output: 2<br>Explanation: We can drop the first egg from floor 1 and the second egg from floor 2.<br>If the first egg breaks, we know that f = 0.<br>If the second egg breaks but the first egg didn't, we know that f = 1.<br>Otherwise, if both eggs survive, we know that f = 2.<br><br>Example 2:<br><br>Input: n = 100<br>Output: 14<br>Explanation: One optimal strategy is:<br>- Drop the 1st egg at floor 9. If it breaks, we know f is between 0 and 8. Drop the 2nd egg starting from floor 1 and going up one at a time to find f within 8 more drops. Total drops is 1 + 8 = 9.<br>- If the 1st egg does not break, drop the 1st egg again at floor 22. If it breaks, we know f is between 9 and 21. Drop the 2nd egg starting from floor 10 and going up one at a time to find f within 12 more drops. Total drops is 2 + 12 = 14.<br>- If the 1st egg does not break again, follow a similar process dropping the 1st egg from floors 34, 45, 55, 64, 72, 79, 85, 90, 94, 97, 99, and 100.<br>Regardless of the outcome, it takes at most 14 drops to determine f.<br><br>Constraints:<br><br>   `1 <= n <= 1000`"""<br><br>    count_a, count_b = 0, 0<br>    for i, c in enumerate(s):<br>        if i % 2 == 0:<br>            if c == '0':<br>                count_a += 1<br>            else:<br>                count_b += 1<br>        else:<br>            if c == '0':<br>                count_b += 1<br>            else:<br>                count_a += 1<br>    return min(count_a, count_b)


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_128.txt
> def longest_consecutive(nums):<br>    """Given an unsorted array of integers `nums`, return _the length of the longest consecutive elements sequence._<br><br>You must write an algorithm that runs in `O(n)` time.<br><br>Example 1:<br><br>Input: nums = \[100,4,200,1,3,2\]<br>Output: 4<br>Explanation: The longest consecutive elements sequence is `[1, 2, 3, 4]`. Therefore its length is 4.<br><br>Example 2:<br><br>Input: nums = \[0,3,7,2,5,8,4,6,0,1\]<br>Output: 9<br><br>Constraints:<br><br>   `0 <= nums.length <= 105`<br>   `-109 <= nums[i] <= 109`"""<br><br>    elements = set(nums)<br>    longest_seq = 0<br><br>    for num in nums:<br>        if num - 1 not in elements:<br>            current_num = num<br>            current_seq = 1<br>            <br>            while current_num + 1 in elements:<br>                current_num += 1<br>                current_seq += 1<br><br>            longest_seq = max(longest_seq, current_seq)<br>    <br>    return longest_seq

### GROUND TRUTH 1, ranked not in top 100, leetcode/leetcode_164.txt
> def maximumGap(nums):<br>    """Given an integer array `nums`, return _the maximum difference between two successive elements in its sorted form_. If the array contains less than two elements, return `0`.<br><br>You must write an algorithm that runs in linear time and uses linear extra space.<br><br>Example 1:<br><br>Input: nums = \[3,6,9,1\]<br>Output: 3<br>Explanation: The sorted form of the array is \[1,3,6,9\], either (3,6) or (6,9) has the maximum difference 3.<br><br>Example 2:<br><br>Input: nums = \[10\]<br>Output: 0<br>Explanation: The array contains less than 2 elements, therefore return 0.<br><br>Constraints:<br><br>   `1 <= nums.length <= 105`<br>   `0 <= nums[i] <= 109`"""<br><br>    if len(nums) < 2:<br>        return 0<br><br>    min_val = min(nums)<br>    max_val = max(nums)<br>    length = len(nums)<br>    bucket_size = max(1, (max_val - min_val) // (length - 1))<br>    bucket_num = (max_val - min_val) // bucket_size + 1<br>    buckets = [[float("inf"), float("-inf")] for _ in range(bucket_num)]<br><br>    for num in nums:<br>        idx = (num - min_val) // bucket_size<br>        buckets[idx][0] = min(buckets[idx][0], num)<br>        buckets[idx][1] = max(buckets[idx][1], num)<br><br>    max_gap = 0<br>    prev_max = min_val<br>    for bucket in buckets:<br>        if bucket[0] == float("inf"):<br>            continue<br>        max_gap = max(max_gap, bucket[0] - prev_max)<br>        prev_max = bucket[1]<br>    return max_gap
