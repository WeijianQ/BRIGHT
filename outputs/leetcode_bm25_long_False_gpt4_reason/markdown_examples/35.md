# Query `35`

## Original Question

*There are several squares being dropped onto the X-axis of a 2D plane.

You are given a 2D integer array `positions` where `positions[i] = [lefti, sideLengthi]` represents the `ith` square with a side length of `sideLengthi` that is dropped with its left edge aligned with X-coordinate `lefti`.

Each square is dropped one at a time from a height above any landed squares. It then falls downward (negative Y direction) until it either lands on the top side of another square or on the X-axis. A square brushing the left/right side of another square does not count as landing on it. Once it lands, it freezes in place and cannot be moved.

After each square is dropped, you must record the height of the current tallest stack of squares.

Return _an integer array_ `ans` _where_ `ans[i]` _represents the height described above after dropping the_ `ith` _square_.

Example 1:

Input: positions = \[\[1,2\],\[2,3\],\[6,1\]\]
Output: \[2,5,5\]
Explanation:
After the first drop, the tallest stack is square 1 with a height of 2.
After the second drop, the tallest stack is squares 1 and 2 with a height of 5.
After the third drop, the tallest stack is still squares 1 and 2 with a height of 5.
Thus, we return an answer of \[2, 5, 5\].

Example 2:

Input: positions = \[\[100,100\],\[200,100\]\]
Output: \[100,100\]
Explanation:
After the first drop, the tallest stack is square 1 with a height of 100.
After the second drop, the tallest stack is either square 1 or square 2, both with heights of 100.
Thus, we return an answer of \[100, 100\].
Note that square 2 only brushes the right side of square 1, which does not count as landing on it.

Constraints:

   `1 <= positions.length <= 1000`
   `1 <= lefti <= 108`
   `1 <= sideLengthi <= 106`*

## gpt4 Reasoning

```markdown
The essential problem here is to simulate the dropping and stacking of squares onto an X-axis, where each square can either land on the X-axis itself or on top of previously dropped squares, and then determine the height of the tallest stack of squares after each drop. The process involves tracking the positions and heights of squares as they accumulate on the X-axis. Let's break down the steps and considerations to solve this problem:

1. **Initial Setup:** We need a way to track the highest point above the X-axis that any part of a square reaches at any given time. This can be done using a data structure, like a map or array, that records the maximum height at various points along the X-axis. Given the potential range of X-coordinates (up to 10^8), a direct mapping of every point on the X-axis to its height could be inefficient. An interval-based approach might be more practical, where we only store the starting point of each square and its height.

2. **Processing Each Square:** For each square in the `positions` array, we perform the following steps:
   - **Find the Base Height:** Determine the highest point on which the square will land. This involves checking the current height at the square's left edge and the heights at all points covered by the square's width. The square's base height is the maximum height among these points, as it must land on the tallest stack underneath its span.
   - **Update Heights:** Once the base height is determined, update the heights for the X-axis span covered by the square. This involves setting the height of the square's footprint (from its left edge to its right edge) to its base height plus its side length.
   - **Record Tallest Stack:** After updating the heights, find the maximum height across the entire X-axis (or at least across the updated interval) to determine the current tallest stack. This height is recorded as part of the output for this square's drop.

3. **Output Generation:** Maintain an array to store the height of the tallest stack after each square is dropped, as described in step 2. This array is the final output.

**Considerations:**
- **Efficiency:** Directly checking and updating every point along the X-axis covered by a square can be inefficient, especially for wide squares or a large number of squares. Optimizations, such as segment trees or interval trees, could be used to efficiently query and update heights over intervals.
- **Edge Cases:** Squares that land on the edges of other squares require careful handling to ensure they don't mistakenly "hover" above a gap. This necessitates accurately updating the base height for every point under the new square.

**Example Solution Approach:**
Given the constraints, a brute-force approach might iterate over each square, checking and updating heights directly in an array or map. However, for a more efficient solution, especially with a large number of squares or a wide X-axis range, using an interval tree or a similar data structure to manage and query heights could significantly reduce the time complexity.

This detailed step-by-step approach aims to provide clarity on how to address the problem, highlighting the importance of efficiently managing and querying the heights of squares on the X-axis to track the tallest stack after each drop.
```

### Metrics

```
recip_rank: 0.0038
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/leetcode_1840.txt
> from collections import defaultdict<br>    """You want to build `n` new buildings in a city. The new buildings will be built in a line and are labeled from `1` to `n`.<br><br>However, there are city restrictions on the heights of the new buildings:<br><br>   The height of each building must be a non-negative integer.<br>   The height of the first building must be `0`.<br>   The height difference between any two adjacent buildings cannot exceed `1`.<br><br>Additionally, there are city restrictions on the maximum height of specific buildings. These restrictions are given as a 2D integer array `restrictions` where `restrictions[i] = [idi, maxHeighti]` indicates that building `idi` must have a height less than or equal to `maxHeighti`.<br><br>It is guaranteed that each building will appear at most once in `restrictions`, and building `1` will not be in `restrictions`.<br><br>Return _the maximum possible height of the tallest building_.<br><br>Example 1:<br><br>Input: n = 5, restrictions = \[\[2,1\],\[4,1\]\]<br>Output: 2<br>Explanation: The green area in the image indicates the maximum allowed height for each building.<br>We can build the buildings with heights \[0,1,2,1,2\], and the tallest building has a height of 2.<br><br>Example 2:<br><br>Input: n = 6, restrictions = \[\]<br>Output: 5<br>Explanation: The green area in the image indicates the maximum allowed height for each building.<br>We can build the buildings with heights \[0,1,2,3,4,5\], and the tallest building has a height of 5.<br><br>Example 3:<br><br>Input: n = 10, restrictions = \[\[5,3\],\[2,5\],\[7,4\],\[10,3\]\]<br>Output: 5<br>Explanation: The green area in the image indicates the maximum allowed height for each building.<br>We can build the buildings with heights \[0,1,2,3,3,4,4,5,4,3\], and the tallest building has a height of 5.<br><br>Constraints:<br><br>   `2 <= n <= 109`<br>   `0 <= restrictions.length <= min(n - 1, 105)`<br>   `2 <= idi <= n`<br>   `idi` is unique.<br>   `0 <= maxHeighti <= 109`"""<br><br><br>def dfs(node, visited, graph, group):<br>    if visited[node]:<br>        return<br>    visited[node] = 1<br>    group.append(node)<br>    for neighbor in graph[node]:<br>        dfs(neighbor, visited, graph, group)<br><br>def minimumHammingDistance(source, target, allowedSwaps):<br>    n = len(source)<br>    graph = defaultdict(list)<br>    for a, b in allowedSwaps:<br>        graph[a].append(b)<br>        graph[b].append(a)<br><br>    res = 0<br>    visited = [0]  n<br>    for i in range(n):<br>        if not visited[i]:<br>            group = []<br>            dfs(i, visited, graph, group)<br>            count = defaultdict(int)<br><br>            for node in group:<br>                count[source[node]] += 1<br>            for node in group:<br>                if count[target[node]]  0:<br>                    res += 1<br>                    count[target[node]] -= 1<br><br>    return n - res

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_144241.txt
> def _refine_downcheck(merge, aliases, min_goodness):<br>    """Prune the merge to avoid it covering up any entries which are below the<br>    merge insertion position.<br><br>    For example, in the (non-orthogonal) table::<br><br>        00001 - N S<br>        00011 - N S<br>        00100 - N S<br>        00X00 - N S<br>        XX1XX - 3 5<br><br>    Merging the first four entries would generate the new key-mask ``00XXX``<br>    which would be inserted above the entry with the key-mask ``XX1XX``.<br>    However ``00XXX`` would stop the key ``00110`` from reaching its correct<br>    route, that is ``00110`` would be covered by ``00XXX``. To avoid this one<br>    could just abandon the merge entirely, but a better solution is to attempt<br>    to reduce the merge such that it no longer covers any entries below it.<br><br>    To do this we first identify the bits that ARE ``X`` s in the merged<br>    key-mask but which are NOT ``X`` s in the entry that we're covering. For<br>    this example this is the 3rd bit. We then look to remove from the merge any<br>    entries which are either ``X`` s in this position OR have the same value as<br>    in this bit as the aliased entry. As the 4th entry in the table has an<br>    ``X`` in this position we remove it, and as the 3rd entry has a ``1`` we<br>    also remove it.  For this example we would then consider merging only the<br>    first two entries, leading to a new key-mask pair of ``000X1`` which can be<br>    safely inserted between ``00X00`` and ``XX1XX``::<br><br>        00100 - N S<br>        00X00 - N S<br>        000X1 - N S<br>        XX1XX - 3 5<br><br>    Returns<br>    -------<br>    :py:class:`~.Merge`<br>        New merge with entries possibly removed. If the goodness of the merge<br>        ever drops below `min_goodness` then an empty merge will be returned.<br>    """<br>    # Operation<br>    # ---------<br>    # While the merge is still better than `min_goodness` we determine which<br>    # entries below it in the table it covers. For each of these covered<br>    # entries we find which bits are Xs in the merged entry and are NOT Xs in<br>    # the covered entry.<br>    #<br>    # For example:<br>    #<br>    #     Merged entry:      ...0XXX1...<br>    #     Covered entry:     ...010XX...<br>    #     Bits of interest:      ^^<br>    #     Label used below:      mn<br>    #<br>    # NOTE:<br>    #   The covered entry may be of lower generality than the prospective<br>    #   merged entry if it is contained within the aliases dictionary (e.g.,<br>    #   ...010XX... may be part of<br>    #   ``aliases = {...XXXXX...: {..., ...010XX..., ...}, ...})``<br>    #<br>    # In this case there are 2 bits of interest highlighted. These are bits in<br>    # the merge entry whose value can be set (by removing entries from the<br>    # merge) to avoid covering the covered entry. Whenever we have multiple<br>    # covered entries we care only about the entries with the fewest number of<br>    # ``settable`` bits because these most constrain which entries we may<br>    # remove from the merge to avoid covering up the lower entry.<br>    #<br>    # NOTE:<br>    #    If there is only 1 ``settable`` bit then we are very constrained in<br>    #     terms of which entries must be removed from the merge to avoid<br>    #     covering a lower entry.<br>    #    If there are no ``settable`` bits then we cannot possibly avoid<br>    #     covering the lower entry - the only correct action is to return an<br>    #     empty merge.<br>    #<br>    # Assuming that there were no covered entries without any ``settable`` bits<br>    # (that is ``stringency  0``) then ``bits_and_vals`` contains pairs of<br>    # bits and boolean values which indicate which values need to be removed<br>    # from which bit positions to avoid covering up lower entries. If the<br>    # example above were the only covered entry then ``bits_and_vals`` would<br>    # contain ``(m, True)`` to indicate that all entries containing Xs or 1s in<br>    # the left-most bit of interest could be removed to avoid the covered entry<br>    # and ``(n, False)`` to indicate that all entries containing Xs or 0s in<br>    # the right-most bit of interest could be removed to avoid covering the<br>    # entry.<br>    #<br>    # NOTE:<br>    #   ``bits_and_vals`` consists of a set of options (e.g., we could remove<br>    #   all entries with Xs or 1s in bit ``m`` or we could remove all entries<br>    #   with Xs or 0s in bit ``n``, either would resolve the above covering).<br>    #<br>    # To determine which course of action to take we build a dictionary mapping<br>    # each of the pairs in ``bits_and_vals`` to the entries that would need to<br>    # be removed to "set" that bit in the merged entry. For example, we might<br>    # end up with:<br>    #<br>    #     options = {(m, True): {1, 4, 5},<br>    #                (n, False): {3, 7}}<br>    #<br>    # Indicating that we'd need to remove entries 1, 4 and 5 from the merge to<br>    # "set" the mth bit of the merged to 0 or that we'd need to remove entries<br>    # 3 and 7 to set the nth bit of the merged entry to set the nth bit to 1.<br>    #<br>    # NOTE:<br>    #   The boolean part of the pair indicates which value needs to be removed<br>    #   (True - remove all 1s and Xs; False - remove all 0s and Xs). If all<br>    #   Xs and 1s in a given bit position are removed from a merge then the<br>    #   merged entry is guaranteed to have a 0 in the bit position. Vice-versa<br>    #   removing all Xs and 0s in a given bit position from a merge will result<br>    #   in a merged entry with a 1 in that position.<br>    #<br>    # As we want to make our merges as large as possible we select the smallest<br>    # set of entries to remove from the merge from ``options``.<br>    #<br>    # The whole process is then repeated since:<br>    #    we ignored covered entries with more ``settable`` bits there may<br>    #     still be covered entries below the merged entry<br>    #    after removing entries from the merge the merged entry is of lower<br>    #     generality and is therefore nearer the top of the table so new<br>    #     entries may be have become covered<br><br>    # Set of bit positions<br>    all_bits = tuple(1 << i for i in range(32))<br><br>    # While the merge is still worth considering continue to perform the<br>    # down-check.<br>    while merge.goodness  min_goodness:<br>        covered = list(_get_covered_keys_and_masks(merge, aliases))<br><br>        # If there are no covered entries (the merge is valid) then break out<br>        # of the loop.<br>        if not covered:<br>            break<br><br>        # For each covered entry work out which bits in the key-mask pair which<br>        # are not Xs are not covered by Xs in the merge key-mask pair. Only<br>        # keep track of the entries which have the fewest bits that we could<br>        # set.<br>        most_stringent = 33  # Not at all stringent<br>        bits_and_vals = set()<br>        for key, mask in covered:<br>            # Get the bit positions where there ISN'T an X in the covered entry<br>            # but there IS an X in the merged entry.<br>            settable = mask & ~merge.mask<br><br>            # Count the number of settable bits, if this is a more stringent<br>            # constraint than the previous constraint then ensure that we<br>            # record the new stringency and store which bits we need to set to<br>            # meet the constraint.<br>            n_settable = sum(1 for bit in all_bits if bit & settable)<br>            if n_settable <= most_stringent:<br>                if n_settable < most_stringent:<br>                    most_stringent = n_settable<br>                    bits_and_vals = set()<br><br>                # Add this settable mask and the required values to the<br>                # settables list.<br>                bits_and_vals.update((bit, not (key & bit)) for bit in<br>                                     all_bits if bit & settable)<br><br>        if most_stringent == 0:<br>            # If are there any instances where we could not possibly change a<br>            # bit to avoid aliasing an entry we'll return an empty merge and<br>            # give up.<br>            merge = _Merge(merge.routing_table, set())<br>            break<br>        else:<br>            # Get the smallest number of entries to remove to modify the<br>            # resultant key-mask to avoid covering a lower entry. Prefer to<br>            # modify more significant bits of the key mask.<br>            remove = set()  # Entries to remove<br>            for bit, val in sorted(bits_and_vals, reverse=True):<br>                working_remove = set()  # Holder for working remove set<br><br>                for i in merge.entries:<br>                    entry = merge.routing_table[i]<br><br>                    if ((not entry.mask & bit) or<br>                            (bool(entry.key & bit) is (not val))):<br>                        # If the entry has an X in this position then it will<br>                        # need to be removed regardless of whether we want to<br>                        # set a 0 or a 1 in this position, likewise it will<br>                        # need to be removed if it is a 0 and we want a 1 or<br>                        # vice-versa.<br>                        working_remove.add(i)<br><br>                # If the current remove set is empty or the new remove set is<br>                # smaller update the remove set.<br>                if not remove or len(working_remove) < len(remove):<br>                    remove = working_remove<br><br>            # Remove the selected entries from the merge<br>            merge = _Merge(merge.routing_table, merge.entries - remove)<br>    else:<br>        # NOTE: If there are no covered entries, that is, if the merge is<br>        # better than min goodness AND valid this `else` clause is not reached.<br>        # Ensure than an empty merge is returned if the above loop was aborted<br>        # early with a non-empty merge.<br>        merge = _Merge(merge.routing_table, set())<br><br>    return merge

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_89416.txt
> def preprocess(lat,lon,sla,<br>               N_min=None,<br>               per_min=15.0,<br>               max_gap=None,<br>               leave_gaps=False,<br>               remove_edges=True,<br>               interp_over_continents=False,<br>               truncate_if_continents=True,<br>               discard_continental_gaps = True, #do not consider gaps on continents to discard time steps using max_gap.<br>               flag_interp=False,<br>               return_lonlat=False,<br>               return_interpolated=False,<br>               last=True,mid=None,first=None, #for get_segments<br>               verbose=1):<br>    '''   <br>    Preprocessing of the SLA data ::<br>        process positions :<br>           interpolate over gaps<br>           find continents (extend the positions over continents to get the discontinuity)<br>           find track edges<br>           find gap lengths<br><br>        clean SLA data::<br>           Remove gaps greater than maximum allowed length over which interpolate is OK.<br>           Remove time steps with not enough coverage<br>           get sub-segments of valid data of a given length<br>          <br>    :parameter lon: longitude<br>    :parameter lat: longitude<br>    :parameter sla: data<br>    <br>    :keyword N_min: Length of subsegments (cf :func:`altimetry.tools.spectrum.get_segments`)<br>    :keyword per_min: Minimum percentage of valid data to allow.<br>    :keyword max_gap: Maximum gap length to interpolate over (interpolation is done 1st, THEN long gaps are eliminated)<br>    :keyword leave_gaps: Leave gaps (equivalent to setting max_gap to number of points in track).<br>    <br>    :keyword remove_edges: discard data at track edges.<br>    :keyword truncate_if_continents: Force truncating data if a continent is found within a segment of data.<br>    :keyword last: Get segments of data sticked to the last point in track (cf :func:`altimetry.tools.spectrum.get_segments`)<br>    :keyword first: Get segments of data sticked to the first point in track (cf :func:`altimetry.tools.spectrum.get_segments`)<br>    :keyword mid: Get segments of data sticked to the middle point in track (cf :func:`altimetry.tools.spectrum.get_segments`)<br>    <br>    '''<br>    sh=sla.shape<br>    nt=sh[0]<br>    nx=sh[1]<br>    dumsla=sla.copy()<br>        <br>    #Remove profiles with less than 3 points<br>    ok=np.where(sla.mask.sum(axis=1) < (nx -3))[0]<br>    if (nt != len(ok)) : message(2, '%i time steps on %i removed: contain less than 3 valid data points' % (nt - len(ok),nt),verbose=verbose)<br>    dumsla=dumsla[ok,:]<br>    ntinit=nt<br>    nt=len(ok)<br>    <br>    # 1 - regrid tracks regularly<br>    #     get gap lengths<br>    #############################<br>    fg_dumlon=nx<br>    for i in np.arange(nt):<br>        <br>        #grid track regularly<br>        fg=~dumsla.mask[i,:]<br>        dst, dumlon, dumlat, dsla, lgaps, n, edges, inter = grid_track(lat[fg], lon[fg], dumsla[i,:][fg],remove_edges=False,backbone=[lon,lat],interp_over_continents=interp_over_continents)<br>        if isinstance(dumlon,np.ma.masked_array) : fg_dumlon_new = dumlon.mask.sum() <br>        else : fg_dumlon_new = np.isfinite(dumlon).sum()<br>        if fg_dumlon_new < fg_dumlon :<br>            fg_dumlon = fg_dumlon_new<br>            lonout = dumlon<br>            latout = dumlat<br>        <br>        #extend matrix width if track has gone over any land (ie. any points not found in the backbone)<br>        if (len(dumlon)  len(lon)) & (i == 0) :            <br>            lendiff = len(dumlon) - len(lon)<br>            print '[WARNING] : Pass goes over a land mass, changing the track size from {0} to {1}'.format(nx,nx+lendiff)<br>            nx+=lendiff<br><br>        #~ if i==0:<br>            #~ dumint=inter.reshape((1,len(dsla)))<br>            #~ dumslaout=dsla.reshape((1,len(dsla)))<br>        #~ else:<br>            #~ dumint=np.ma.array(np.concatenate([dumint.data,inter.data.reshape((1,len(dsla)))],axis=0),<br>                               #~ mask=np.concatenate([dumint.mask,inter.mask.reshape((1,len(dsla)))],axis=0))<br>            #~ dumslaout=np.ma.array(np.concatenate([dumslaout.data,dsla.data.reshape((1,len(dsla)))],axis=0),<br>                                  #~ mask=np.concatenate([dumslaout.data,dsla.data.reshape((1,len(dsla)))],axis=0))<br>        dumint=inter.reshape((1,len(dsla))) if i == 0 else np.ma.concatenate([dumint,inter.reshape((1,len(dsla)))],axis=0)<br>        dumslaout=dsla.reshape((1,len(dsla))) if i == 0 else np.ma.concatenate([dumslaout,dsla.reshape((1,len(dsla)))],axis=0)<br>    <br>        if i == 0 :<br>            gaplen = [lgaps]<br>            gapedges= [edges]<br>        else :<br>            gaplen.append(lgaps)<br>            gapedges.append(edges)<br>        ngaps = n if i == 0 else np.append(ngaps,n) <br>    dumsla=dumslaout.copy()<br>    <br>    #These points are the continental ones.<br>    dumint=dumint.astype(bool)<br>    continent=dumsla.mask & dumint<br>    flagged=dumint.astype(bool) & ~continent<br>    <br>    #check for continental gaps<br>    iscont=np.sum(continent,axis=0) == nt<br>    indcont=np.arange(nx)[iscont]<br>    <br>    #here we get the position of the gap from its egdes and intersect with the continental points<br>    # - if any point is over continent, cont_gap is set to True<br>    cont_gap = [[len(set(indcont).intersection(range(gapedges[j][0][jj],gapedges[j][1][jj])))  0 for jj in xrange(ngaps[j])] for j in xrange(nt)]<br><br>    if discard_continental_gaps :<br>        gaplen=[ np.array([g[jj] for jj in xrange(len(g)) if not cont_gap[j][jj]]) for j,g in enumerate(gaplen)]<br>    <br>    # 2 - remove/subsample using stats from previous loop and keyword arguments <br>    ###########################################################################<br>    if max_gap is not None:<br>        <br>        #Remove profiles with long gaps<br>        gapmax=np.array([np.max(g) if len(g)  0 else 0 for g in gaplen])<br>        id1 = np.where(gapmax <= max_gap)[0] if not leave_gaps else ok<br>        <br>        if len(id1) == 0 : raise Exception('[ERROR] : All gaps in current track are greater than the maximum specified gap')<br>        if (len(id1) != nt) : message(2, '%i time steps on %i removed: gaps  %i point' %(nt - len(id1), ntinit, int(max_gap)),verbose=verbose)<br>        <br>        dumsla=dumsla[id1,:]<br>        <br>        #Remove profiles with not enough coverage :<br>        # - based on points with interpolation flag from grid_track and not on continents<br>#        per=100  dumsla.mask.sum(axis=0) / np.float(nt)<br>        per = 100.  flagged[id1,:].sum(axis=1)/np.float(nx)<br>        if N_min is None :<br>#             N_min = np.round((0.01 (100- per_min))  nx)<br>            N_min = nx<br>#        if per_min is None : per_min = 100 (1 - N_min/np.float(nx))<br>        <br>        id2 = np.where( per <= per_min)[0]<br>        <br>        if len(id2) == 0 : raise Exception('[ERROR] : All time steps in current track have a percentage of invalid data  than the maximum allowed (%i)' % int(per_min))<br>        if (len(id2) != len(id1)) : message(2, '%i time steps on %i removed: exceed maximum allowed percentage of invalid data (%i)' %(len(id1) - len(id2), ntinit, int(per_min)),verbose=verbose)<br>        <br>        <br>        dumsla=dumsla[id2,:]<br>        <br>        <br>        <br>        #At this point track edges are removed<br>        dumsla,id3=get_segment(dumsla,N_min,remove_edges=remove_edges,truncate_if_continents=truncate_if_continents,last=last,mid=mid,first=first)<br>        <br>        if len(id3) == 0 : raise Exception('[ERROR] : Remaining time steps do not reach the minimum length of %i points' % int(N_min))<br>        if (len(id3) != len(id2)) : message(2, '%i time steps no reaching rhe minimum length of %i points have been removed)' %(len(id2) - len(id3), int(N_min)),verbose=verbose)<br>        <br>        res=(dumsla, ok[id1[id2[id3]]])<br>        <br>        <br>    else :<br>        res=(dumsla, ngaps, gaplen)<br>    <br>    nt=res[0].shape[0]<br>    if (nt != ntinit) : message(1, '%i time steps on %i removed by data pre-processing' %(ntinit - nt, ntinit),verbose=verbose)<br>    if return_lonlat : res+=(lonout, latout)<br>    if return_interpolated : res += (dumint,)<br>    return res

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_144450.txt
> def ner_net(source, destinations, width, height, wrap_around=False, radius=10):<br>    """Produce a shortest path tree for a given net using NER.<br><br>    This is the kernel of the NER algorithm.<br><br>    Parameters<br>    ----------<br>    source : (x, y)<br>        The coordinate of the source vertex.<br>    destinations : iterable([(x, y), ...])<br>        The coordinates of destination vertices.<br>    width : int<br>        Width of the system (nodes)<br>    height : int<br>        Height of the system (nodes)<br>    wrap_around : bool<br>        True if wrap-around links should be used, false if they should be<br>        avoided.<br>    radius : int<br>        Radius of area to search from each node. 20 is arbitrarily selected in<br>        the paper and shown to be acceptable in practice.<br><br>    Returns<br>    -------<br>    (:py:class:`~.rig.place_and_route.routing_tree.RoutingTree`,<br>     {(x,y): :py:class:`~.rig.place_and_route.routing_tree.RoutingTree`, ...})<br>        A RoutingTree is produced rooted at the source and visiting all<br>        destinations but which does not contain any vertices etc. For<br>        convenience, a dictionarry mapping from destination (x, y) coordinates<br>        to the associated RoutingTree is provided to allow the caller to insert<br>        these items.<br>    """<br>    # Map from (x, y) to RoutingTree objects<br>    route = {source: RoutingTree(source)}<br><br>    # Handle each destination, sorted by distance from the source, closest<br>    # first.<br>    for destination in sorted(destinations,<br>                              key=(lambda destination:<br>                                   shortest_mesh_path_length(<br>                                       to_xyz(source), to_xyz(destination))<br>                                   if not wrap_around else<br>                                   shortest_torus_path_length(<br>                                       to_xyz(source), to_xyz(destination),<br>                                       width, height))):<br>        # We shall attempt to find our nearest neighbouring placed node.<br>        neighbour = None<br><br>        # Try to find a nearby (within radius hops) node in the routing tree<br>        # that we can route to (falling back on just routing to the source).<br>        #<br>        # In an implementation according to the algorithm's original<br>        # specification looks for nodes at each point in a growing set of rings<br>        # of concentric hexagons. If it doesn't find any destinations this<br>        # means an awful lot of checks: 1261 for the default radius of 20.<br>        #<br>        # An alternative (but behaviourally identical) implementation scans the<br>        # list of all route nodes created so far and finds the closest node<br>        # which is < radius hops (falling back on the origin if no node is<br>        # closer than radius hops).  This implementation requires one check per<br>        # existing route node. In most routes this is probably a lot less than<br>        # 1261 since most routes will probably have at most a few hundred route<br>        # nodes by the time the last destination is being routed.<br>        #<br>        # Which implementation is best is a difficult question to answer:<br>        #  In principle nets with quite localised connections (e.g.<br>        #   nearest-neighbour or centroids traffic) may route slightly more<br>        #   quickly with the original algorithm since it may very quickly find<br>        #   a neighbour.<br>        #  In nets which connect very spaced-out destinations the second<br>        #   implementation may be quicker since in such a scenario it is<br>        #   unlikely that a neighbour will be found.<br>        #  In extremely high-fan-out nets (e.g. broadcasts), the original<br>        #   method is very likely to perform far better than the alternative<br>        #   method since most iterations will complete immediately while the<br>        #   alternative method must scan all the route vertices.<br>        # As such, it should be clear that neither method alone is 'best' and<br>        # both have degenerate performance in certain completely reasonable<br>        # styles of net. As a result, a simple heuristic is used to decide<br>        # which technique to use.<br>        #<br>        # The following micro-benchmarks are crude estimate of the<br>        # runtime-per-iteration of each approach (at least in the case of a<br>        # torus topology)::<br>        #<br>        #     $ # Original approach<br>        #     $ python -m timeit --setup 'x, y, w, h, r = 1, 2, 5, 10, \<br>        #                                     {x:None for x in range(10)}' \<br>        #                        'x += 1; y += 1; x %= w; y %= h; (x, y) in r'<br>        #     1000000 loops, best of 3: 0.207 usec per loop<br>        #     $ # Alternative approach<br>        #     $ python -m timeit --setup 'from rig.geometry import \<br>        #                                 shortest_torus_path_length' \<br>        #                        'shortest_torus_path_length( \<br>        #                             (0, 1, 2), (3, 2, 1), 10, 10)'<br>        #     1000000 loops, best of 3: 0.666 usec per loop<br>        #<br>        # From this we can approximately suggest that the alternative approach<br>        # is 3x more expensive per iteration. A very crude heuristic is to use<br>        # the original approach when the number of route nodes is more than<br>        # 1/3rd of the number of routes checked by the original method.<br>        concentric_hexagons = memoized_concentric_hexagons(radius)<br>        if len(concentric_hexagons) < len(route) / 3:<br>            # Original approach: Start looking for route nodes in a concentric<br>            # spiral pattern out from the destination node.<br>            for x, y in concentric_hexagons:<br>                x += destination[0]<br>                y += destination[1]<br>                if wrap_around:<br>                    x %= width<br>                    y %= height<br>                if (x, y) in route:<br>                    neighbour = (x, y)<br>                    break<br>        else:<br>            # Alternative approach: Scan over every route node and check to see<br>            # if any are < radius, picking the closest one if so.<br>            neighbour = None<br>            neighbour_distance = None<br>            for candidate_neighbour in route:<br>                if wrap_around:<br>                    distance = shortest_torus_path_length(<br>                        to_xyz(candidate_neighbour), to_xyz(destination),<br>                        width, height)<br>                else:<br>                    distance = shortest_mesh_path_length(<br>                        to_xyz(candidate_neighbour), to_xyz(destination))<br><br>                if distance <= radius and (neighbour is None or<br>                                           distance < neighbour_distance):<br>                    neighbour = candidate_neighbour<br>                    neighbour_distance = distance<br><br>        # Fall back on routing directly to the source if no nodes within radius<br>        # hops of the destination was found.<br>        if neighbour is None:<br>            neighbour = source<br><br>        # Find the shortest vector from the neighbour to this destination<br>        if wrap_around:<br>            vector = shortest_torus_path(to_xyz(neighbour),<br>                                         to_xyz(destination),<br>                                         width, height)<br>        else:<br>            vector = shortest_mesh_path(to_xyz(neighbour), to_xyz(destination))<br><br>        # The longest-dimension-first route may inadvertently pass through an<br>        # already connected node. If the route is allowed to pass through that<br>        # node it would create a cycle in the route which would be VeryBad(TM).<br>        # As a result, we work backward through the route and truncate it at<br>        # the first point where the route intersects with a connected node.<br>        ldf = longest_dimension_first(vector, neighbour, width, height)<br>        i = len(ldf)<br>        for direction, (x, y) in reversed(ldf):<br>            i -= 1<br>            if (x, y) in route:<br>                # We've just bumped into a node which is already part of the<br>                # route, this becomes our new neighbour and we truncate the LDF<br>                # route. (Note ldf list is truncated just after the current<br>                # position since it gives (direction, destination) pairs).<br>                neighbour = (x, y)<br>                ldf = ldf[i + 1:]<br>                break<br><br>        # Take the longest dimension first route.<br>        last_node = route[neighbour]<br>        for direction, (x, y) in ldf:<br>            this_node = RoutingTree((x, y))<br>            route[(x, y)] = this_node<br><br>            last_node.children.append((Routes(direction), this_node))<br>            last_node = this_node<br><br>    return (route[source], route)

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_141268.txt
> def find_contours(array, level,<br>                  fully_connected='low', positive_orientation='low'):<br>    """Find iso-valued contours in a 2D array for a given level value.<br><br>    Uses the "marching squares" method to compute a the iso-valued contours of<br>    the input 2D array for a particular level value. Array values are linearly<br>    interpolated to provide better precision for the output contours.<br><br>    Parameters<br>    ----------<br>    array : 2D ndarray of double<br>        Input data in which to find contours.<br>    level : float<br>        Value along which to find contours in the array.<br>    fully_connected : str, {'low', 'high'}<br>         Indicates whether array elements below the given level value are to be<br>         considered fully-connected (and hence elements above the value will<br>         only be face connected), or vice-versa. (See notes below for details.)<br>    positive_orientation : either 'low' or 'high'<br>         Indicates whether the output contours will produce positively-oriented<br>         polygons around islands of low- or high-valued elements. If 'low' then<br>         contours will wind counter- clockwise around elements below the<br>         iso-value. Alternately, this means that low-valued elements are always<br>         on the left of the contour. (See below for details.)<br><br>    Returns<br>    -------<br>    contours : list of (n,2)-ndarrays<br>        Each contour is an ndarray of shape ``(n, 2)``,<br>        consisting of n ``(row, column)`` coordinates along the contour.<br><br>    Notes<br>    -----<br>    The marching squares algorithm is a special case of the marching cubes<br>    algorithm [1]_.  A simple explanation is available here::<br><br>      http://www.essi.fr/~lingrand/MarchingCubes/algo.html<br><br>    There is a single ambiguous case in the marching squares algorithm: when<br>    a given ``2 x 2``-element square has two high-valued and two low-valued<br>    elements, each pair diagonally adjacent. (Where high- and low-valued is<br>    with respect to the contour value sought.) In this case, either the<br>    high-valued elements can be 'connected together' via a thin isthmus that<br>    separates the low-valued elements, or vice-versa. When elements are<br>    connected together across a diagonal, they are considered 'fully<br>    connected' (also known as 'face+vertex-connected' or '8-connected'). Only<br>    high-valued or low-valued elements can be fully-connected, the other set<br>    will be considered as 'face-connected' or '4-connected'. By default,<br>    low-valued elements are considered fully-connected; this can be altered<br>    with the 'fully_connected' parameter.<br><br>    Output contours are not guaranteed to be closed: contours which intersect<br>    the array edge will be left open. All other contours will be closed. (The<br>    closed-ness of a contours can be tested by checking whether the beginning<br>    point is the same as the end point.)<br><br>    Contours are oriented. By default, array values lower than the contour<br>    value are to the left of the contour and values greater than the contour<br>    value are to the right. This means that contours will wind<br>    counter-clockwise (i.e. in 'positive orientation') around islands of<br>    low-valued pixels. This behavior can be altered with the<br>    'positive_orientation' parameter.<br><br>    The order of the contours in the output list is determined by the position<br>    of the smallest ``x,y`` (in lexicographical order) coordinate in the<br>    contour.  This is a side-effect of how the input array is traversed, but<br>    can be relied upon.<br><br>    .. warning::<br><br>       Array coordinates/values are assumed to refer to the center of the<br>       array element. Take a simple example input: ``[0, 1]``. The interpolated<br>       position of 0.5 in this array is midway between the 0-element (at<br>       ``x=0``) and the 1-element (at ``x=1``), and thus would fall at<br>       ``x=0.5``.<br><br>    This means that to find reasonable contours, it is best to find contours<br>    midway between the expected "light" and "dark" values. In particular,<br>    given a binarized array, do not choose to find contours at the low or<br>    high value of the array. This will often yield degenerate contours,<br>    especially around structures that are a single array element wide. Instead<br>    choose a middle value, as above.<br><br>    References<br>    ----------<br>    .. [1] Lorensen, William and Harvey E. Cline. Marching Cubes: A High<br>           Resolution 3D Surface Construction Algorithm. Computer Graphics<br>           (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170).<br><br>    Examples<br>    --------<br>     a = np.zeros((3, 3))<br>     a[0, 0] = 1<br>     a<br>    array([[ 1.,  0.,  0.],<br>           [ 0.,  0.,  0.],<br>           [ 0.,  0.,  0.]])<br>     find_contours(a, 0.5)<br>    [array([[ 0. ,  0.5],<br>           [ 0.5,  0. ]])]<br>    """<br>    array = np.asarray(array, dtype=np.double)<br>    if array.ndim != 2:<br>        raise ValueError('Only 2D arrays are supported.')<br>    level = float(level)<br>    if (fully_connected not in _param_options or<br>            positive_orientation not in _param_options):<br>        raise ValueError('Parameters "fully_connected" and'<br>                         ' "positive_orientation" must be either "high" or'<br>                         ' "low".')<br>    point_list = _find_contours_cy.iterate_and_store(array, level,<br>                                                     fully_connected == 'high')<br>    contours = _assemble_contours(_take_2(point_list))<br>    if positive_orientation == 'high':<br>        contours = [c[::-1] for c in contours]<br>    return contours


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_218.txt
> from sortedcontainers import SortedList<br>    """A city's skyline is the outer contour of the silhouette formed by all the buildings in that city when viewed from a distance. Given the locations and heights of all the buildings, return _the skyline formed by these buildings collectively_.<br><br>The geometric information of each building is given in the array `buildings` where `buildings[i] = [lefti, righti, heighti]`:<br><br>   `lefti` is the x coordinate of the left edge of the `ith` building.<br>   `righti` is the x coordinate of the right edge of the `ith` building.<br>   `heighti` is the height of the `ith` building.<br><br>You may assume all buildings are perfect rectangles grounded on an absolutely flat surface at height `0`.<br><br>The skyline should be represented as a list of "key points " sorted by their x-coordinate in the form `[[x1,y1],[x2,y2],...]`. Each key point is the left endpoint of some horizontal segment in the skyline except the last point in the list, which always has a y-coordinate `0` and is used to mark the skyline's termination where the rightmost building ends. Any ground between the leftmost and rightmost buildings should be part of the skyline's contour.<br><br>Note: There must be no consecutive horizontal lines of equal height in the output skyline. For instance, `[...,[2 3],[4 5],[7 5],[11 5],[12 7],...]` is not acceptable; the three lines of height 5 should be merged into one in the final output as such: `[...,[2 3],[4 5],[12 7],...]`<br><br>Example 1:<br><br>Input: buildings = \[\[2,9,10\],\[3,7,15\],\[5,12,12\],\[15,20,10\],\[19,24,8\]\]<br>Output: \[\[2,10\],\[3,15\],\[7,12\],\[12,0\],\[15,10\],\[20,8\],\[24,0\]\]<br>Explanation:<br>Figure A shows the buildings of the input.<br>Figure B shows the skyline formed by those buildings. The red points in figure B represent the key points in the output list.<br><br>Example 2:<br><br>Input: buildings = \[\[0,2,3\],\[2,5,3\]\]<br>Output: \[\[0,3\],\[5,0\]\]<br><br>Constraints:<br><br>   `1 <= buildings.length <= 104`<br>   `0 <= lefti < righti <= 231 - 1`<br>   `1 <= heighti <= 231 - 1`<br>   `buildings` is sorted by `lefti` in non-decreasing order."""<br><br><br>def getSkyline(buildings):<br>    points = []<br>    for l, r, h in buildings:<br>        points.append((l, -h))<br>        points.append((r, h))<br>    points.sort()<br>    heights = SortedList([0])<br>    prev = 0<br>    skyline = []<br>    for x, h in points:<br>        if h < 0:<br>            heights.add(-h)<br>        else:<br>            heights.remove(h)<br>        cur = heights[-1]<br>        if cur != prev:<br>            skyline.append([x, cur])<br>            prev = cur<br>    return skyline
