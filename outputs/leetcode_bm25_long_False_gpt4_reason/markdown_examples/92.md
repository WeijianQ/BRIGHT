# Query `92`

## Original Question

*There are `n` tasks assigned to you. The task times are represented as an integer array `tasks` of length `n`, where the `ith` task takes `tasks[i]` hours to finish. A work session is when you work for at most `sessionTime` consecutive hours and then take a break.

You should finish the given tasks in a way that satisfies the following conditions:

   If you start a task in a work session, you must complete it in the same work session.
   You can start a new task immediately after finishing the previous one.
   You may complete the tasks in any order.

Given `tasks` and `sessionTime`, return _the minimum number of work sessions needed to finish all the tasks following the conditions above._

The tests are generated such that `sessionTime` is greater than or equal to the maximum element in `tasks[i]`.

Example 1:

Input: tasks = \[1,2,3\], sessionTime = 3
Output: 2
Explanation: You can finish the tasks in two work sessions.
- First work session: finish the first and the second tasks in 1 + 2 = 3 hours.
- Second work session: finish the third task in 3 hours.

Example 2:

Input: tasks = \[3,1,3,1,1\], sessionTime = 8
Output: 2
Explanation: You can finish the tasks in two work sessions.
- First work session: finish all the tasks except the last one in 3 + 1 + 3 + 1 = 8 hours.
- Second work session: finish the last task in 1 hour.

Example 3:

Input: tasks = \[1,2,3,4,5\], sessionTime = 15
Output: 1
Explanation: You can finish all the tasks in one work session.

Constraints:

   `n == tasks.length`
   `1 <= n <= 14`
   `1 <= tasks[i] <= 10`
   `max(tasks[i]) <= sessionTime <= 15`*

## gpt4 Reasoning

```markdown
The essential problem here is an optimization problem that falls under the category of task scheduling with constraints. Specifically, it's about minimizing the number of work sessions needed to complete all tasks, given that each task:
- Must be completed within the same work session it's started in.
- Can be done in any order.
- The total time spent in any given work session cannot exceed `sessionTime`.

To approach this problem, we can consider several strategies, taking into account the constraints and characteristics of the problem:

1. **Brute Force Approach:**
   - This would involve trying every possible permutation of tasks to see how they can fit within the minimum number of sessions. Given the constraints, this approach quickly becomes infeasible as the number of tasks grows, due to the factorial growth in permutations.

2. **Greedy Approach:**
   - One might think of a greedy strategy where we try to fill each session as much as possible before moving on to the next session. However, this approach doesn't always yield the optimal solution because filling a session completely might force us to use more sessions later.

3. **Dynamic Programming (DP):**
   - A more sophisticated approach involves using dynamic programming to break down the problem into smaller subproblems. However, the challenge lies in defining the subproblems in a way that captures the essence of the problem without leading to an exponential time complexity.

4. **Backtracking with Pruning:**
   - Given the relatively small maximum size of the problem (up to 14 tasks), a backtracking approach where we try different combinations of tasks for each session, with intelligent pruning to avoid exploring obviously suboptimal paths, could be effective. Pruning can be based on the current session time, the remaining tasks, and the number of sessions already used.

5. **Binary Search on the Number of Sessions:**
   - Since we're looking to minimize the number of sessions, another angle is to perform a binary search on the number of sessions. For each potential number of sessions, we check if it's possible to complete all tasks within those sessions using a feasible task distribution strategy. This approach also relies on a form of backtracking or DP to distribute tasks efficiently.

6. **Bitmasking for State Representation:**
   - For DP or backtracking solutions, using bitmasks to represent the state (i.e., which tasks have been completed) can significantly reduce the complexity and memory usage. This is particularly effective given the maximum number of tasks is 14, which fits well within the range of an integer bitmask.

To implement a solution based on these insights, one would likely start by choosing between a backtracking approach with intelligent pruning or a DP approach with bitmasking, both potentially combined with binary search on the number of sessions for optimization. The key is to efficiently explore the space of task distributions among sessions, respecting the sessionTime limit and seeking to minimize the session count. This involves careful consideration of how to represent states (e.g., using bitmasks), how to transition between states (e.g., adding a task to a session), and how to prune or avoid unnecessary work (e.g., recognizing when a configuration cannot possibly lead to a minimal solution).
```

### Metrics

```
recip_rank: 0.0043
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_154085.txt
> def _conc_alloc_size(self, sim_size):<br>        """<br>        Concretizes a size argument, if necessary, to something that makes sense when allocating space. Here we just<br>        maximize its potential size up to the maximum variable size specified in the libc plugin.<br><br>        TODO:<br>        Further consideration of the tradeoffs of this approach is probably warranted. SimHeapPTMalloc especially makes<br>        a lot of different concretization strategy assumptions, but this function handles one of the more important<br>        problems that any heap implementation will face: how to decide the amount of space to allocate upon request for<br>        a symbolic size. Either we do as we do here and silently constrain the amount returned to a default max value,<br>        or we could add a path constraint to the state to prevent exploration of any paths that would have legitimately<br>        occurred given a larger allocation size.<br><br>        The first approach (the silent maximum) has its benefit in that the explored state space will not be<br>        constrained. Sometimes this could work out, as when an allocation is returned that is smaller than requested but<br>        which the program doesn't end up making full use of anyways. Alternatively, this lack of fidelity could cause<br>        the program to overwrite other allocations made, since it should be able to assume the allocation is as large as<br>        it requested it be.<br><br>        The second approach (the path constraint) has its benefit in that no paths will be explored that could fail<br>        when an allocation is made too small. On the other hand, as stated above, some of these paths might not have<br>        failed anyways, and doing this causes us to lose the opportunity to explore those paths.<br><br>        Perhaps these behaviors could be parameterized in the future?<br>        """<br>        if self.state.solver.symbolic(sim_size):<br>            size = self.state.solver.max_int(sim_size)<br>            if size  self.state.libc.max_variable_size:<br>                l.warning("Allocation request of %d bytes exceeded maximum of %d bytes; allocating %d bytes",<br>                          size, self.state.libc.max_variable_size, size)<br>                size = self.state.libc.max_variable_size<br>        else:<br>            size = self.state.solver.eval(sim_size)<br>        return size

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_385093.txt
> def generate_ANSI_to_move_cursor(cur_x, cur_y, target_x, target_y):<br>    """<br>        Note that x positions are absolute (0=SOL) while y positions are relative. That is,<br>        we move the y position the relative distance between cur_y and target_y. It doesn't<br>        mean that cur_y=0 means we are on the first line of the screen. We have no way of<br>        knowing how tall the screen is, etc. at draw-time so we can't know this. <br>    """<br><br><br>    """<br>        SIZE - this code (in concert with its caller) implements what I would call "local optimizations"<br>        to try to minimize the number and size of cursor movements outputted. It does not attempt "global<br>        optimizations" which I think are rarely going to be worthwhile. See the DESIGN NOTE on global<br>        optimizations in this file for more details <br>    """        <br><br><br>    string = ""<br><br>    if cur_y < target_y:    # MOVE DOWN<br>        y_dist = target_y - cur_y<br><br>        # See if we can optimize moving x and y together<br>        if cur_x == target_x: <br>        <br>            # Need to move in y only<br>            if target_x != 0: <br>                # Already in correct x position which is NOT SOL. Just output code to move cursor <br>                # down. No special optimization is possible since \n would take us to SOL and then <br>                # we'd also need to output a move for x. <br>                return "\x1b[{0}B".format(y_dist)  # ANSI code to move down N lines<br>            else:<br>                # Already in correct x position which is SOL. Output efficient code to move down.<br>                return generate_optimized_y_move_down_x_SOL(y_dist)<br>        else:<br>        <br>            # Need to move in x and y<br>            if target_x != 0: <br>                # x move is going to be required so we'll move y efficiently and as a side<br>                # effect, x will become 0. Code below will move x to the right place<br>                string += generate_optimized_y_move_down_x_SOL(y_dist)<br>                cur_x = 0<br>            else:<br>                # Output move down that brings x to SOL. Then we're done.<br>                return generate_optimized_y_move_down_x_SOL(y_dist)<br>                <br>    elif cur_y  target_y:  # MOVE UP<br>        if target_x == 0:        <br>            # We want to move up and be at the SOL. That can be achieved with one command so we're<br>            # done and we return it. However note that some terminals may not support this so we<br>            # might have to remove this optimization or make it optional if that winds up mattering for terminals we care about.  <br>            return "\x1b[{0}F".format(cur_y - target_y)     # ANSI code to move up N lines and move x to SOL<br>        else:<br>            string += "\x1b[{0}A".format(cur_y - target_y)  # ANSI code to move up N lines <br><br>    if cur_x < target_x:    # MOVE RIGHT<br>        # SIZE - Note that when the bgcolor is specified (not None) and not overdrawing another drawing (as in an animation case)<br>        # an optimization could be performed to draw spaces rather than output cursor advances. This would use less<br>        # size when advancing less than 3 columns since the min escape sequence here is len 4. Not implementing this now<br>        # \t (tab) could also be a cheap way to move forward, but not clear we can determine how far it goes or if that would<br>        # be consistent, nor whether it is ever destructive.<br>        string += "\x1b[{0}C".format(target_x - cur_x)  # ANSI code to move cursor right N columns<br>    elif cur_x  target_x:  # MOVE LEFT<br>        # SIZE - potential optimizations: \b (backspace) could be a cheaper way to move backwards when there is only a short<br>        # way to go. However, not sure if it is ever destructive so not bothering with it now.    <br>        # If we need to move to x=0, \r could be a cheap way to get there. However not entirely clear whether some terminals<br>        # will move to next line as well, and might sometimes be destructive. Not going to research this so not doing it now. <br>        string += "\x1b[{0}D".format(cur_x - target_x)  # ANSI code to move cursor left N columns <br><br>    return string

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_226291.txt
> def find_prime_polynomials(generator=2, c_exp=8, fast_primes=False, single=False):<br>    '''Compute the list of prime polynomials for the given generator and galois field characteristic exponent.'''<br>    # fast_primes will output less results but will be significantly faster.<br>    # single will output the first prime polynomial found, so if all you want is to just find one prime polynomial to generate the LUT for Reed-Solomon to work, then just use that.<br><br>    # A prime polynomial (necessarily irreducible) is necessary to reduce the multiplications in the Galois Field, so as to avoid overflows.<br>    # Why do we need a "prime polynomial"? Can't we just reduce modulo 255 (for GF(2^8) for example)? Because we need the values to be unique.<br>    # For example: if the generator (alpha) = 2 and c_exp = 8 (GF(2^8) == GF(256)), then the generated Galois Field (0, 1, α, α^1, α^2, ..., α^(p-1)) will be galois field it becomes 0, 1, 2, 4, 8, 16, etc. However, upon reaching 128, the next value will be doubled (ie, next power of 2), which will give 256. Then we must reduce, because we have overflowed above the maximum value of 255. But, if we modulo 255, this will generate 256 == 1. Then 2, 4, 8, 16, etc. giving us a repeating pattern of numbers. This is very bad, as it's then not anymore a bijection (ie, a non-zero value doesn't have a unique index). That's why we can't just modulo 255, but we need another number above 255, which is called the prime polynomial.<br>    # Why so much hassle? Because we are using precomputed look-up tables for multiplication: instead of multiplying ab, we precompute alpha^a, alpha^b and alpha^(a+b), so that we can just use our lookup table at alpha^(a+b) and get our result. But just like in our original field we had 0,1,2,...,p-1 distinct unique values, in our "LUT" field using alpha we must have unique distinct values (we don't care that they are different from the original field as long as they are unique and distinct). That's why we need to avoid duplicated values, and to avoid duplicated values we need to use a prime irreducible polynomial.<br><br>    # Here is implemented a bruteforce approach to find all these prime polynomials, by generating every possible prime polynomials (ie, every integers between field_charac+1 and field_charac2), and then we build the whole Galois Field, and we reject the candidate prime polynomial if it duplicates even one value or if it generates a value above field_charac (ie, cause an overflow).<br>    # Note that this algorithm is slow if the field is too big (above 12), because it's an exhaustive search algorithm. There are probabilistic approaches, and almost surely prime approaches, but there is no determistic polynomial time algorithm to find irreducible monic polynomials. More info can be found at: http://people.mpi-inf.mpg.de/~csaha/lectures/lec9.pdf<br>    # Another faster algorithm may be found at Adleman, Leonard M., and Hendrik W. Lenstra. "Finding irreducible polynomials over finite fields." Proceedings of the eighteenth annual ACM symposium on Theory of computing. ACM, 1986.<br><br>    # Prepare the finite field characteristic (2^p - 1), this also represent the maximum possible value in this field<br>    root_charac = 2 # we're in GF(2)<br>    field_charac = int(root_characc_exp - 1)<br>    field_charac_next = int(root_charac(c_exp+1) - 1)<br><br>    prim_candidates = []<br>    if fast_primes:<br>        prim_candidates = rwh_primes1(field_charac_next) # generate maybe prime polynomials and check later if they really are irreducible<br>        prim_candidates = [x for x in prim_candidates if x  field_charac] # filter out too small primes<br>    else:<br>        prim_candidates = _range(field_charac+2, field_charac_next, root_charac) # try each possible prime polynomial, but skip even numbers (because divisible by 2 so necessarily not irreducible)<br><br>    # Start of the main loop<br>    correct_primes = []<br>    for prim in prim_candidates: # try potential candidates primitive irreducible polys<br>        seen = bytearray(field_charac+1) # memory variable to indicate if a value was already generated in the field (value at index x is set to 1) or not (set to 0 by default)<br>        conflict = False # flag to know if there was at least one conflict<br><br>        # Second loop, build the whole Galois Field<br>        x = GF2int(1)<br>        for i in _range(field_charac):<br>            # Compute the next value in the field (ie, the next power of alpha/generator)<br>            x = x.multiply(generator, prim, field_charac+1)<br><br>            # Rejection criterion: if the value overflowed (above field_charac) or is a duplicate of a previously generated power of alpha, then we reject this polynomial (not prime)<br>            if x  field_charac or seen[x] == 1:<br>                conflict = True<br>                break<br>            # Else we flag this value as seen (to maybe detect future duplicates), and we continue onto the next power of alpha<br>            else:<br>                seen[x] = 1<br><br>        # End of the second loop: if there's no conflict (no overflow nor duplicated value), this is a prime polynomial!<br>        if not conflict: <br>            correct_primes.append(prim)<br>            if single: return prim<br><br>    # Return the list of all prime polynomials<br>    return correct_primes

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_226519.txt
> def find_prime_polys(generator=2, c_exp=8, fast_primes=False, single=False):<br>    '''Compute the list of prime polynomials for the given generator and galois field characteristic exponent.'''<br>    # fast_primes will output less results but will be significantly faster.<br>    # single will output the first prime polynomial found, so if all you want is to just find one prime polynomial to generate the LUT for Reed-Solomon to work, then just use that.<br><br>    # A prime polynomial (necessarily irreducible) is necessary to reduce the multiplications in the Galois Field, so as to avoid overflows.<br>    # Why do we need a "prime polynomial"? Can't we just reduce modulo 255 (for GF(2^8) for example)? Because we need the values to be unique.<br>    # For example: if the generator (alpha) = 2 and c_exp = 8 (GF(2^8) == GF(256)), then the generated Galois Field (0, 1, α, α^1, α^2, ..., α^(p-1)) will be galois field it becomes 0, 1, 2, 4, 8, 16, etc. However, upon reaching 128, the next value will be doubled (ie, next power of 2), which will give 256. Then we must reduce, because we have overflowed above the maximum value of 255. But, if we modulo 255, this will generate 256 == 1. Then 2, 4, 8, 16, etc. giving us a repeating pattern of numbers. This is very bad, as it's then not anymore a bijection (ie, a non-zero value doesn't have a unique index). That's why we can't just modulo 255, but we need another number above 255, which is called the prime polynomial.<br>    # Why so much hassle? Because we are using precomputed look-up tables for multiplication: instead of multiplying ab, we precompute alpha^a, alpha^b and alpha^(a+b), so that we can just use our lookup table at alpha^(a+b) and get our result. But just like in our original field we had 0,1,2,...,p-1 distinct unique values, in our "LUT" field using alpha we must have unique distinct values (we don't care that they are different from the original field as long as they are unique and distinct). That's why we need to avoid duplicated values, and to avoid duplicated values we need to use a prime irreducible polynomial.<br><br>    # Here is implemented a bruteforce approach to find all these prime polynomials, by generating every possible prime polynomials (ie, every integers between field_charac+1 and field_charac2), and then we build the whole Galois Field, and we reject the candidate prime polynomial if it duplicates even one value or if it generates a value above field_charac (ie, cause an overflow).<br>    # Note that this algorithm is slow if the field is too big (above 12), because it's an exhaustive search algorithm. There are probabilistic approaches, and almost surely prime approaches, but there is no determistic polynomial time algorithm to find irreducible monic polynomials. More info can be found at: http://people.mpi-inf.mpg.de/~csaha/lectures/lec9.pdf<br>    # Another faster algorithm may be found at Adleman, Leonard M., and Hendrik W. Lenstra. "Finding irreducible polynomials over finite fields." Proceedings of the eighteenth annual ACM symposium on Theory of computing. ACM, 1986.<br><br>    # Prepare the finite field characteristic (2^p - 1), this also represent the maximum possible value in this field<br>    root_charac = 2 # we're in GF(2)<br>    field_charac = int(root_characc_exp - 1)<br>    field_charac_next = int(root_charac(c_exp+1) - 1)<br><br>    prim_candidates = []<br>    if fast_primes:<br>        prim_candidates = rwh_primes1(field_charac_next) # generate maybe prime polynomials and check later if they really are irreducible<br>        prim_candidates = [x for x in prim_candidates if x  field_charac] # filter out too small primes<br>    else:<br>        prim_candidates = xrange(field_charac+2, field_charac_next, root_charac) # try each possible prime polynomial, but skip even numbers (because divisible by 2 so necessarily not irreducible)<br><br>    # Start of the main loop<br>    correct_primes = []<br>    for prim in prim_candidates: # try potential candidates primitive irreducible polys<br>        seen = bytearray(field_charac+1) # memory variable to indicate if a value was already generated in the field (value at index x is set to 1) or not (set to 0 by default)<br>        conflict = False # flag to know if there was at least one conflict<br><br>        # Second loop, build the whole Galois Field<br>        x = 1<br>        for i in xrange(field_charac):<br>            # Compute the next value in the field (ie, the next power of alpha/generator)<br>            x = gf_mult_noLUT(x, generator, prim, field_charac+1)<br><br>            # Rejection criterion: if the value overflowed (above field_charac) or is a duplicate of a previously generated power of alpha, then we reject this polynomial (not prime)<br>            if x  field_charac or seen[x] == 1:<br>                conflict = True<br>                break<br>            # Else we flag this value as seen (to maybe detect future duplicates), and we continue onto the next power of alpha<br>            else:<br>                seen[x] = 1<br><br>        # End of the second loop: if there's no conflict (no overflow nor duplicated value), this is a prime polynomial!<br>        if not conflict: <br>            correct_primes.append(prim)<br>            if single: return prim<br><br>    # Return the list of all prime polynomials<br>    return correct_primes

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_6417.txt
> def take_step(self,<br>                  state: StateType,<br>                  max_actions: int = None,<br>                  allowed_actions: List[Set] = None) - List[StateType]:<br>        """<br>        The main method in the ``TransitionFunction`` API.  This function defines the computation<br>        done at each step of decoding and returns a ranked list of next states.<br><br>        The input state is `grouped`, to allow for efficient computation, but the output states<br>        should all have a ``group_size`` of 1, to make things easier on the decoding algorithm.<br>        They will get regrouped later as needed.<br><br>        Because of the way we handle grouping in the decoder states, constructing a new state is<br>        actually a relatively expensive operation.  If you know a priori that only some of the<br>        states will be needed (either because you have a set of gold action sequences, or you have<br>        a fixed beam size), passing that information into this function will keep us from<br>        constructing more states than we need, which will greatly speed up your computation.<br><br>        IMPORTANT: This method `must` returns states already sorted by their score, otherwise<br>        ``BeamSearch`` and other methods will break.  For efficiency, we do not perform an<br>        additional sort in those methods.<br><br>        ALSO IMPORTANT: When ``allowed_actions`` is given and ``max_actions`` is not, we assume you<br>        want to evaluate all possible states and do not need any sorting (e.g., this is true for<br>        maximum marginal likelihood training that does not use a beam search).  In this case, we<br>        may skip the sorting step for efficiency reasons.<br><br>        Parameters<br>        ----------<br>        state : ``State``<br>            The current state of the decoder, which we will take a step `from`.  We may be grouping<br>            together computation for several states here.  Because we can have several states for<br>            each instance in the original batch being evaluated at the same time, we use<br>            ``group_size`` for this kind of batching, and ``batch_size`` for the `original` batch<br>            in ``model.forward.``<br>        max_actions : ``int``, optional<br>            If you know that you will only need a certain number of states out of this (e.g., in a<br>            beam search), you can pass in the max number of actions that you need, and we will only<br>            construct that many states (for each `batch` instance - `not` for each `group`<br>            instance!).  This can save a whole lot of computation if you have an action space<br>            that's much larger than your beam size.<br>        allowed_actions : ``List[Set]``, optional<br>            If the ``DecoderTrainer`` has constraints on which actions need to be evaluated (e.g.,<br>            maximum marginal likelihood only needs to evaluate action sequences in a given set),<br>            you can pass those constraints here, to avoid constructing state objects unnecessarily.<br>            If there are no constraints from the trainer, passing a value of ``None`` here will<br>            allow all actions to be considered.<br><br>            This is a list because it is `batched` - every instance in the batch has a set of<br>            allowed actions.  Note that the size of this list is the ``group_size`` in the<br>            ``State``, `not` the ``batch_size`` of ``model.forward``.  The training algorithm needs<br>            to convert from the `batched` allowed action sequences that it has to a `grouped`<br>            allowed action sequence list.<br><br>        Returns<br>        -------<br>        next_states : ``List[State]``<br>            A list of next states, ordered by score.<br>        """<br>        raise NotImplementedError


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_1125.txt
> from collections import defaultdict<br>    """In a project, you have a list of required skills `req_skills`, and a list of people. The `ith` person `people[i]` contains a list of skills that the person has.<br><br>Consider a sufficient team: a set of people such that for every required skill in `req_skills`, there is at least one person in the team who has that skill. We can represent these teams by the index of each person.<br><br>   For example, `team = [0, 1, 3]` represents the people with skills `people[0]`, `people[1]`, and `people[3]`.<br><br>Return _any sufficient team of the smallest possible size, represented by the index of each person_. You may return the answer in any order.<br><br>It is guaranteed an answer exists.<br><br>Example 1:<br><br>Input: req\_skills = \["java","nodejs","reactjs"\], people = \[\["java"\],\["nodejs"\],\["nodejs","reactjs"\]\]<br>Output: \[0,2\]<br><br>Example 2:<br><br>Input: req\_skills = \["algorithms","math","java","reactjs","csharp","aws"\], people = \[\["algorithms","math","java"\],\["algorithms","math","reactjs"\],\["java","csharp","aws"\],\["reactjs","csharp"\],\["csharp","math"\],\["aws","java"\]\]<br>Output: \[1,2\]<br><br>Constraints:<br><br>   `1 <= req_skills.length <= 16`<br>   `1 <= req_skills[i].length <= 16`<br>   `req_skills[i]` consists of lowercase English letters.<br>   All the strings of `req_skills` are unique.<br>   `1 <= people.length <= 60`<br>   `0 <= people[i].length <= 16`<br>   `1 <= people[i][j].length <= 16`<br>   `people[i][j]` consists of lowercase English letters.<br>   All the strings of `people[i]` are unique.<br>   Every skill in `people[i]` is a skill in `req_skills`.<br>   It is guaranteed a sufficient team exists."""<br><br>from math import log2<br><br>def smallestSufficientTeam(req_skills, people):<br>    skill_to_id = {skill: idx for idx, skill in enumerate(req_skills)}<br><br>    n, m = len(people), len(req_skills)<br>    dp = [float("inf")]  (1 << m)<br>    pre = [0]  (1 << m)<br>    skill_set = [0]  (1 << m)<br><br>    dp[0] = 0<br><br>    for i, skills in enumerate(people):<br>        skill_mask = sum(1 << skill_to_id[skill] for skill in skills)<br>        for j in range((1 << m) - 1, -1, -1):<br>            if dp[j | skill_mask]  dp[j] + 1:<br>                dp[j | skill_mask] = dp[j] + 1<br>                pre[j | skill_mask] = j<br>                skill_set[j | skill_mask] = i<br><br>    res = []<br>    mask = (1 << m) - 1<br>    while mask:<br>        res.append(skill_set[mask])<br>        mask = pre[mask]<br><br>    return res

### GROUND TRUTH 1, ranked not in top 100, leetcode/leetcode_2305.txt
> def min_append_k_sum(nums, k):<br>    """You are given an integer array `cookies`, where `cookies[i]` denotes the number of cookies in the `ith` bag. You are also given an integer `k` that denotes the number of children to distribute all the bags of cookies to. All the cookies in the same bag must go to the same child and cannot be split up.<br><br>The unfairness of a distribution is defined as the maximum total cookies obtained by a single child in the distribution.<br><br>Return _the minimum unfairness of all distributions_.<br><br>Example 1:<br><br>Input: cookies = \[8,15,10,20,8\], k = 2<br>Output: 31<br>Explanation: One optimal distribution is \[8,15,8\] and \[10,20\]<br>- The 1st child receives \[8,15,8\] which has a total of 8 + 15 + 8 = 31 cookies.<br>- The 2nd child receives \[10,20\] which has a total of 10 + 20 = 30 cookies.<br>The unfairness of the distribution is max(31,30) = 31.<br>It can be shown that there is no distribution with an unfairness less than 31.<br><br>Example 2:<br><br>Input: cookies = \[6,1,3,2,2,4,1,2\], k = 3<br>Output: 7<br>Explanation: One optimal distribution is \[6,1\], \[3,2,2\], and \[4,1,2\]<br>- The 1st child receives \[6,1\] which has a total of 6 + 1 = 7 cookies.<br>- The 2nd child receives \[3,2,2\] which has a total of 3 + 2 + 2 = 7 cookies.<br>- The 3rd child receives \[4,1,2\] which has a total of 4 + 1 + 2 = 7 cookies.<br>The unfairness of the distribution is max(7,7,7) = 7.<br>It can be shown that there is no distribution with an unfairness less than 7.<br><br>Constraints:<br><br>   `2 <= cookies.length <= 8`<br>   `1 <= cookies[i] <= 105`<br>   `2 <= k <= cookies.length`"""<br><br>    max_elem = max(nums)<br>    return sum(range(max_elem + 1, max_elem + k + 1))

### GROUND TRUTH 2, ranked not in top 100, leetcode/leetcode_1723.txt
> def max_achievable_requests(n, requests, idx=0, counts=None):<br>    """You are given an integer array `jobs`, where `jobs[i]` is the amount of time it takes to complete the `ith` job.<br><br>There are `k` workers that you can assign jobs to. Each job should be assigned to exactly one worker. The working time of a worker is the sum of the time it takes to complete all jobs assigned to them. Your goal is to devise an optimal assignment such that the maximum working time of any worker is minimized.<br><br>_Return the minimum possible maximum working time of any assignment._<br><br>Example 1:<br><br>Input: jobs = \[3,2,3\], k = 3<br>Output: 3<br>Explanation: By assigning each person one job, the maximum time is 3.<br><br>Example 2:<br><br>Input: jobs = \[1,2,4,7,8\], k = 2<br>Output: 11<br>Explanation: Assign the jobs the following way:<br>Worker 1: 1, 2, 8 (working time = 1 + 2 + 8 = 11)<br>Worker 2: 4, 7 (working time = 4 + 7 = 11)<br>The maximum working time is 11.<br><br>Constraints:<br><br>   `1 <= k <= jobs.length <= 12`<br>   `1 <= jobs[i] <= 107`"""<br><br>    if counts is None:<br>        counts = [0]  n<br>    if idx == len(requests):<br>        if all(count == 0 for count in counts):<br>            return 0<br>        return 0<br><br>    ignored = max_achievable_requests(n, requests, idx+1, counts[:])<br>    counts[requests[idx][0]] += 1<br>    counts[requests[idx][1]] -= 1<br><br>    accepted = 1 + max_achievable_requests(n, requests, idx+1, counts)<br><br>    return max(ignored, accepted)
