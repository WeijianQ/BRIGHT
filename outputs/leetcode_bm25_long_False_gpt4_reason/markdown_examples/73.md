# Query `73`

## Original Question

*You are a hiker preparing for an upcoming hike. You are given `heights`, a 2D array of size `rows x columns`, where `heights[row][col]` represents the height of cell `(row, col)`. You are situated in the top-left cell, `(0, 0)`, and you hope to travel to the bottom-right cell, `(rows-1, columns-1)` (i.e., 0-indexed). You can move up, down, left, or right, and you wish to find a route that requires the minimum effort.

A route's effort is the maximum absolute difference in heights between two consecutive cells of the route.

Return _the minimum effort required to travel from the top-left cell to the bottom-right cell._

Example 1:

Input: heights = \[\[1,2,2\],\[3,8,2\],\[5,3,5\]\]
Output: 2
Explanation: The route of \[1,3,5,3,5\] has a maximum absolute difference of 2 in consecutive cells.
This is better than the route of \[1,2,2,2,5\], where the maximum absolute difference is 3.

Example 2:

Input: heights = \[\[1,2,3\],\[3,8,4\],\[5,3,5\]\]
Output: 1
Explanation: The route of \[1,2,3,4,5\] has a maximum absolute difference of 1 in consecutive cells, which is better than route \[1,3,5,3,5\].

Example 3:

Input: heights = \[\[1,2,1,1,1\],\[1,2,1,2,1\],\[1,2,1,2,1\],\[1,2,1,2,1\],\[1,1,1,2,1\]\]
Output: 0
Explanation: This route does not require any effort.

Constraints:

   `rows == heights.length`
   `columns == heights[i].length`
   `1 <= rows, columns <= 100`
   `1 <= heights[i][j] <= 106`*

## gpt4 Reasoning

```markdown
The essential problem here is to find the path from the top-left cell to the bottom-right cell of a 2D array (representing a terrain with varying heights) that minimizes the maximum absolute difference in heights between two consecutive cells along the path. This problem is a variant of the classic pathfinding problem, with a specific focus on minimizing the "effort" required to travel, where effort is defined by the height difference between adjacent cells.

To address this problem, consider the following steps and pieces of information:

### 1. Representation of the Problem:
- The terrain is represented as a 2D array where each element corresponds to the height of that cell.
- Movement is allowed in four directions: up, down, left, and right.
- The goal is to move from the top-left corner to the bottom-right corner.

### 2. Understanding "Effort":
- "Effort" is quantified as the maximum absolute difference in heights between two consecutive cells along a path.
- Minimizing effort means finding a path where the steepest ascent or descent is as gentle as possible.

### 3. Potential Approaches:
- **Dijkstra's Algorithm**: Traditionally used for finding the shortest path in a graph with non-negative weights. In this context, the "weight" of an edge could be considered the absolute difference in height between two cells. However, since we're interested in minimizing the maximum effort and not the total sum of efforts, modifications are necessary.
- **Binary Search on Effort**: Since we're looking for the minimum effort, one approach is to use binary search on the possible effort values (ranging from 0 to the maximum height difference in the grid). For each possible effort value, we check if there's a path from the start to the end where no step exceeds this effort. This involves a depth-first search (DFS) or breadth-first search (BFS) where only steps that do not exceed the current effort threshold are considered valid moves.
- **Union-Find**: Another method involves using a Union-Find data structure to dynamically group cells into "reachable" clusters based on the current effort threshold being considered. This is more efficient in scenarios where the grid is large, and direct pathfinding (like DFS/BFS) may be too slow.

### 4. Implementation Notes:
- For the binary search approach, the lower bound of the search is 0 (indicating no effort required, which is possible if all cells have the same height), and the upper bound is the maximum absolute difference between any two adjacent cells in the grid.
- In each iteration of the binary search, use BFS or DFS to attempt to reach the bottom-right cell from the top-left cell under the current effort threshold. If successful, try a lower effort. If not, increase the effort.
- The search ends when the lowest possible effort that allows reaching the destination is found.

### 5. Complexity Analysis:
- The time complexity of the binary search approach is O(log(maxHeightDifference) * (rows * columns)), where maxHeightDifference is the maximum absolute difference between any two adjacent cells. This accounts for the binary search across effort levels and the BFS/DFS required to check each effort level's viability.
- Space complexity is primarily O(rows * columns) due to the need to store the visited state of cells during BFS/DFS.

This problem encapsulates both the challenge of navigating through a terrain with varying elevations and the computational complexity of finding an optimal path under unique constraints. The key to solving it lies in efficiently searching through the space of possible efforts to find the minimum required to traverse the terrain.
```

### Metrics

```
recip_rank: 0.0115
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0100
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 1.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.1548
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0115
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/leetcode_2245.txt
> def can_destroy_all_asteroids(mass, asteroids):<br>    """You are given a 2D integer array `grid` of size `m x n`, where each cell contains a positive integer.<br><br>A cornered path is defined as a set of adjacent cells with at most one turn. More specifically, the path should exclusively move either horizontally or vertically up to the turn (if there is one), without returning to a previously visited cell. After the turn, the path will then move exclusively in the alternate direction: move vertically if it moved horizontally, and vice versa, also without returning to a previously visited cell.<br><br>The product of a path is defined as the product of all the values in the path.<br><br>Return _the maximum number of trailing zeros in the product of a cornered path found in_ `grid`.<br><br>Note:<br><br>   Horizontal movement means moving in either the left or right direction.<br>   Vertical movement means moving in either the up or down direction.<br><br>Example 1:<br><br>Input: grid = \[\[23,17,15,3,20\],\[8,1,20,27,11\],\[9,4,6,2,21\],\[40,9,1,10,6\],\[22,7,4,5,3\]\]<br>Output: 3<br>Explanation: The grid on the left shows a valid cornered path.<br>It has a product of 15 \ 20 \ 6 \ 1 \ 10 = 18000 which has 3 trailing zeros.<br>It can be shown that this is the maximum trailing zeros in the product of a cornered path.<br><br>The grid in the middle is not a cornered path as it has more than one turn.<br>The grid on the right is not a cornered path as it requires a return to a previously visited cell.<br><br>Example 2:<br><br>Input: grid = \[\[4,3,2\],\[7,6,1\],\[8,8,8\]\]<br>Output: 0<br>Explanation: The grid is shown in the figure above.<br>There are no cornered paths in the grid that result in a product with a trailing zero.<br><br>Constraints:<br><br>   `m == grid.length`<br>   `n == grid[i].length`<br>   `1 <= m, n <= 105`<br>   `1 <= m  n <= 105`<br>   `1 <= grid[i][j] <= 1000`"""<br><br>    asteroids.sort()<br>    for asteroid in asteroids:<br>        if mass = asteroid:<br>            mass += asteroid<br>        else:<br>            return False<br>    return True

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_240491.txt
> def get_mean_inclination_and_azimuth(self):<br>        """<br>        Calculate weighted average inclination and azimuth of the mesh surface.<br><br>        :returns:<br>            Tuple of two float numbers: inclination angle in a range [0, 90]<br>            and azimuth in range [0, 360) (in decimal degrees).<br><br>        The mesh is triangulated, the inclination and azimuth for each triangle<br>        is computed and average values weighted on each triangle's area<br>        are calculated. Azimuth is always defined in a way that inclination<br>        angle doesn't exceed 90 degree.<br>        """<br>        assert 1 not in self.lons.shape, (<br>            "inclination and azimuth are only defined for mesh of more than "<br>            "one row and more than one column of points")<br>        assert ((self.depths[1:] - self.depths[:-1]) = 0).all(), (<br>            "get_mean_inclination_and_azimuth() requires next mesh row "<br>            "to be not shallower than the previous one")<br><br>        points, along_azimuth, updip, diag = self.triangulate()<br><br>        # define planes that are perpendicular to each point's vector<br>        # as normals to those planes<br>        earth_surface_tangent_normal = geo_utils.normalized(points)<br><br>        # calculating triangles' area and normals for top-left triangles<br>        e1 = along_azimuth[:-1]<br>        e2 = updip[:, :-1]<br>        tl_area = geo_utils.triangle_area(e1, e2, diag)<br>        tl_normal = geo_utils.normalized(numpy.cross(e1, e2))<br>        # ... and bottom-right triangles<br>        e1 = along_azimuth[1:]<br>        e2 = updip[:, 1:]<br>        br_area = geo_utils.triangle_area(e1, e2, diag)<br>        br_normal = geo_utils.normalized(numpy.cross(e1, e2))<br><br>        if (self.depths == 0).all():<br>            # mesh is on earth surface, inclination is zero<br>            inclination = 0<br>        else:<br>            # inclination calculation<br>            # top-left triangles<br>            en = earth_surface_tangent_normal[:-1, :-1]<br>            # cosine of inclination of the triangle is scalar product<br>            # of vector normal to triangle plane and (normalized) vector<br>            # pointing to top left corner of a triangle from earth center<br>            incl_cos = numpy.sum(en  tl_normal, axis=-1).clip(-1.0, 1.0)<br>            # we calculate average angle using mean of circular quantities<br>            # formula: define 2d vector for each triangle where length<br>            # of the vector corresponds to triangle's weight (we use triangle<br>            # area) and angle is equal to inclination angle. then we calculate<br>            # the angle of vector sum of all those vectors and that angle<br>            # is the weighted average.<br>            xx = numpy.sum(tl_area  incl_cos)<br>            # express sine via cosine using Pythagorean trigonometric identity,<br>            # this is a bit faster than sin(arccos(incl_cos))<br>            yy = numpy.sum(tl_area  sqrt(1 - incl_cos  incl_cos))<br><br>            # bottom-right triangles<br>            en = earth_surface_tangent_normal[1:, 1:]<br>            # we need to clip scalar product values because in some cases<br>            # they might exceed range where arccos is defined ([-1, 1])<br>            # because of floating point imprecision<br>            incl_cos = numpy.sum(en  br_normal, axis=-1).clip(-1.0, 1.0)<br>            # weighted angle vectors are calculated independently for top-left<br>            # and bottom-right triangles of each cell in a mesh. here we<br>            # combine both and finally get the weighted mean angle<br>            xx += numpy.sum(br_area  incl_cos)<br>            yy += numpy.sum(br_area  sqrt(1 - incl_cos  incl_cos))<br>            inclination = numpy.degrees(numpy.arctan2(yy, xx))<br><br>        # azimuth calculation is done similar to one for inclination. we also<br>        # do separate calculations for top-left and bottom-right triangles<br>        # and also combine results using mean of circular quantities approach<br><br>        # unit vector along z axis<br>        z_unit = numpy.array([0.0, 0.0, 1.0])<br><br>        # unit vectors pointing west from each point of the mesh, they define<br>        # planes that contain meridian of respective point<br>        norms_west = geo_utils.normalized(numpy.cross(points + z_unit, points))<br>        # unit vectors parallel to planes defined by previous ones. they are<br>        # directed from each point to a point lying on z axis on the same<br>        # distance from earth center<br>        norms_north = geo_utils.normalized(numpy.cross(points, norms_west))<br>        # need to normalize triangles' azimuthal edges because we will project<br>        # them on other normals and thus calculate an angle in between<br>        along_azimuth = geo_utils.normalized(along_azimuth)<br><br>        # process top-left triangles<br>        # here we identify the sign of direction of the triangles' azimuthal<br>        # edges: is edge pointing west or east? for finding that we project<br>        # those edges to vectors directing to west by calculating scalar<br>        # product and get the sign of resulting value: if it is negative<br>        # than the resulting azimuth should be negative as top edge is pointing<br>        # west.<br>        sign = numpy.sign(numpy.sign(<br>            numpy.sum(along_azimuth[:-1]  norms_west[:-1, :-1], axis=-1))<br>            # we run numpy.sign(numpy.sign(...) + 0.1) to make resulting values<br>            # be only either -1 or 1 with zero values (when edge is pointing<br>            # strictly north or south) expressed as 1 (which means "don't<br>            # change the sign")<br>            + 0.1)<br><br>        # the length of projection of azimuthal edge on norms_north is cosine<br>        # of edge's azimuth<br>        az_cos = numpy.sum(along_azimuth[:-1]  norms_north[:-1, :-1], axis=-1)<br>        # use the same approach for finding the weighted mean<br>        # as for inclination (see above)<br>        xx = numpy.sum(tl_area  az_cos)<br>        # the only difference is that azimuth is defined in a range<br>        # [0, 360), so we need to have two reference planes and change<br>        # sign of projection on one normal to sign of projection to another one<br>        yy = numpy.sum(tl_area  sqrt(1 - az_cos  az_cos)  sign)<br>        # bottom-right triangles<br>        sign = numpy.sign(numpy.sign(<br>            numpy.sum(along_azimuth[1:]  norms_west[1:, 1:], axis=-1))<br>            + 0.1)<br>        az_cos = numpy.sum(along_azimuth[1:]  norms_north[1:, 1:], axis=-1)<br>        xx += numpy.sum(br_area  az_cos)<br>        yy += numpy.sum(br_area  sqrt(1 - az_cos  az_cos)  sign)<br><br>        azimuth = numpy.degrees(numpy.arctan2(yy, xx))<br>        if azimuth < 0:<br>            azimuth += 360<br><br>        if inclination  90:<br>            # average inclination is over 90 degree, that means that we need<br>            # to reverse azimuthal direction in order for inclination to be<br>            # in range [0, 90]<br>            inclination = 180 - inclination<br>            azimuth = (azimuth + 180) % 360<br><br>        return inclination, azimuth

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_171589.txt
> def gen3d_conformer(self):<br>        """<br>        A combined method to first generate 3D structures from 0D or 2D<br>        structures and then find the minimum energy conformer:<br>        1. Use OBBuilder to create a 3D structure using rules and ring templates<br>        2. Do 250 steps of a steepest descent geometry optimization with the<br>           MMFF94 forcefield<br>        3. Do 200 iterations of a Weighted Rotor conformational search<br>           (optimizing each conformer with 25 steps of a steepest descent)<br>        4. Do 250 steps of a conjugate gradient geometry optimization.<br><br>        Warning from openbabel docs:<br>        For many applications where 100s if not 1000s of molecules need to be<br>        processed, gen3d is rather SLOW. Sometimes this function can cause a<br>        segmentation fault.<br>        A future version of Open Babel will provide options for slow/medium/fast<br>        3D structure generation which will involve different compromises<br>        between speed and finding the global energy minimum.<br>        """<br>        gen3d = ob.OBOp.FindType("Gen3D")<br>        gen3d.Do(self._obmol)

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_144360.txt
> def place(vertices_resources, nets, machine, constraints,<br>          effort=1.0, random=default_random, on_temperature_change=None,<br>          kernel=default_kernel, kernel_kwargs={}):<br>    """A flat Simulated Annealing based placement algorithm.<br><br>    This placement algorithm uses simulated annealing directly on the supplied<br>    problem graph with the objective of reducing wire lengths (and thus,<br>    indirectly, the potential for congestion). Though computationally<br>    expensive, this placer produces relatively good placement solutions.<br><br>    The annealing temperature schedule used by this algorithm is taken from<br>    "VPR: A New Packing, Placement and Routing Tool for FPGA Research" by<br>    Vaughn Betz and Jonathan Rose from the "1997 International Workshop on<br>    Field Programmable Logic and Applications".<br><br>    Two implementations of the algorithm's kernel are available:<br><br>     :py:class:`~rig.place_and_route.place.sa.python_kernel.PythonKernel` A<br>      pure Python implementation which is available on all platforms supported<br>      by Rig.<br>     :py:class:`~rig.place_and_route.place.sa.c_kernel.CKernel` A C<br>      implementation which is typically 50-150x faster than the basic Python<br>      kernel. Since this implementation requires a C compiler during<br>      installation, it is an optional feature of Rig. See the<br>      :py:class:`CKernel's documentation<br>      <rig.place_and_route.place.sa.c_kernel.CKernel` for details.<br><br>    The fastest kernel installed is used by default and can be manually chosen<br>    using the ``kernel`` argument.<br><br>    This algorithm produces INFO level logging information describing the<br>    progress made by the algorithm.<br><br>    .. warning:<br>        This algorithm does not attempt to produce good solutions to the<br>        bin-packing problem of optimally fitting vertices into chips and it may<br>        fail if a good placement requires good bin packing.<br><br>    Parameters<br>    ----------<br>    effort : float<br>        A scaling factor for the number of iterations the algorithm should run<br>        for. 1.0 is probably about as low as you'll want to go in practice and<br>        runtime increases linearly as you increase this parameter.<br>    random : :py:class:`random.Random`<br>        A Python random number generator. Defaults to ``import random`` but can<br>        be set to your own instance of :py:class:`random.Random` to allow you<br>        to control the seed and produce deterministic results. For results to<br>        be deterministic, vertices_resources must be supplied as an<br>        :py:class:`collections.OrderedDict`.<br>    on_temperature_change : callback_function or None<br>        An (optional) callback function which is called every time the<br>        temperature is changed. This callback can be used to provide status<br>        updates<br><br>        The callback function is passed the following arguments:<br><br>         ``iteration_count``: the number of iterations the placer has<br>          attempted (integer)<br>         ``placements``: The current placement solution.<br>         ``cost``: the weighted sum over all nets of bounding-box size.<br>          (float)<br>         ``acceptance_rate``: the proportion of iterations which have resulted<br>          in an accepted change since the last callback call. (float between<br>          0.0 and 1.0)<br>         ``temperature``: The current annealing temperature. (float)<br>         ``distance_limit``: The maximum distance any swap may be made over.<br>          (integer)<br><br>        If the callback returns False, the anneal is terminated immediately and<br>        the current solution is returned.<br>    kernel : :py:class:`~rig.place_and_route.place.sa.kernel.Kernel`<br>        A simulated annealing placement kernel. A sensible default will be<br>        chosen based on the available kernels on this machine. The kernel may<br>        not be used if the placement problem has a trivial solution.<br>    kernel_kwargs : dict<br>        Optional kernel-specific keyword arguments to pass to the kernel<br>        constructor.<br>    """<br>    # Special case: just return immediately when there's nothing to place<br>    if len(vertices_resources) == 0:<br>        return {}<br><br>    # Within the algorithm we modify the resource availability values in the<br>    # machine to account for the effects of the current placement. As a result,<br>    # an internal copy of the structure must be made.<br>    machine = machine.copy()<br><br>    # {vertex: (x, y), ...} gives the location of all vertices whose position<br>    # is fixed by a LocationConstraint.<br>    fixed_vertices = {}<br><br>    # Handle constraints<br>    vertices_resources, nets, constraints, substitutions = \<br>        apply_same_chip_constraints(vertices_resources, nets, constraints)<br>    for constraint in constraints:<br>        if isinstance(constraint, LocationConstraint):<br>            # Location constraints are handled by recording the set of fixed<br>            # vertex locations and subtracting their resources from the chips<br>            # they're allocated to. These vertices will then not be added to<br>            # the internal placement data structure to prevent annealing from<br>            # moving them. They will be re-introduced at the last possible<br>            # moment.<br>            location = constraint.location<br>            if location not in machine:<br>                raise InvalidConstraintError(<br>                    "Chip requested by {} unavailable".format(machine))<br>            vertex = constraint.vertex<br><br>            # Record the constrained vertex's location<br>            fixed_vertices[vertex] = location<br><br>            # Make sure the vertex fits at the requested location (updating the<br>            # resource availability after placement)<br>            resources = vertices_resources[vertex]<br>            machine[location] = subtract_resources(machine[location],<br>                                                   resources)<br>            if overallocated(machine[location]):<br>                raise InsufficientResourceError(<br>                    "Cannot meet {}".format(constraint))<br>        elif isinstance(constraint,  # pragma: no branch<br>                        ReserveResourceConstraint):<br>            apply_reserve_resource_constraint(machine, constraint)<br><br>    # Initially randomly place the movable vertices<br>    movable_vertices = {v for v in vertices_resources<br>                        if v not in fixed_vertices}<br>    initial_placements = _initial_placement(movable_vertices,<br>                                            vertices_resources,<br>                                            machine, random)<br><br>    # Include the fixed vertices in initial placement<br>    initial_placements.update(fixed_vertices)<br><br>    # Filter out empty or singleton nets and those weighted as zero since they<br>    # cannot influence placement.<br>    nets = [n for n in nets if len(set(n))  1 and n.weight  0.0]<br><br>    # Special cases where no placement effort is required:<br>    #  There is only one chip<br>    #  There are no resource types to be consumed<br>    #  No effort is to be made<br>    #  No movable vertices<br>    #  There are no nets (and moving things has no effect)<br>    trivial = ((machine.width, machine.height) == (1, 1) or<br>               len(machine.chip_resources) == 0 or<br>               effort == 0.0 or<br>               len(movable_vertices) == 0 or<br>               len(nets) == 0)<br>    if trivial:<br>        logger.info("Placement has trivial solution. SA not used.")<br>        finalise_same_chip_constraints(substitutions, initial_placements)<br>        return initial_placements<br><br>    # Intialise the algorithm kernel<br>    k = kernel(vertices_resources, movable_vertices, set(fixed_vertices),<br>               initial_placements, nets, machine, random, kernel_kwargs)<br><br>    logger.info("SA placement kernel: %s", kernel.__name__)<br><br>    # Specifies the maximum distance any swap can span. Initially consider<br>    # swaps that span the entire machine.<br>    distance_limit = max(machine.width, machine.height)<br><br>    # Determine initial temperature according to the heuristic used by VPR: 20<br>    # times the standard deviation of len(movable_vertices) random swap costs.<br>    # The arbitrary very-high temperature is used to cause "all" swaps to be<br>    # accepted.<br>    _0, _1, cost_delta_sd = k.run_steps(len(movable_vertices),<br>                                        distance_limit,<br>                                        1e100)<br>    temperature = 20.0  cost_delta_sd<br><br>    # The number of swap-attempts between temperature changes is selected by<br>    # the heuristic used by VPR. This value is scaled linearly by the effort<br>    # parameter.<br>    num_steps = max(1, int(effort  len(vertices_resources)1.33))<br><br>    logger.info("Initial placement temperature: %0.1f", temperature)<br><br>    # Counter for the number of swap attempts made (used for diagnostic<br>    # purposes)<br>    iteration_count = 0<br><br>    # Holds the total cost of the current placement. This default value chosen<br>    # to ensure the loop below iterates at least once.<br>    current_cost = 0.0<br><br>    # The annealing algorithm runs until a heuristic termination condition<br>    # (taken from VPR) is hit. The heuristic waits until the temperature falls<br>    # below a small fraction of the average net cost.<br>    while temperature  (0.005  current_cost) / len(nets):<br>        # Run an iteration at the current temperature<br>        num_accepted, current_cost, _ = k.run_steps(<br>            num_steps, int(math.ceil(distance_limit)), temperature)<br><br>        # The ratio of accepted-to-not-accepted changes<br>        r_accept = num_accepted / float(num_steps)<br><br>        # Special case: Can't do better than 0 cost! This is a special case<br>        # since the normal termination condition will not terminate if the cost<br>        # doesn't drop below 0.<br>        if current_cost == 0:<br>            break<br><br>        # The temperature is reduced by a factor heuristically based on the<br>        # acceptance rate. The schedule below attempts to maximise the time<br>        # spent at temperatures where a large portion (but not all) of changes<br>        # are being accepted. If lots of changes are being accepted (e.g.<br>        # during high-temperature periods) then most of them are likely not to<br>        # be beneficial. If few changes are being accepted, we're probably<br>        # pretty close to the optimal placement.<br>        if r_accept  0.96:<br>            alpha = 0.5<br>        elif r_accept  0.8:<br>            alpha = 0.9<br>        elif r_accept  0.15:<br>            alpha = 0.95<br>        else:<br>            alpha = 0.8<br>        temperature = alpha  temperature<br><br>        # According to:<br>        #  M. Huang, F. Romeo, and A. Sangiovanni-Vincentelli, "An Efficient<br>        #   General Cooling Schedule for Simulated Annealing" ICCAD, 1986, pp.<br>        #   381 - 384 and J. Lam<br>        #  J. Delosme, "Performance of a New Annealing Schedule" DAC, 1988,<br>        #   pp. 306 - 311.<br>        # It is desirable to keep the acceptance ratio as close to 0.44 for as<br>        # long as possible. As a result, when r_accept falls below this we can<br>        # help increase the acceptance rate by reducing the set of possible<br>        # swap candidates based on the observation that near the end of<br>        # placement, most things are near their optimal location and thus long<br>        # distance swaps are unlikely to be useful.<br>        distance_limit = 1.0 - 0.44 + r_accept<br>        distance_limit = min(max(distance_limit, 1.0),<br>                             max(machine.width, machine.height))<br><br>        iteration_count += num_steps<br>        logger.debug("Iteration: %d, "<br>                     "Cost: %0.1f, "<br>                     "Kept: %0.1f%%, "<br>                     "Temp: %0.3f, "<br>                     "Dist: %d.",<br>                     iteration_count, current_cost,<br>                     r_accept100, temperature, math.ceil(distance_limit))<br><br>        # Call the user callback before the next iteration, terminating if<br>        # requested.<br>        if on_temperature_change is not None:<br>            placements = k.get_placements().copy()<br>            finalise_same_chip_constraints(substitutions, placements)<br>            ret_val = on_temperature_change(iteration_count,<br>                                            placements,<br>                                            current_cost,<br>                                            r_accept,<br>                                            temperature,<br>                                            distance_limit)<br>            if ret_val is False:<br>                break<br><br>    logger.info("Anneal terminated after %d iterations.", iteration_count)<br><br>    placements = k.get_placements()<br>    finalise_same_chip_constraints(substitutions, placements)<br><br>    return placements

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_314836.txt
> def traveling_salesman(G, sampler=None, lagrange=2, weight='weight',<br>                       sampler_args):<br>    """Returns an approximate minimum traveling salesperson route.<br><br>    Defines a QUBO with ground states corresponding to the<br>    minimum routes and uses the sampler to sample<br>    from it.<br><br>    A route is a cycle in the graph that reaches each node exactly once.<br>    A minimum route is a route with the smallest total edge weight.<br><br>    Parameters<br>    ----------<br>    G : NetworkX graph<br>        The graph on which to find a minimum traveling salesperson route.<br>        This should be a complete graph with non-zero weights on every edge.<br><br>    sampler :<br>        A binary quadratic model sampler. A sampler is a process that<br>        samples from low energy states in models defined by an Ising<br>        equation or a Quadratic Unconstrained Binary Optimization<br>        Problem (QUBO). A sampler is expected to have a 'sample_qubo'<br>        and 'sample_ising' method. A sampler is expected to return an<br>        iterable of samples, in order of increasing energy. If no<br>        sampler is provided, one must be provided using the<br>        `set_default_sampler` function.<br><br>    lagrange : optional (default 2)<br>        Lagrange parameter to weight constraints (visit every city once)<br>        versus objective (shortest distance route).<br><br>    weight : optional (default 'weight')<br>        The name of the edge attribute containing the weight.<br><br>    sampler_args :<br>        Additional keyword parameters are passed to the sampler.<br><br>    Returns<br>    -------<br>    route : list<br>       List of nodes in order to be visited on a route<br><br>    Examples<br>    --------<br>    This example uses a `dimod <https://github.com/dwavesystems/dimod`_ sampler<br>    to find a minimum route in a five-cities problem.<br><br>     import dwave_networkx as dnx<br>     import networkx as nx<br>     import dimod<br>    ...<br>     G = nx.complete_graph(4)<br>     G.add_weighted_edges_from({(0, 1, 1), (0, 2, 2), (0, 3, 3), (1, 2, 3),<br>    ...                            (1, 3, 4), (2, 3, 5)})<br>     dnx.traveling_salesman(G, dimod.ExactSolver())<br>    [2, 1, 0, 3]<br><br>    Notes<br>    -----<br>    Samplers by their nature may not return the optimal solution. This<br>    function does not attempt to confirm the quality of the returned<br>    sample.<br><br>    """<br>    # Get a QUBO representation of the problem<br>    Q = traveling_salesman_qubo(G, lagrange, weight)<br><br>    # use the sampler to find low energy states<br>    response = sampler.sample_qubo(Q, sampler_args)<br><br>    # we want the lowest energy sample, in order by stop number<br>    sample = next(iter(response))<br>    route = []<br>    for entry in sample:<br>        if sample[entry]  0:<br>            route.append(entry)<br>    route.sort(key=lambda x: x[1])<br>    return list((x[0] for x in route))


## Ground Truth

### GROUND TRUTH 0, ranked 86, leetcode/leetcode_778.txt
> import heapq<br>    """You are given an `n x n` integer matrix `grid` where each value `grid[i][j]` represents the elevation at that point `(i, j)`.<br><br>The rain starts to fall. At time `t`, the depth of the water everywhere is `t`. You can swim from a square to another 4-directionally adjacent square if and only if the elevation of both squares individually are at most `t`. You can swim infinite distances in zero time. Of course, you must stay within the boundaries of the grid during your swim.<br><br>Return _the least time until you can reach the bottom right square_ `(n - 1, n - 1)` _if you start at the top left square_ `(0, 0)`.<br><br>Example 1:<br><br>Input: grid = \[\[0,2\],\[1,3\]\]<br>Output: 3<br>Explanation:<br>At time 0, you are in grid location (0, 0).<br>You cannot go anywhere else because 4-directionally adjacent neighbors have a higher elevation than t = 0.<br>You cannot reach point (1, 1) until time 3.<br>When the depth of water is 3, we can swim anywhere inside the grid.<br><br>Example 2:<br><br>Input: grid = \[\[0,1,2,3,4\],\[24,23,22,21,5\],\[12,13,14,15,16\],\[11,17,18,19,20\],\[10,9,8,7,6\]\]<br>Output: 16<br>Explanation: The final route is shown.<br>We need to wait until time 16 so that (0, 0) and (4, 4) are connected.<br><br>Constraints:<br><br>   `n == grid.length`<br>   `n == grid[i].length`<br>   `1 <= n <= 50`<br>   `0 <= grid[i][j] < n2`<br>   Each value `grid[i][j]` is unique."""<br><br><br>def rearrange_string(s):<br>    counts = {}<br>    for c in s:<br>        counts[c] = counts.get(c, 0) + 1<br>    <br>    pq = [(-count, char) for char, count in counts.items()]<br>    heapq.heapify(pq)<br>    <br>    result = []<br>    previous = (0, '')<br>    <br>    while pq:<br>        count, char = heapq.heappop(pq)<br>        result.append(char)<br>        <br>        if previous[0] < 0:<br>            heapq.heappush(pq, previous)<br>        <br>        count += 1<br>        previous = (count, char)<br>    <br>    result_str = ''.join(result)<br>    return result_str if len(result_str) == len(s) else ""
