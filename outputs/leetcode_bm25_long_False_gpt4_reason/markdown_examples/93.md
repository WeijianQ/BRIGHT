# Query `93`

## Original Question

*You are playing a game that contains multiple characters, and each of the characters has two main properties: attack and defense. You are given a 2D integer array `properties` where `properties[i] = [attacki, defensei]` represents the properties of the `ith` character in the game.

A character is said to be weak if any other character has both attack and defense levels strictly greater than this character's attack and defense levels. More formally, a character `i` is said to be weak if there exists another character `j` where `attackj > attacki` and `defensej > defensei`.

Return _the number of weak characters_.

Example 1:

Input: properties = \[\[5,5\],\[6,3\],\[3,6\]\]
Output: 0
Explanation: No character has strictly greater attack and defense than the other.

Example 2:

Input: properties = \[\[2,2\],\[3,3\]\]
Output: 1
Explanation: The first character is weak because the second character has a strictly greater attack and defense.

Example 3:

Input: properties = \[\[1,5\],\[10,4\],\[4,3\]\]
Output: 1
Explanation: The third character is weak because the second character has a strictly greater attack and defense.

Constraints:

   `2 <= properties.length <= 105`
   `properties[i].length == 2`
   `1 <= attacki, defensei <= 105`*

## gpt4 Reasoning

```markdown
The essential problem here is to identify and count the number of characters in a game that are considered "weak" based on their attack and defense properties, according to the given definition. To solve this problem efficiently, considering the potential large size of the input array (up to 105 elements), we need to devise a strategy that allows us to compare each character's properties without having to compare each character with every other character in a brute force manner, which would be highly inefficient.

### Step-by-Step Strategy:

1. **Sort the Characters**: The first step is to sort the characters in a way that allows us to efficiently determine if a character is weaker than any other. A logical approach is to sort the characters primarily by their attack in descending order. For characters with the same attack value, we sort them by their defense in ascending order. This way, when we traverse the sorted list, a character with a lower defense could only be potentially weaker than any character that has appeared before it (since any character coming after it would have either a lower attack or the same attack but lower or equal defense).

2. **Traverse and Compare**: After sorting, we traverse the sorted array from the beginning. We keep track of the highest defense value we've encountered so far. For each character, if its defense is lower than the maximum defense value we've seen, it means there's at least one character with both a higher attack and a higher defense, making this character weak. We increment our count of weak characters in such cases.

3. **Edge Cases and Optimization**: Since we've sorted characters with the same attack value by their defense in ascending order, it's guaranteed that if a character's defense is not higher than the maximum defense seen so far, it cannot be the strongest among characters with the same attack value. This subtle detail helps us avoid mistakenly counting characters as weak when they're only outmatched by characters with the same attack but higher defense.

### Algorithm:

1. Sort `properties` based on the described criteria: first by attack in descending order, then by defense in ascending order for ties.
2. Initialize a variable `maxDefense` to keep track of the highest defense encountered so far, and set it to a very low value initially.
3. Initialize a counter `weakCharacters` to 0.
4. Iterate through the sorted list of characters. For each character:
   - If the character's defense is lower than `maxDefense`, increment `weakCharacters`.
   - Update `maxDefense` if the current character's defense is higher than the current `maxDefense`.
5. Return `weakCharacters` as the result.

### Relevant Information:

- The sorting strategy is crucial for efficiency, reducing the problem to a single pass through the data after sorting.
- Keeping track of the maximum defense encountered as we traverse the list allows us to efficiently determine if a character is weak without needing to compare it directly against all others.
- The constraint on the size of the input array (up to 105 elements) suggests that an \(O(n \log n)\) sorting operation followed by an \(O(n)\) traversal is an acceptable approach, as more naive \(O(n^2)\) comparison methods would be impractical.

By following this approach, we can efficiently identify and count the weak characters in the game, adhering to the problem's constraints and requirements.
```

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/csn_python_train_6417.txt
> def take_step(self,<br>                  state: StateType,<br>                  max_actions: int = None,<br>                  allowed_actions: List[Set] = None) - List[StateType]:<br>        """<br>        The main method in the ``TransitionFunction`` API.  This function defines the computation<br>        done at each step of decoding and returns a ranked list of next states.<br><br>        The input state is `grouped`, to allow for efficient computation, but the output states<br>        should all have a ``group_size`` of 1, to make things easier on the decoding algorithm.<br>        They will get regrouped later as needed.<br><br>        Because of the way we handle grouping in the decoder states, constructing a new state is<br>        actually a relatively expensive operation.  If you know a priori that only some of the<br>        states will be needed (either because you have a set of gold action sequences, or you have<br>        a fixed beam size), passing that information into this function will keep us from<br>        constructing more states than we need, which will greatly speed up your computation.<br><br>        IMPORTANT: This method `must` returns states already sorted by their score, otherwise<br>        ``BeamSearch`` and other methods will break.  For efficiency, we do not perform an<br>        additional sort in those methods.<br><br>        ALSO IMPORTANT: When ``allowed_actions`` is given and ``max_actions`` is not, we assume you<br>        want to evaluate all possible states and do not need any sorting (e.g., this is true for<br>        maximum marginal likelihood training that does not use a beam search).  In this case, we<br>        may skip the sorting step for efficiency reasons.<br><br>        Parameters<br>        ----------<br>        state : ``State``<br>            The current state of the decoder, which we will take a step `from`.  We may be grouping<br>            together computation for several states here.  Because we can have several states for<br>            each instance in the original batch being evaluated at the same time, we use<br>            ``group_size`` for this kind of batching, and ``batch_size`` for the `original` batch<br>            in ``model.forward.``<br>        max_actions : ``int``, optional<br>            If you know that you will only need a certain number of states out of this (e.g., in a<br>            beam search), you can pass in the max number of actions that you need, and we will only<br>            construct that many states (for each `batch` instance - `not` for each `group`<br>            instance!).  This can save a whole lot of computation if you have an action space<br>            that's much larger than your beam size.<br>        allowed_actions : ``List[Set]``, optional<br>            If the ``DecoderTrainer`` has constraints on which actions need to be evaluated (e.g.,<br>            maximum marginal likelihood only needs to evaluate action sequences in a given set),<br>            you can pass those constraints here, to avoid constructing state objects unnecessarily.<br>            If there are no constraints from the trainer, passing a value of ``None`` here will<br>            allow all actions to be considered.<br><br>            This is a list because it is `batched` - every instance in the batch has a set of<br>            allowed actions.  Note that the size of this list is the ``group_size`` in the<br>            ``State``, `not` the ``batch_size`` of ``model.forward``.  The training algorithm needs<br>            to convert from the `batched` allowed action sequences that it has to a `grouped`<br>            allowed action sequence list.<br><br>        Returns<br>        -------<br>        next_states : ``List[State]``<br>            A list of next states, ordered by score.<br>        """<br>        raise NotImplementedError

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_178638.txt
> def _concordance_summary_statistics(<br>    event_times, predicted_event_times, event_observed<br>):  # pylint: disable=too-many-locals<br>    """Find the concordance index in n  log(n) time.<br><br>    Assumes the data has been verified by lifelines.utils.concordance_index first.<br>    """<br>    # Here's how this works.<br>    #<br>    # It would be pretty easy to do if we had no censored data and no ties. There, the basic idea<br>    # would be to iterate over the cases in order of their true event time (from least to greatest),<br>    # while keeping track of a pool of predicted event times for all cases previously seen (= all<br>    # cases that we know should be ranked lower than the case we're looking at currently).<br>    #<br>    # If the pool has O(log n) insert and O(log n) RANK (i.e., "how many things in the pool have<br>    # value less than x"), then the following algorithm is n log n:<br>    #<br>    # Sort the times and predictions by time, increasing<br>    # n_pairs, n_correct := 0<br>    # pool := {}<br>    # for each prediction p:<br>    #     n_pairs += len(pool)<br>    #     n_correct += rank(pool, p)<br>    #     add p to pool<br>    #<br>    # There are three complications: tied ground truth values, tied predictions, and censored<br>    # observations.<br>    #<br>    # - To handle tied true event times, we modify the inner loop to work in batches of observations<br>    # p_1, ..., p_n whose true event times are tied, and then add them all to the pool<br>    # simultaneously at the end.<br>    #<br>    # - To handle tied predictions, which should each count for 0.5, we switch to<br>    #     n_correct += min_rank(pool, p)<br>    #     n_tied += count(pool, p)<br>    #<br>    # - To handle censored observations, we handle each batch of tied, censored observations just<br>    # after the batch of observations that died at the same time (since those censored observations<br>    # are comparable all the observations that died at the same time or previously). However, we do<br>    # NOT add them to the pool at the end, because they are NOT comparable with any observations<br>    # that leave the study afterward--whether or not those observations get censored.<br>    if np.logical_not(event_observed).all():<br>        return (0, 0, 0)<br><br>    died_mask = event_observed.astype(bool)<br>    # TODO: is event_times already sorted? That would be nice...<br>    died_truth = event_times[died_mask]<br>    ix = np.argsort(died_truth)<br>    died_truth = died_truth[ix]<br>    died_pred = predicted_event_times[died_mask][ix]<br><br>    censored_truth = event_times[~died_mask]<br>    ix = np.argsort(censored_truth)<br>    censored_truth = censored_truth[ix]<br>    censored_pred = predicted_event_times[~died_mask][ix]<br><br>    censored_ix = 0<br>    died_ix = 0<br>    times_to_compare = _BTree(np.unique(died_pred))<br>    num_pairs = np.int64(0)<br>    num_correct = np.int64(0)<br>    num_tied = np.int64(0)<br><br>    # we iterate through cases sorted by exit time:<br>    # - First, all cases that died at time t0. We add these to the sortedlist of died times.<br>    # - Then, all cases that were censored at time t0. We DON'T add these since they are NOT<br>    #   comparable to subsequent elements.<br>    while True:<br>        has_more_censored = censored_ix < len(censored_truth)<br>        has_more_died = died_ix < len(died_truth)<br>        # Should we look at some censored indices next, or died indices?<br>        if has_more_censored and (not has_more_died or died_truth[died_ix]  censored_truth[censored_ix]):<br>            pairs, correct, tied, next_ix = _handle_pairs(censored_truth, censored_pred, censored_ix, times_to_compare)<br>            censored_ix = next_ix<br>        elif has_more_died and (not has_more_censored or died_truth[died_ix] <= censored_truth[censored_ix]):<br>            pairs, correct, tied, next_ix = _handle_pairs(died_truth, died_pred, died_ix, times_to_compare)<br>            for pred in died_pred[died_ix:next_ix]:<br>                times_to_compare.insert(pred)<br>            died_ix = next_ix<br>        else:<br>            assert not (has_more_died or has_more_censored)<br>            break<br><br>        num_pairs += pairs<br>        num_correct += correct<br>        num_tied += tied<br><br>    return (num_correct, num_tied, num_pairs)

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_65186.txt
> def zrangebyscore(self,<br>                      key,<br>                      min_score,<br>                      max_score,<br>                      with_scores=False,<br>                      offset=0,<br>                      count=0):<br>        """Returns all the elements in the sorted set at key with a score<br>        between min and max (including elements with score equal to min or<br>        max). The elements are considered to be ordered from low to high<br>        scores.<br><br>        The elements having the same score are returned in lexicographical<br>        order (this follows from a property of the sorted set implementation in<br>        Redis and does not involve further computation).<br><br>        The optional ``offset`` and ``count`` arguments can be used to only get<br>        a range of the matching elements (similar to SELECT LIMIT offset, count<br>        in SQL). Keep in mind that if offset is large, the sorted set needs to<br>        be traversed for offset elements before getting to the elements to<br>        return, which can add up to ``O(N)`` time complexity.<br><br>        The optional ``with_scores`` argument makes the command return both the<br>        element and its score, instead of the element alone. This option is<br>        available since Redis 2.0.<br><br>        Exclusive intervals and infinity<br><br>        ``min_score`` and ``max_score`` can be ``-inf`` and ``+inf``, so that<br>        you are not required to know the highest or lowest score in the sorted<br>        set to get all elements from or up to a certain score.<br><br>        By default, the interval specified by ``min_score`` and ``max_score``<br>        is closed (inclusive). It is possible to specify an open interval<br>        (exclusive) by prefixing the score with the character ``(``. For<br>        example:<br><br>        .. code::<br><br>            ZRANGEBYSCORE zset (1 5<br><br>        Will return all elements with ``1 < score <= 5`` while:<br><br>        .. code::<br><br>            ZRANGEBYSCORE zset (5 (10<br><br>        Will return all the elements with ``5 < score < 10`` (5 and 10<br>        excluded).<br><br>        .. note::<br><br>           Time complexity: ``O(log(N)+M)`` with ``N`` being the number of<br>           elements in the sorted set and ``M`` the number of elements being<br>           returned. If ``M`` is constant (e.g. always asking for the first<br>           10 elements with ``count``), you can consider it ``O(log(N))``.<br><br>        :param key: The key of the sorted set<br>        :type key: :class:`str`, :class:`bytes`<br>        :param min_score: Lowest score definition<br>        :type min_score: :class:`str`, :class:`bytes`<br>        :param max_score: Highest score definition<br>        :type max_score: :class:`str`, :class:`bytes`<br>        :param bool with_scores: Return elements and scores<br>        :param offset: The number of elements to skip<br>        :type min_score: :class:`str`, :class:`bytes`<br>        :param count: The number of elements to return<br>        :type min_score: :class:`str`, :class:`bytes`<br>        :rtype: list<br>        :raises: :exc:`~tredis.exceptions.RedisError`<br>        """<br>        command = [b'ZRANGEBYSCORE', key, min_score, max_score]<br>        if with_scores:<br>            command += ['WITHSCORES']<br>        if offset or count:<br>            command += ['LIMIT', offset, count]<br>        return self._execute(command)

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_226148.txt
> def majority_vote_byte_scan(relfilepath, fileslist, outpath, blocksize=65535, default_char_null=False):<br>    '''Takes a list of files in string format representing the same data, and disambiguate by majority vote: for position in string, if the character is not the same accross all entries, we keep the major one. If none, it will be replaced by a null byte (because we can't know if any of the entries are correct about this character).<br>    relfilepath is the filename or the relative file path relative to the parent directory (ie, this is the relative path so that we can compare the files from several directories).'''<br>    # The idea of replication combined with ECC was a bit inspired by this paper: Friedman, Roy, Yoav Kantor, and Amir Kantor. "Combining Erasure-Code and Replication Redundancy Schemes for Increased Storage and Repair Efficiency in P2P Storage Systems.", 2013, Technion, Computer Science Department, Technical Report CS-2013-03<br>    # But it is a very well known concept in redundancy engineering, usually called triple-modular redundancy (which is here extended to n-modular since we can supply any number of files we want, not just three).<br>    # Preference in case of ambiguity is always given to the file of the first folder.<br><br>    fileshandles = []<br>    for filepath in fileslist:<br>        if filepath:<br>            # Already a file handle? Just store it in the fileshandles list<br>            if hasattr(filepath, 'read'):<br>                fileshandles.append(filepath)<br>            # Else it's a string filepath, open the file<br>            else:<br>                fileshandles.append(open(filepath, 'rb'))<br><br>    # Create and open output (merged) file, except if we were already given a file handle<br>    if hasattr(outpath, 'write'):<br>        outfile = outpath<br>    else:<br>        outpathfull = os.path.join(outpath, relfilepath)<br>        pardir = os.path.dirname(outpathfull)<br>        if not os.path.exists(pardir):<br>            os.makedirs(pardir)<br>        outfile = open(outpathfull, 'wb')<br><br>    # Cannot vote if there's not at least 3 files!<br>    # In this case, just copy the file from the first folder, verbatim<br>    if len(fileshandles) < 3:<br>        # If there's at least one input file, then copy it verbatim to the output folder<br>        if fileshandles:<br>            create_dir_if_not_exist(os.path.dirname(outpathfull))<br>            buf = 1<br>            while (buf):<br>                buf = fileshandles[0].read()<br>                outfile.write(buf)<br>                outfile.flush()<br>        return (1, "Error with file %s: only %i copies available, cannot vote (need at least 3)! Copied the first file from the first folder, verbatim." % (relfilepath, len(fileshandles)))<br><br>    errors = []<br>    entries = [1]len(fileshandles)  # init with 0 to start the while loop<br>    while (entries.count('') < len(fileshandles)):<br>        final_entry = []<br>        # Read a block from all input files into memory<br>        for i in xrange(len(fileshandles)):<br>            entries[i] = fileshandles[i].read(blocksize)<br><br>        # End of file for all files, we exit<br>        if entries.count('') == len(fileshandles):<br>            break<br>        # Else if there's only one file, just copy the file's content over<br>        elif len(entries) == 1:<br>            final_entry = entries[0]<br><br>        # Else, do the majority vote<br>        else:<br>            # Walk along each column (imagine the strings being rows in a matrix, then we pick one column at each iteration = all characters at position i of each string), so that we can compare these characters easily<br>            for i in xrange(max(len(entry) for entry in entries)):<br>                hist = {} # kind of histogram, we just memorize how many times a character is presented at the position i in each string TODO: use collections.Counter instead of dict()?<br>                # Extract the character at position i of each string and compute the histogram at the same time (number of time this character appear among all strings at this position i)<br>                for entry in entries:<br>                    # Check if we are not beyond the current entry's length<br>                    if i < len(entry): # TODO: check this line, this should allow the vote to continue even if some files are shorter than others<br>                        # Extract the character and use it to contribute to the histogram<br>                        # TODO: add warning message when one file is not of the same size as the others<br>                        key = str(ord(entry[i])) # convert to the ascii value to avoid any funky problem with encoding in dict keys<br>                        hist[key] = hist.get(key, 0) + 1 # increment histogram for this value. If it does not exists, use 0. (essentially equivalent to hist[key] += 1 but with exception management if key did not already exists)<br>                # If there's only one character (it's the same accross all strings at position i), then it's an exact match, we just save the character and we can skip to the next iteration<br>                if len(hist) == 1:<br>                    final_entry.append(chr(int(hist.iterkeys().next())))<br>                    continue<br>                # Else, the character is different among different entries, we will pick the major one (mode)<br>                elif len(hist)  1:<br>                    # Sort the dict by value (and reverse because we want the most frequent first)<br>                    skeys = sorted(hist, key=hist.get, reverse=True)<br>                    # Ambiguity! If each entries present a different character (thus the major has only an occurrence of 1), then it's too ambiguous and we just set a null byte to signal that<br>                    if hist[skeys[0]] == 1:<br>                        if default_char_null:<br>                            if default_char_null is True:<br>                                final_entry.append("\x00")<br>                            else:<br>                                final_entry.append(default_char_null)<br>                        else:<br>                            # Use the entry of the first file that is still open<br>                            first_char = ''<br>                            for entry in entries:<br>                                # Found the first file that has a character at this position: store it and break loop<br>                                if i < len(entry):<br>                                    first_char = entry[i]<br>                                    break<br>                            # Use this character in spite of ambiguity<br>                            final_entry.append(first_char)<br>                        errors.append(outfile.tell() + i) # Print an error indicating the characters that failed<br>                    # Else if there is a tie (at least two characters appear with the same frequency), then we just pick one of them<br>                    elif hist[skeys[0]] == hist[skeys[1]]:<br>                        final_entry.append(chr(int(skeys[0]))) # TODO: find a way to account for both characters. Maybe return two different strings that will both have to be tested? (eg: maybe one has a tampered hash, both will be tested and if one correction pass the hash then it's ok we found the correct one)<br>                    # Else we have a clear major character that appear in more entries than any other character, then we keep this one<br>                    else:<br>                        final_entry.append(chr(int(skeys[0]))) # alternative one-liner: max(hist.iteritems(), key=operator.itemgetter(1))[0]<br>                    continue<br>            # Concatenate to a string (this is faster than using a string from the start and concatenating at each iteration because Python strings are immutable so Python has to copy over the whole string, it's in O(n^2)<br>            final_entry = ''.join(final_entry)<br>            # Commit to output file<br>            outfile.write(final_entry)<br>            outfile.flush()<br><br>    # Errors signaling<br>    if errors:<br>        error_msg = "Unrecoverable corruptions (because of ambiguity) in file %s on characters: %s." % (relfilepath, [hex(int(x)) for x in errors]) # Signal to user that this file has unrecoverable corruptions (he may try to fix the bits manually or with his own script)<br>        return (1, error_msg) # return an error<br>    # Close all input files<br>    for fh in fileshandles:<br>        fh.close()<br>    # Close output file<br>    if outfile != outpath:  # close only if we were not given a file handle in the first place<br>        outfile.flush()<br>        outfile.close()<br>    return (0, None)

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_333992.txt
> def canonical_order(self):<br>        """The vertices in a canonical or normalized order.<br><br>           This routine will return a list of vertices in an order that does not<br>           depend on the initial order, but only depends on the connectivity and<br>           the return values of the function self.get_vertex_string.<br><br>           Only the vertices that are involved in edges will be included. The<br>           result can be given as first argument to self.get_subgraph, with<br>           reduce=True as second argument. This will return a complete canonical<br>           graph.<br><br>           The routine is designed not to use symmetry relations that are<br>           obtained with the GraphSearch routine. We also tried to create an<br>           ordering that feels like natural, i.e. starting in the center and<br>           pushing vertices with few equivalents to the front. If necessary, the<br>           nature of the vertices and  their bonds to atoms closer to the center<br>           will also play a role, but only as a last resort.<br>        """<br>        # A) find an appropriate starting vertex.<br>        # Here we take a central vertex that has a minimal number of symmetrical<br>        # equivalents, 'the highest atom number', and the highest fingerprint.<br>        # Note that the symmetrical equivalents are computed from the vertex<br>        # fingerprints, i.e. without the GraphSearch.<br>        starting_vertex = max(<br>            (<br>                -len(self.equivalent_vertices[vertex]),<br>                self.get_vertex_string(vertex),<br>                self.vertex_fingerprints[vertex].tobytes(),<br>                vertex<br>            ) for vertex in self.central_vertices<br>        )[-1]<br><br>        # B) sort all vertices based on<br>        #      1) distance from central vertex<br>        #      2) number of equivalent vertices<br>        #      3) vertex string, (higher atom numbers come first)<br>        #      4) fingerprint<br>        #      5) vertex index<br>        # The last field is only included to collect the result of the sort.<br>        # The fingerprint on itself would be sufficient, but the three first are<br>        # there to have a naturally appealing result.<br>        l = [<br>            [<br>                -distance,<br>                -len(self.equivalent_vertices[vertex]),<br>                self.get_vertex_string(vertex),<br>                self.vertex_fingerprints[vertex].tobytes(),<br>                vertex<br>            ] for vertex, distance in self.iter_breadth_first(starting_vertex)<br>            if len(self.neighbors[vertex])  0<br>        ]<br>        l.sort(reverse=True)<br><br>        # C) The order of some vertices is still not completely set. e.g.<br>        # consider the case of allene. The four hydrogen atoms are equivalent,<br>        # but one can have two different orders: make geminiles consecutive or<br>        # don't. It is more trikcy than one would think at first sight. In the<br>        # case of allene, geminility could easily solve the problem. Consider a<br>        # big flat rotationally symmetric molecule (order 2). The first five<br>        # shells are order 4 and one would just give a random order to four<br>        # segemnts in the first shell. Only when one reaches the outer part that<br>        # has order two, it turns out that the arbitrary choices in the inner<br>        # shell play a role. So it does not help to look at relations with<br>        # vertices at inner or current shells only. One has to consider the<br>        # whole picture. (unit testing reveals troubles like these)<br><br>        # I need some sleep now. The code below checks for potential fuzz and<br>        # will raise an error if the ordering is not fully determined yet. One<br>        # day, I'll need this code more than I do now, and I'll fix things up.<br>        # I know how to do this, but I don't care enough right now.<br>        # -- Toon<br>        for i in range(1, len(l)):<br>            if l[i][:-1] == l[i-1][:-1]:<br>                raise NotImplementedError<br><br>        # D) Return only the vertex indexes.<br>        return [record[-1] for record in l]


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_354.txt
> from bisect import bisect_left<br>    """You are given a 2D array of integers `envelopes` where `envelopes[i] = [wi, hi]` represents the width and the height of an envelope.<br><br>One envelope can fit into another if and only if both the width and height of one envelope are greater than the other envelope's width and height.<br><br>Return _the maximum number of envelopes you can Russian doll (i.e., put one inside the other)_.<br><br>Note: You cannot rotate an envelope.<br><br>Example 1:<br><br>Input: envelopes = \[\[5,4\],\[6,4\],\[6,7\],\[2,3\]\]<br>Output: 3<br>Explanation: The maximum number of envelopes you can Russian doll is `3` (\[2,3\] = \[5,4\] = \[6,7\]).<br><br>Example 2:<br><br>Input: envelopes = \[\[1,1\],\[1,1\],\[1,1\]\]<br>Output: 1<br><br>Constraints:<br><br>   `1 <= envelopes.length <= 105`<br>   `envelopes[i].length == 2`<br>   `1 <= wi, hi <= 105`"""<br><br><br>def max_envelopes(envelopes):<br>    envelopes.sort(key=lambda x: (x[0], -x[1]))<br>    dp = []<br>    <br>    for _, h in envelopes:<br>        idx = bisect_left(dp, h)<br>        if idx == len(dp):<br>            dp.append(h)<br>        else:<br>            dp[idx] = h<br><br>    return len(dp)
