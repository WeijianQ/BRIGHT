# Query `44`

## Original Question

*There are `n` workers. You are given two integer arrays `quality` and `wage` where `quality[i]` is the quality of the `ith` worker and `wage[i]` is the minimum wage expectation for the `ith` worker.

We want to hire exactly `k` workers to form a paid group. To hire a group of `k` workers, we must pay them according to the following rules:

1.  Every worker in the paid group should be paid in the ratio of their quality compared to other workers in the paid group.
2.  Every worker in the paid group must be paid at least their minimum wage expectation.

Given the integer `k`, return _the least amount of money needed to form a paid group satisfying the above conditions_. Answers within `10-5` of the actual answer will be accepted.

Example 1:

Input: quality = \[10,20,5\], wage = \[70,50,30\], k = 2
Output: 105.00000
Explanation: We pay 70 to 0th worker and 35 to 2nd worker.

Example 2:

Input: quality = \[3,1,10,10,1\], wage = \[4,8,2,2,7\], k = 3
Output: 30.66667
Explanation: We pay 4 to 0th worker, 13.33333 to 2nd and 3rd workers separately.

Constraints:

   `n == quality.length == wage.length`
   `1 <= k <= n <= 104`
   `1 <= quality[i], wage[i] <= 104`*

## gpt4 Reasoning

```markdown
The essential problem here is to find the minimum total wage that needs to be paid to hire exactly `k` workers under the constraints that each worker is paid at least their minimum wage expectation and that the wages are proportional to their quality compared to other workers in the group. This is a problem of optimization under specific constraints.

### Step 1: Understanding the Wage Calculation

First, let's understand how the wages are calculated. If we decide to form a group of workers, the total wage to be paid is proportional to the sum of their qualities. The ratio of payment needs to be such that the least ratio that satisfies all workers' minimum wage expectations is chosen. This ratio, let's call it `wagePerQuality`, is effectively the wage paid per unit of quality. For any worker `i`, their wage is calculated as `wagePerQuality * quality[i]`.

### Step 2: Sorting Workers by Wage Expectation Ratio

A logical approach to solving this problem involves sorting the workers based on their ratio of minimum wage expectation to their quality (`wage[i] / quality[i]`). This ratio effectively represents the minimum `wagePerQuality` that each worker is willing to work for. By sorting the workers based on this ratio, we ensure that we consider workers in the order of their cost-effectiveness.

### Step 3: Finding the Minimum Total Wage for `k` Workers

After sorting, we need to find a group of `k` workers that minimizes the total wage. Since the workers are sorted by their minimum acceptable `wagePerQuality`, as we iterate through the list, each worker potentially increases the `wagePerQuality` for the group. For each worker (considering them as the last worker in a potential group), we need to calculate the total wage needed to hire them along with the `k-1` most cost-effective workers before them. This involves:

- Keeping a running sum of the qualities of the `k` workers.
- Calculating the total wage as `currentWagePerQuality * sumOfQualities`, where `currentWagePerQuality` is the ratio for the current worker being considered as the last member of the group.

### Step 4: Implementing a Priority Queue

To efficiently manage the group of `k` workers as we iterate through the sorted list, we can use a priority queue (or a max heap) to keep track of the workers with the highest qualities. Since adding a new worker might increase the `wagePerQuality`, we want to remove the worker with the highest quality (and thus the highest wage) if we have more than `k` workers in our current group. This helps in ensuring that we always have the most cost-effective group of `k` workers.

### Step 5: Calculating the Minimum Total Wage

- Initialize a priority queue to store the qualities of the workers.
- Iterate through the sorted list of workers, adding each worker's quality to the priority queue.
- If the priority queue's size exceeds `k`, remove the worker with the highest quality.
- Calculate the total wage for the current group of `k` workers and update the minimum total wage if the current total wage is lower.

### Conclusion

This approach leverages sorting and a priority queue to efficiently find the minimum total wage needed to hire `k` workers under the given constraints. By focusing on the ratio of wage expectation to quality, we ensure that we always consider the most cost-effective options while respecting each worker's minimum wage expectation.
```

### Metrics

```
recip_rank: 0.0000
P_1: 0.0000
P_5: 0.0000
P_10: 0.0000
P_25: 0.0000
P_50: 0.0000
P_100: 0.0000
recall_1: 0.0000
recall_5: 0.0000
recall_10: 0.0000
recall_25: 0.0000
recall_50: 0.0000
recall_100: 0.0000
ndcg_cut_1: 0.0000
ndcg_cut_5: 0.0000
ndcg_cut_10: 0.0000
ndcg_cut_25: 0.0000
ndcg_cut_50: 0.0000
ndcg_cut_100: 0.0000
map_cut_1: 0.0000
map_cut_5: 0.0000
map_cut_10: 0.0000
map_cut_25: 0.0000
map_cut_50: 0.0000
map_cut_100: 0.0000
```

## Retrieval Results

### DOC[1] (IRRELEVANT) leetcode/leetcode_887.txt
> import heapq<br>    """You are given `k` identical eggs and you have access to a building with `n` floors labeled from `1` to `n`.<br><br>You know that there exists a floor `f` where `0 <= f <= n` such that any egg dropped at a floor higher than `f` will break, and any egg dropped at or below floor `f` will not break.<br><br>Each move, you may take an unbroken egg and drop it from any floor `x` (where `1 <= x <= n`). If the egg breaks, you can no longer use it. However, if the egg does not break, you may reuse it in future moves.<br><br>Return _the minimum number of moves that you need to determine with certainty what the value of_ `f` is.<br><br>Example 1:<br><br>Input: k = 1, n = 2<br>Output: 2<br>Explanation: <br>Drop the egg from floor 1. If it breaks, we know that f = 0.<br>Otherwise, drop the egg from floor 2. If it breaks, we know that f = 1.<br>If it does not break, then we know f = 2.<br>Hence, we need at minimum 2 moves to determine with certainty what the value of f is.<br><br>Example 2:<br><br>Input: k = 2, n = 6<br>Output: 3<br><br>Example 3:<br><br>Input: k = 3, n = 14<br>Output: 4<br><br>Constraints:<br><br>   `1 <= k <= 100`<br>   `1 <= n <= 104`"""<br><br><br>def mincost_to_hire_workers(quality, wage, k):<br>    workers = sorted((w / q, q) for w, q in zip(wage, quality))<br>    total_quality = 0<br>    max_heap = []<br>    result = float("inf")<br><br>    for ratio, q in workers:<br>        heapq.heappush(max_heap, -q)<br>        total_quality += q<br><br>        if len(max_heap)  k:<br>            total_quality += heapq.heappop(max_heap)<br><br>        if len(max_heap) == k:<br>            result = min(result, total_quality  ratio)<br><br>    return result

### DOC[2] (IRRELEVANT) leetcode/csn_python_train_144360.txt
> def place(vertices_resources, nets, machine, constraints,<br>          effort=1.0, random=default_random, on_temperature_change=None,<br>          kernel=default_kernel, kernel_kwargs={}):<br>    """A flat Simulated Annealing based placement algorithm.<br><br>    This placement algorithm uses simulated annealing directly on the supplied<br>    problem graph with the objective of reducing wire lengths (and thus,<br>    indirectly, the potential for congestion). Though computationally<br>    expensive, this placer produces relatively good placement solutions.<br><br>    The annealing temperature schedule used by this algorithm is taken from<br>    "VPR: A New Packing, Placement and Routing Tool for FPGA Research" by<br>    Vaughn Betz and Jonathan Rose from the "1997 International Workshop on<br>    Field Programmable Logic and Applications".<br><br>    Two implementations of the algorithm's kernel are available:<br><br>     :py:class:`~rig.place_and_route.place.sa.python_kernel.PythonKernel` A<br>      pure Python implementation which is available on all platforms supported<br>      by Rig.<br>     :py:class:`~rig.place_and_route.place.sa.c_kernel.CKernel` A C<br>      implementation which is typically 50-150x faster than the basic Python<br>      kernel. Since this implementation requires a C compiler during<br>      installation, it is an optional feature of Rig. See the<br>      :py:class:`CKernel's documentation<br>      <rig.place_and_route.place.sa.c_kernel.CKernel` for details.<br><br>    The fastest kernel installed is used by default and can be manually chosen<br>    using the ``kernel`` argument.<br><br>    This algorithm produces INFO level logging information describing the<br>    progress made by the algorithm.<br><br>    .. warning:<br>        This algorithm does not attempt to produce good solutions to the<br>        bin-packing problem of optimally fitting vertices into chips and it may<br>        fail if a good placement requires good bin packing.<br><br>    Parameters<br>    ----------<br>    effort : float<br>        A scaling factor for the number of iterations the algorithm should run<br>        for. 1.0 is probably about as low as you'll want to go in practice and<br>        runtime increases linearly as you increase this parameter.<br>    random : :py:class:`random.Random`<br>        A Python random number generator. Defaults to ``import random`` but can<br>        be set to your own instance of :py:class:`random.Random` to allow you<br>        to control the seed and produce deterministic results. For results to<br>        be deterministic, vertices_resources must be supplied as an<br>        :py:class:`collections.OrderedDict`.<br>    on_temperature_change : callback_function or None<br>        An (optional) callback function which is called every time the<br>        temperature is changed. This callback can be used to provide status<br>        updates<br><br>        The callback function is passed the following arguments:<br><br>         ``iteration_count``: the number of iterations the placer has<br>          attempted (integer)<br>         ``placements``: The current placement solution.<br>         ``cost``: the weighted sum over all nets of bounding-box size.<br>          (float)<br>         ``acceptance_rate``: the proportion of iterations which have resulted<br>          in an accepted change since the last callback call. (float between<br>          0.0 and 1.0)<br>         ``temperature``: The current annealing temperature. (float)<br>         ``distance_limit``: The maximum distance any swap may be made over.<br>          (integer)<br><br>        If the callback returns False, the anneal is terminated immediately and<br>        the current solution is returned.<br>    kernel : :py:class:`~rig.place_and_route.place.sa.kernel.Kernel`<br>        A simulated annealing placement kernel. A sensible default will be<br>        chosen based on the available kernels on this machine. The kernel may<br>        not be used if the placement problem has a trivial solution.<br>    kernel_kwargs : dict<br>        Optional kernel-specific keyword arguments to pass to the kernel<br>        constructor.<br>    """<br>    # Special case: just return immediately when there's nothing to place<br>    if len(vertices_resources) == 0:<br>        return {}<br><br>    # Within the algorithm we modify the resource availability values in the<br>    # machine to account for the effects of the current placement. As a result,<br>    # an internal copy of the structure must be made.<br>    machine = machine.copy()<br><br>    # {vertex: (x, y), ...} gives the location of all vertices whose position<br>    # is fixed by a LocationConstraint.<br>    fixed_vertices = {}<br><br>    # Handle constraints<br>    vertices_resources, nets, constraints, substitutions = \<br>        apply_same_chip_constraints(vertices_resources, nets, constraints)<br>    for constraint in constraints:<br>        if isinstance(constraint, LocationConstraint):<br>            # Location constraints are handled by recording the set of fixed<br>            # vertex locations and subtracting their resources from the chips<br>            # they're allocated to. These vertices will then not be added to<br>            # the internal placement data structure to prevent annealing from<br>            # moving them. They will be re-introduced at the last possible<br>            # moment.<br>            location = constraint.location<br>            if location not in machine:<br>                raise InvalidConstraintError(<br>                    "Chip requested by {} unavailable".format(machine))<br>            vertex = constraint.vertex<br><br>            # Record the constrained vertex's location<br>            fixed_vertices[vertex] = location<br><br>            # Make sure the vertex fits at the requested location (updating the<br>            # resource availability after placement)<br>            resources = vertices_resources[vertex]<br>            machine[location] = subtract_resources(machine[location],<br>                                                   resources)<br>            if overallocated(machine[location]):<br>                raise InsufficientResourceError(<br>                    "Cannot meet {}".format(constraint))<br>        elif isinstance(constraint,  # pragma: no branch<br>                        ReserveResourceConstraint):<br>            apply_reserve_resource_constraint(machine, constraint)<br><br>    # Initially randomly place the movable vertices<br>    movable_vertices = {v for v in vertices_resources<br>                        if v not in fixed_vertices}<br>    initial_placements = _initial_placement(movable_vertices,<br>                                            vertices_resources,<br>                                            machine, random)<br><br>    # Include the fixed vertices in initial placement<br>    initial_placements.update(fixed_vertices)<br><br>    # Filter out empty or singleton nets and those weighted as zero since they<br>    # cannot influence placement.<br>    nets = [n for n in nets if len(set(n))  1 and n.weight  0.0]<br><br>    # Special cases where no placement effort is required:<br>    #  There is only one chip<br>    #  There are no resource types to be consumed<br>    #  No effort is to be made<br>    #  No movable vertices<br>    #  There are no nets (and moving things has no effect)<br>    trivial = ((machine.width, machine.height) == (1, 1) or<br>               len(machine.chip_resources) == 0 or<br>               effort == 0.0 or<br>               len(movable_vertices) == 0 or<br>               len(nets) == 0)<br>    if trivial:<br>        logger.info("Placement has trivial solution. SA not used.")<br>        finalise_same_chip_constraints(substitutions, initial_placements)<br>        return initial_placements<br><br>    # Intialise the algorithm kernel<br>    k = kernel(vertices_resources, movable_vertices, set(fixed_vertices),<br>               initial_placements, nets, machine, random, kernel_kwargs)<br><br>    logger.info("SA placement kernel: %s", kernel.__name__)<br><br>    # Specifies the maximum distance any swap can span. Initially consider<br>    # swaps that span the entire machine.<br>    distance_limit = max(machine.width, machine.height)<br><br>    # Determine initial temperature according to the heuristic used by VPR: 20<br>    # times the standard deviation of len(movable_vertices) random swap costs.<br>    # The arbitrary very-high temperature is used to cause "all" swaps to be<br>    # accepted.<br>    _0, _1, cost_delta_sd = k.run_steps(len(movable_vertices),<br>                                        distance_limit,<br>                                        1e100)<br>    temperature = 20.0  cost_delta_sd<br><br>    # The number of swap-attempts between temperature changes is selected by<br>    # the heuristic used by VPR. This value is scaled linearly by the effort<br>    # parameter.<br>    num_steps = max(1, int(effort  len(vertices_resources)1.33))<br><br>    logger.info("Initial placement temperature: %0.1f", temperature)<br><br>    # Counter for the number of swap attempts made (used for diagnostic<br>    # purposes)<br>    iteration_count = 0<br><br>    # Holds the total cost of the current placement. This default value chosen<br>    # to ensure the loop below iterates at least once.<br>    current_cost = 0.0<br><br>    # The annealing algorithm runs until a heuristic termination condition<br>    # (taken from VPR) is hit. The heuristic waits until the temperature falls<br>    # below a small fraction of the average net cost.<br>    while temperature  (0.005  current_cost) / len(nets):<br>        # Run an iteration at the current temperature<br>        num_accepted, current_cost, _ = k.run_steps(<br>            num_steps, int(math.ceil(distance_limit)), temperature)<br><br>        # The ratio of accepted-to-not-accepted changes<br>        r_accept = num_accepted / float(num_steps)<br><br>        # Special case: Can't do better than 0 cost! This is a special case<br>        # since the normal termination condition will not terminate if the cost<br>        # doesn't drop below 0.<br>        if current_cost == 0:<br>            break<br><br>        # The temperature is reduced by a factor heuristically based on the<br>        # acceptance rate. The schedule below attempts to maximise the time<br>        # spent at temperatures where a large portion (but not all) of changes<br>        # are being accepted. If lots of changes are being accepted (e.g.<br>        # during high-temperature periods) then most of them are likely not to<br>        # be beneficial. If few changes are being accepted, we're probably<br>        # pretty close to the optimal placement.<br>        if r_accept  0.96:<br>            alpha = 0.5<br>        elif r_accept  0.8:<br>            alpha = 0.9<br>        elif r_accept  0.15:<br>            alpha = 0.95<br>        else:<br>            alpha = 0.8<br>        temperature = alpha  temperature<br><br>        # According to:<br>        #  M. Huang, F. Romeo, and A. Sangiovanni-Vincentelli, "An Efficient<br>        #   General Cooling Schedule for Simulated Annealing" ICCAD, 1986, pp.<br>        #   381 - 384 and J. Lam<br>        #  J. Delosme, "Performance of a New Annealing Schedule" DAC, 1988,<br>        #   pp. 306 - 311.<br>        # It is desirable to keep the acceptance ratio as close to 0.44 for as<br>        # long as possible. As a result, when r_accept falls below this we can<br>        # help increase the acceptance rate by reducing the set of possible<br>        # swap candidates based on the observation that near the end of<br>        # placement, most things are near their optimal location and thus long<br>        # distance swaps are unlikely to be useful.<br>        distance_limit = 1.0 - 0.44 + r_accept<br>        distance_limit = min(max(distance_limit, 1.0),<br>                             max(machine.width, machine.height))<br><br>        iteration_count += num_steps<br>        logger.debug("Iteration: %d, "<br>                     "Cost: %0.1f, "<br>                     "Kept: %0.1f%%, "<br>                     "Temp: %0.3f, "<br>                     "Dist: %d.",<br>                     iteration_count, current_cost,<br>                     r_accept100, temperature, math.ceil(distance_limit))<br><br>        # Call the user callback before the next iteration, terminating if<br>        # requested.<br>        if on_temperature_change is not None:<br>            placements = k.get_placements().copy()<br>            finalise_same_chip_constraints(substitutions, placements)<br>            ret_val = on_temperature_change(iteration_count,<br>                                            placements,<br>                                            current_cost,<br>                                            r_accept,<br>                                            temperature,<br>                                            distance_limit)<br>            if ret_val is False:<br>                break<br><br>    logger.info("Anneal terminated after %d iterations.", iteration_count)<br><br>    placements = k.get_placements()<br>    finalise_same_chip_constraints(substitutions, placements)<br><br>    return placements

### DOC[3] (IRRELEVANT) leetcode/csn_python_train_309525.txt
> def solveConsAggShock(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,PermGroFac,<br>                      PermGroFacAgg,aXtraGrid,BoroCnstArt,Mgrid,AFunc,Rfunc,wFunc,DeprFac):<br>    '''<br>    Solve one period of a consumption-saving problem with idiosyncratic and<br>    aggregate shocks (transitory and permanent).  This is a basic solver that<br>    can't handle cubic splines, nor can it calculate a value function.<br><br>    Parameters<br>    ----------<br>    solution_next : ConsumerSolution<br>        The solution to the succeeding one period problem.<br>    IncomeDstn : [np.array]<br>        A list containing five arrays of floats, representing a discrete<br>        approximation to the income process between the period being solved<br>        and the one immediately following (in solution_next). Order: event<br>        probabilities, idisyncratic permanent shocks, idiosyncratic transitory<br>        shocks, aggregate permanent shocks, aggregate transitory shocks.<br>    LivPrb : float<br>        Survival probability; likelihood of being alive at the beginning of<br>        the succeeding period.<br>    DiscFac : float<br>        Intertemporal discount factor for future utility.<br>    CRRA : float<br>        Coefficient of relative risk aversion.<br>    PermGroFac : float<br>        Expected permanent income growth factor at the end of this period.<br>    PermGroFacAgg : float<br>        Expected aggregate productivity growth factor.<br>    aXtraGrid : np.array<br>        Array of "extra" end-of-period asset values-- assets above the<br>        absolute minimum acceptable level.<br>    BoroCnstArt : float<br>        Artificial borrowing constraint; minimum allowable end-of-period asset-to-<br>        permanent-income ratio.  Unlike other models, this can't be None.<br>    Mgrid : np.array<br>        A grid of aggregate market resourses to permanent income in the economy.<br>    AFunc : function<br>        Aggregate savings as a function of aggregate market resources.<br>    Rfunc : function<br>        The net interest factor on assets as a function of capital ratio k.<br>    wFunc : function<br>        The wage rate for labor as a function of capital-to-labor ratio k.<br>    DeprFac : float<br>        Capital Depreciation Rate<br><br>    Returns<br>    -------<br>    solution_now : ConsumerSolution<br>        The solution to the single period consumption-saving problem.  Includes<br>        a consumption function cFunc (linear interpolation over linear interpola-<br>        tions) and marginal value function vPfunc.<br>    '''<br>    # Unpack next period's solution<br>    vPfuncNext = solution_next.vPfunc<br>    mNrmMinNext = solution_next.mNrmMin<br><br>    # Unpack the income shocks<br>    ShkPrbsNext  = IncomeDstn[0]<br>    PermShkValsNext = IncomeDstn[1]<br>    TranShkValsNext = IncomeDstn[2]<br>    PermShkAggValsNext = IncomeDstn[3]<br>    TranShkAggValsNext = IncomeDstn[4]<br>    ShkCount = ShkPrbsNext.size<br><br>    # Make the grid of end-of-period asset values, and a tiled version<br>    aNrmNow = aXtraGrid<br>    aCount = aNrmNow.size<br>    Mcount = Mgrid.size<br>    aXtra_tiled = np.tile(np.reshape(aNrmNow,(1,aCount,1)),(Mcount,1,ShkCount))<br><br>    # Make tiled versions of the income shocks<br>    # Dimension order: Mnow, aNow, Shk<br>    ShkPrbsNext_tiled = np.tile(np.reshape(ShkPrbsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>    PermShkValsNext_tiled = np.tile(np.reshape(PermShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>    TranShkValsNext_tiled = np.tile(np.reshape(TranShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>    PermShkAggValsNext_tiled = np.tile(np.reshape(PermShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>    TranShkAggValsNext_tiled = np.tile(np.reshape(TranShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br><br>    # Calculate returns to capital and labor in the next period<br>    AaggNow_tiled = np.tile(np.reshape(AFunc(Mgrid),(Mcount,1,1)),(1,aCount,ShkCount))<br>    kNext_array = AaggNow_tiled/(PermGroFacAggPermShkAggValsNext_tiled) # Next period's aggregate capital to labor ratio<br>    kNextEff_array = kNext_array/TranShkAggValsNext_tiled # Same thing, but account for transitory shock<br>    R_array = Rfunc(kNextEff_array) # Interest factor on aggregate assets<br>    Reff_array = R_array/LivPrb # Effective interest factor on individual assets for survivors<br>    wEff_array = wFunc(kNextEff_array)TranShkAggValsNext_tiled # Effective wage rate (accounts for labor supply)<br>    PermShkTotal_array = PermGroFacPermGroFacAggPermShkValsNext_tiledPermShkAggValsNext_tiled # total / combined permanent shock<br>    Mnext_array = kNext_arrayR_array + wEff_array # next period's aggregate market resources<br><br>    # Find the natural borrowing constraint for each value of M in the Mgrid.<br>    # There is likely a faster way to do this, but someone needs to do the math:<br>    # is aNrmMin determined by getting the worst shock of all four types?<br>    aNrmMin_candidates = PermGroFacPermGroFacAggPermShkValsNext_tiled[:,0,:]PermShkAggValsNext_tiled[:,0,:]/Reff_array[:,0,:]\<br>                         (mNrmMinNext(Mnext_array[:,0,:]) - wEff_array[:,0,:]TranShkValsNext_tiled[:,0,:])<br>    aNrmMin_vec = np.max(aNrmMin_candidates,axis=1)<br>    BoroCnstNat_vec = aNrmMin_vec<br>    aNrmMin_tiled = np.tile(np.reshape(aNrmMin_vec,(Mcount,1,1)),(1,aCount,ShkCount))<br>    aNrmNow_tiled = aNrmMin_tiled + aXtra_tiled<br><br>    # Calculate market resources next period (and a constant array of capital-to-labor ratio)<br>    mNrmNext_array = Reff_arrayaNrmNow_tiled/PermShkTotal_array + TranShkValsNext_tiledwEff_array<br><br>    # Find marginal value next period at every income shock realization and every aggregate market resource gridpoint<br>    vPnext_array = Reff_arrayPermShkTotal_array(-CRRA)vPfuncNext(mNrmNext_array,Mnext_array)<br><br>    # Calculate expectated marginal value at the end of the period at every asset gridpoint<br>    EndOfPrdvP = DiscFacLivPrbnp.sum(vPnext_arrayShkPrbsNext_tiled,axis=2)<br><br>    # Calculate optimal consumption from each asset gridpoint<br>    cNrmNow = EndOfPrdvP(-1.0/CRRA)<br>    mNrmNow = aNrmNow_tiled[:,:,0] + cNrmNow<br><br>    # Loop through the values in Mgrid and make a linear consumption function for each<br>    cFuncBaseByM_list = []<br>    for j in range(Mcount):<br>        c_temp = np.insert(cNrmNow[j,:],0,0.0) # Add point at bottom<br>        m_temp = np.insert(mNrmNow[j,:] - BoroCnstNat_vec[j],0,0.0)<br>        cFuncBaseByM_list.append(LinearInterp(m_temp,c_temp))<br>        # Add the M-specific consumption function to the list<br><br>    # Construct the overall unconstrained consumption function by combining the M-specific functions<br>    BoroCnstNat = LinearInterp(np.insert(Mgrid,0,0.0),np.insert(BoroCnstNat_vec,0,0.0))<br>    cFuncBase = LinearInterpOnInterp1D(cFuncBaseByM_list,Mgrid)<br>    cFuncUnc  = VariableLowerBoundFunc2D(cFuncBase,BoroCnstNat)<br><br>    # Make the constrained consumption function and combine it with the unconstrained component<br>    cFuncCnst = BilinearInterp(np.array([[0.0,0.0],[1.0,1.0]]),<br>                                           np.array([BoroCnstArt,BoroCnstArt+1.0]),np.array([0.0,1.0]))<br>    cFuncNow = LowerEnvelope2D(cFuncUnc,cFuncCnst)<br><br>    # Make the minimum m function as the greater of the natural and artificial constraints<br>    mNrmMinNow = UpperEnvelope(BoroCnstNat,ConstantFunction(BoroCnstArt))<br><br>    # Construct the marginal value function using the envelope condition<br>    vPfuncNow = MargValueFunc2D(cFuncNow,CRRA)<br><br>    # Pack up and return the solution<br>    solution_now = ConsumerSolution(cFunc=cFuncNow,vPfunc=vPfuncNow,mNrmMin=mNrmMinNow)<br>    return solution_now

### DOC[4] (IRRELEVANT) leetcode/csn_python_train_271615.txt
> def solve(self, solver_klass=None):<br>        """ Solves the combined unit decommitment / optimal power flow problem.<br>        """<br>        case = self.case<br>        generators = case.online_generators<br><br>        logger.info("Solving OPF with unit de-commitment [%s]." % case.name)<br><br>        t0 = time()<br><br>        # 1. Begin at stage zero (N = 0), assuming all generators are on-line<br>        # with all limits in place. At most one generator shutdown per stage.<br>        i_stage = 0<br><br>        # Check for sum(p_min)  total load, decommit as necessary.<br>        online = [g for g in generators if not g.is_load]<br>        online_vload = [g for g in generators if g.is_load]<br><br>        # Total dispatchable load capacity.<br>        vload_capacity = sum([g.p_min for g in online_vload])<br>        # Total load capacity.<br>        load_capacity = sum([b.p_demand for b in case.buses]) - vload_capacity<br><br>        # Minimum total online generation capacity.<br>        p_min_tot = sum([g.p_min for g in online])<br><br>        # Shutdown the most expensive units until the minimum generation<br>        # capacity is less than the total load capacity.<br>        while p_min_tot  load_capacity:<br>            i_stage += 1<br>            logger.debug("De-commitment stage %d." % i_stage)<br><br>            # Find generator with the maximum average cost at Pmin.<br>            avg_pmin_cost = [g.total_cost(g.p_min) / g.p_min for g in online]<br>            # Select at random from maximal generators with equal cost.<br>            g_idx, _ = fair_max(avg_pmin_cost)<br>            generator = online[g_idx]<br><br>            logger.info("Shutting down generator [%s] to satisfy all "<br>                        "p_min limits." % generator.name)<br><br>            # Shut down most expensive unit.<br>            generator.online = False<br><br>            # Update minimum generation capacity for while loop.<br>            online = [g for g in case.online_generators if not g.is_load]<br>            p_min_tot = sum([g.p_min for g in online])<br><br>        # 2. Solve a normal OPF and save the solution as the current best.<br>        solution = super(UDOPF, self).solve(solver_klass)<br><br>        logger.debug("Initial system cost: $%.3f" % solution["f"])<br><br>        if not solution["converged"] == True:<br>            logger.error("Non-convergent UDOPF [%s]." %<br>                         solution["output"]["message"])<br>            return solution<br><br>        # 3. Go to the next stage, N = N + 1. Using the best solution from the<br>        # previous stage as the base case for this stage, ...<br><br>        # Best case so far. A list of the on-line status of all generators.<br>        overall_online = [g.online for g in case.generators]<br>        # The objective function value is the total system cost.<br>        overall_cost = solution["f"]<br><br>        # Best case for this stage.<br>        stage_online = overall_online<br>        stage_cost = overall_cost<br><br>        # Shutdown at most one generator per stage.<br>        while True:<br>            # 4. Form a candidate list of generators with minimum<br>            # generation limits binding.<br><br>            # Activate generators according to the stage best.<br>            for i, generator in enumerate(case.generators):<br>                generator.online = stage_online[i]<br><br>            # Get candidates for shutdown. Lagrangian multipliers are often<br>            # very small so we round to four decimal places.<br>            candidates = [g for g in case.online_generators if \<br>                          (round(g.mu_pmin, 4)  0.0) and (g.p_min  0.0)]<br><br>            if len(candidates) == 0:<br>                break<br><br>            # Assume no improvement during this stage.<br>            done = True<br><br>            i_stage += 1<br>            logger.debug("De-commitment stage %d." % i_stage)<br><br>            for candidate in candidates:<br>                # 5. For each generator on the candidate list, solve an OPF to<br>                # find the total system cost with the generator shut down.<br><br>                # Activate generators according to the stage best.<br>                for i, generator in enumerate(case.generators):<br>                    generator.online = stage_online[i]<br><br>                # Shutdown candidate generator.<br>                candidate.online = False<br><br>                logger.debug("Solving OPF with generator '%s' shutdown." %<br>                    candidate.name)<br><br>                # Run OPF.<br>                solution = super(UDOPF, self).solve(solver_klass)<br><br>                # Compare total system costs for improvement.<br>                if solution["converged"] == True \<br>                    and (solution["f"] < overall_cost):<br>                    logger.debug("System cost improvement: $%.3f ($%.3f)" %<br>                                 (stage_cost - solution["f"], solution["f"]))<br>                    # 6. Replace the current best solution with this one if<br>                    # it has a lower cost.<br>                    overall_online = [g.online for g in case.generators]<br>                    overall_cost = solution["f"]<br>                    best_candidate = candidate<br>                    # Check for further decommitment.<br>                    done = False<br>                else:<br>                    logger.debug("Candidate OPF failed [%s]." %<br>                                 solution["output"]["message"])<br><br>                # Reactivate the candidate before deactivating the next.<br>#                candidate.online = True<br><br>            if done:<br>                # Decommits at this stage did not help.<br>                break<br>            else:<br>                # 7. If any of the candidate solutions produced an improvement,<br>                # return to step 3.<br><br>                # Shutting something else down helps, so let's keep going.<br>                logger.info("Shutting down generator '%s'.",<br>                            best_candidate.name)<br><br>                stage_online = overall_online<br>                stage_cost = overall_cost<br><br>        # 8. Use the best overall solution as the final solution.<br>        for i, generator in enumerate(case.generators):<br>            generator.online = overall_online[i]<br><br>        # One final solve using the best case to ensure all results are<br>        # up-to-date.<br>        solution = super(UDOPF, self).solve(solver_klass)<br><br>        logger.debug("UDOPF system cost: $%.3f" % solution["f"])<br><br>        # Compute elapsed time and log it.<br>        elapsed = time() - t0<br><br>        plural = "" if i_stage == 1 else "s"<br>        logger.info("Unit decommitment OPF solved in %.3fs (%d decommitment "<br>                    "stage%s)." % (elapsed, i_stage, plural))<br><br>        return solution

### DOC[5] (IRRELEVANT) leetcode/csn_python_train_309526.txt
> def solveConsAggMarkov(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,MrkvArray,<br>                       PermGroFac,PermGroFacAgg,aXtraGrid,BoroCnstArt,Mgrid,<br>                       AFunc,Rfunc,wFunc,DeprFac):<br>    '''<br>    Solve one period of a consumption-saving problem with idiosyncratic and<br>    aggregate shocks (transitory and permanent).  Moreover, the macroeconomic<br>    state follows a Markov process that determines the income distribution and<br>    aggregate permanent growth factor. This is a basic solver that can't handle<br>    cubic splines, nor can it calculate a value function.<br><br>    Parameters<br>    ----------<br>    solution_next : ConsumerSolution<br>        The solution to the succeeding one period problem.<br>    IncomeDstn : [[np.array]]<br>        A list of lists, each containing five arrays of floats, representing a<br>        discrete approximation to the income process between the period being<br>        solved and the one immediately following (in solution_next). Order: event<br>        probabilities, idisyncratic permanent shocks, idiosyncratic transitory<br>        shocks, aggregate permanent shocks, aggregate transitory shocks.<br>    LivPrb : float<br>        Survival probability; likelihood of being alive at the beginning of<br>        the succeeding period.<br>    DiscFac : float<br>        Intertemporal discount factor for future utility.<br>    CRRA : float<br>        Coefficient of relative risk aversion.<br>    MrkvArray : np.array<br>        Markov transition matrix between discrete macroeconomic states.<br>        MrkvArray[i,j] is probability of being in state j next period conditional<br>        on being in state i this period.<br>    PermGroFac : float<br>        Expected permanent income growth factor at the end of this period,<br>        for the individual's productivity.<br>    PermGroFacAgg : [float]<br>        Expected aggregate productivity growth in each Markov macro state.<br>    aXtraGrid : np.array<br>        Array of "extra" end-of-period asset values-- assets above the<br>        absolute minimum acceptable level.<br>    BoroCnstArt : float<br>        Artificial borrowing constraint; minimum allowable end-of-period asset-to-<br>        permanent-income ratio.  Unlike other models, this can't be None.<br>    Mgrid : np.array<br>        A grid of aggregate market resourses to permanent income in the economy.<br>    AFunc : [function]<br>        Aggregate savings as a function of aggregate market resources, for each<br>        Markov macro state.<br>    Rfunc : function<br>        The net interest factor on assets as a function of capital ratio k.<br>    wFunc : function<br>        The wage rate for labor as a function of capital-to-labor ratio k.<br>    DeprFac : float<br>        Capital Depreciation Rate<br><br>    Returns<br>    -------<br>    solution_now : ConsumerSolution<br>        The solution to the single period consumption-saving problem.  Includes<br>        a consumption function cFunc (linear interpolation over linear interpola-<br>        tions) and marginal value function vPfunc.<br>    '''<br>    # Get sizes of grids<br>    aCount = aXtraGrid.size<br>    Mcount = Mgrid.size<br>    StateCount = MrkvArray.shape[0]<br><br>    # Loop through next period's states, assuming we reach each one at a time.<br>    # Construct EndOfPrdvP_cond functions for each state.<br>    EndOfPrdvPfunc_cond = []<br>    BoroCnstNat_cond = []<br>    for j in range(StateCount):<br>        # Unpack next period's solution<br>        vPfuncNext = solution_next.vPfunc[j]<br>        mNrmMinNext = solution_next.mNrmMin[j]<br><br>        # Unpack the income shocks<br>        ShkPrbsNext  = IncomeDstn[j][0]<br>        PermShkValsNext = IncomeDstn[j][1]<br>        TranShkValsNext = IncomeDstn[j][2]<br>        PermShkAggValsNext = IncomeDstn[j][3]<br>        TranShkAggValsNext = IncomeDstn[j][4]<br>        ShkCount = ShkPrbsNext.size<br>        aXtra_tiled = np.tile(np.reshape(aXtraGrid,(1,aCount,1)),(Mcount,1,ShkCount))<br><br>        # Make tiled versions of the income shocks<br>        # Dimension order: Mnow, aNow, Shk<br>        ShkPrbsNext_tiled = np.tile(np.reshape(ShkPrbsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>        PermShkValsNext_tiled = np.tile(np.reshape(PermShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>        TranShkValsNext_tiled = np.tile(np.reshape(TranShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>        PermShkAggValsNext_tiled = np.tile(np.reshape(PermShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br>        TranShkAggValsNext_tiled = np.tile(np.reshape(TranShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))<br><br>        # Make a tiled grid of end-of-period aggregate assets.  These lines use<br>        # next prd state j's aggregate saving rule to get a relevant set of Aagg,<br>        # which will be used to make an interpolated EndOfPrdvP_cond function.<br>        # After constructing these functions, we will use the aggregate saving<br>        # rule for current state i to get values of Aagg at which to evaluate<br>        # these conditional marginal value functions.  In the strange, maybe even<br>        # impossible case where the aggregate saving rules differ wildly across<br>        # macro states and there is "anti-persistence", so that the macro state<br>        # is very likely to change each period, then this procedure will lead to<br>        # an inaccurate solution because the grid of Aagg values on which the<br>        # conditional marginal value functions are constructed is not relevant<br>        # to the values at which it will actually be evaluated.<br>        AaggGrid = AFunc[j](Mgrid)<br>        AaggNow_tiled = np.tile(np.reshape(AaggGrid,(Mcount,1,1)),(1,aCount,ShkCount))<br><br>        # Calculate returns to capital and labor in the next period<br>        kNext_array = AaggNow_tiled/(PermGroFacAgg[j]PermShkAggValsNext_tiled) # Next period's aggregate capital to labor ratio<br>        kNextEff_array = kNext_array/TranShkAggValsNext_tiled # Same thing, but account for transitory shock<br>        R_array = Rfunc(kNextEff_array) # Interest factor on aggregate assets<br>        Reff_array = R_array/LivPrb # Effective interest factor on individual assets for survivors<br>        wEff_array = wFunc(kNextEff_array)TranShkAggValsNext_tiled # Effective wage rate (accounts for labor supply)<br>        PermShkTotal_array = PermGroFacPermGroFacAgg[j]PermShkValsNext_tiledPermShkAggValsNext_tiled # total / combined permanent shock<br>        Mnext_array = kNext_arrayR_array + wEff_array # next period's aggregate market resources<br><br>        # Find the natural borrowing constraint for each value of M in the Mgrid.<br>        # There is likely a faster way to do this, but someone needs to do the math:<br>        # is aNrmMin determined by getting the worst shock of all four types?<br>        aNrmMin_candidates = PermGroFacPermGroFacAgg[j]PermShkValsNext_tiled[:,0,:]PermShkAggValsNext_tiled[:,0,:]/Reff_array[:,0,:]\<br>                             (mNrmMinNext(Mnext_array[:,0,:]) - wEff_array[:,0,:]TranShkValsNext_tiled[:,0,:])<br>        aNrmMin_vec = np.max(aNrmMin_candidates,axis=1)<br>        BoroCnstNat_vec = aNrmMin_vec<br>        aNrmMin_tiled = np.tile(np.reshape(aNrmMin_vec,(Mcount,1,1)),(1,aCount,ShkCount))<br>        aNrmNow_tiled = aNrmMin_tiled + aXtra_tiled<br><br>        # Calculate market resources next period (and a constant array of capital-to-labor ratio)<br>        mNrmNext_array = Reff_arrayaNrmNow_tiled/PermShkTotal_array + TranShkValsNext_tiledwEff_array<br><br>        # Find marginal value next period at every income shock realization and every aggregate market resource gridpoint<br>        vPnext_array = Reff_arrayPermShkTotal_array(-CRRA)vPfuncNext(mNrmNext_array,Mnext_array)<br><br>        # Calculate expectated marginal value at the end of the period at every asset gridpoint<br>        EndOfPrdvP = DiscFacLivPrbnp.sum(vPnext_arrayShkPrbsNext_tiled,axis=2)<br><br>        # Make the conditional end-of-period marginal value function<br>        BoroCnstNat = LinearInterp(np.insert(AaggGrid,0,0.0),np.insert(BoroCnstNat_vec,0,0.0))<br>        EndOfPrdvPnvrs = np.concatenate((np.zeros((Mcount,1)),EndOfPrdvP(-1./CRRA)),axis=1)<br>        EndOfPrdvPnvrsFunc_base = BilinearInterp(np.transpose(EndOfPrdvPnvrs),np.insert(aXtraGrid,0,0.0),AaggGrid)<br>        EndOfPrdvPnvrsFunc = VariableLowerBoundFunc2D(EndOfPrdvPnvrsFunc_base,BoroCnstNat)<br>        EndOfPrdvPfunc_cond.append(MargValueFunc2D(EndOfPrdvPnvrsFunc,CRRA))<br>        BoroCnstNat_cond.append(BoroCnstNat)<br><br>    # Prepare some objects that are the same across all current states<br>    aXtra_tiled = np.tile(np.reshape(aXtraGrid,(1,aCount)),(Mcount,1))<br>    cFuncCnst = BilinearInterp(np.array([[0.0,0.0],[1.0,1.0]]),np.array([BoroCnstArt,BoroCnstArt+1.0]),np.array([0.0,1.0]))<br><br>    # Now loop through this period's discrete states, calculating end-of-period<br>    # marginal value (weighting across state transitions), then construct consumption<br>    # and marginal value function for each state.<br>    cFuncNow = []<br>    vPfuncNow = []<br>    mNrmMinNow = []<br>    for i in range(StateCount):<br>        # Find natural borrowing constraint for this state by Aagg<br>        AaggNow = AFunc[i](Mgrid)<br>        aNrmMin_candidates = np.zeros((StateCount,Mcount)) + np.nan<br>        for j in range(StateCount):<br>            if MrkvArray[i,j]  0.: # Irrelevant if transition is impossible<br>                aNrmMin_candidates[j,:] = BoroCnstNat_cond[j](AaggNow)<br>        aNrmMin_vec = np.nanmax(aNrmMin_candidates,axis=0)<br>        BoroCnstNat_vec = aNrmMin_vec<br><br>        # Make tiled grids of aNrm and Aagg<br>        aNrmMin_tiled = np.tile(np.reshape(aNrmMin_vec,(Mcount,1)),(1,aCount))<br>        aNrmNow_tiled = aNrmMin_tiled + aXtra_tiled<br>        AaggNow_tiled = np.tile(np.reshape(AaggNow,(Mcount,1)),(1,aCount))<br><br>        # Loop through feasible transitions and calculate end-of-period marginal value<br>        EndOfPrdvP = np.zeros((Mcount,aCount))<br>        for j in range(StateCount):<br>            if MrkvArray[i,j]  0.:<br>                temp = EndOfPrdvPfunc_cond[j](aNrmNow_tiled,AaggNow_tiled)<br>                EndOfPrdvP += MrkvArray[i,j]temp<br><br>        # Calculate consumption and the endogenous mNrm gridpoints for this state<br>        cNrmNow = EndOfPrdvP(-1./CRRA)<br>        mNrmNow = aNrmNow_tiled + cNrmNow<br><br>        # Loop through the values in Mgrid and make a piecewise linear consumption function for each<br>        cFuncBaseByM_list = []<br>        for n in range(Mcount):<br>            c_temp = np.insert(cNrmNow[n,:],0,0.0) # Add point at bottom<br>            m_temp = np.insert(mNrmNow[n,:] - BoroCnstNat_vec[n],0,0.0)<br>            cFuncBaseByM_list.append(LinearInterp(m_temp,c_temp))<br>            # Add the M-specific consumption function to the list<br><br>        # Construct the unconstrained consumption function by combining the M-specific functions<br>        BoroCnstNat = LinearInterp(np.insert(Mgrid,0,0.0),np.insert(BoroCnstNat_vec,0,0.0))<br>        cFuncBase = LinearInterpOnInterp1D(cFuncBaseByM_list,Mgrid)<br>        cFuncUnc  = VariableLowerBoundFunc2D(cFuncBase,BoroCnstNat)<br><br>        # Combine the constrained consumption function with unconstrained component<br>        cFuncNow.append(LowerEnvelope2D(cFuncUnc,cFuncCnst))<br><br>        # Make the minimum m function as the greater of the natural and artificial constraints<br>        mNrmMinNow.append(UpperEnvelope(BoroCnstNat,ConstantFunction(BoroCnstArt)))<br><br>        # Construct the marginal value function using the envelope condition<br>        vPfuncNow.append(MargValueFunc2D(cFuncNow[-1],CRRA))<br><br>    # Pack up and return the solution<br>    solution_now = ConsumerSolution(cFunc=cFuncNow,vPfunc=vPfuncNow,mNrmMin=mNrmMinNow)<br>    return solution_now


## Ground Truth

### GROUND TRUTH 0, ranked not in top 100, leetcode/leetcode_2542.txt
> def maxScore(nums1, nums2, k):<br>    """You are given two 0-indexed integer arrays `nums1` and `nums2` of equal length `n` and a positive integer `k`. You must choose a subsequence of indices from `nums1` of length `k`.<br><br>For chosen indices `i0`, `i1`, ..., `ik - 1`, your score is defined as:<br><br>   The sum of the selected elements from `nums1` multiplied with the minimum of the selected elements from `nums2`.<br>   It can defined simply as: `(nums1[i0] + nums1[i1] +...+ nums1[ik - 1])  min(nums2[i0] , nums2[i1], ... ,nums2[ik - 1])`.<br><br>Return _the maximum possible score._<br><br>A subsequence of indices of an array is a set that can be derived from the set `{0, 1, ..., n-1}` by deleting some or no elements.<br><br>Example 1:<br><br>Input: nums1 = \[1,3,3,2\], nums2 = \[2,1,3,4\], k = 3<br>Output: 12<br>Explanation: <br>The four possible subsequence scores are:<br>- We choose the indices 0, 1, and 2 with score = (1+3+3) \ min(2,1,3) = 7.<br>- We choose the indices 0, 1, and 3 with score = (1+3+2) \ min(2,1,4) = 6. <br>- We choose the indices 0, 2, and 3 with score = (1+3+2) \ min(2,3,4) = 12. <br>- We choose the indices 1, 2, and 3 with score = (3+3+2) \ min(1,3,4) = 8.<br>Therefore, we return the max score, which is 12.<br><br>Example 2:<br><br>Input: nums1 = \[4,2,3,1,1\], nums2 = \[7,5,10,9,6\], k = 1<br>Output: 30<br>Explanation: <br>Choosing index 2 is optimal: nums1\[2\] \ nums2\[2\] = 3 \ 10 = 30 is the maximum possible score.<br><br>Constraints:<br><br>   `n == nums1.length == nums2.length`<br>   `1 <= n <= 105`<br>   `0 <= nums1[i], nums2[j] <= 105`<br>   `1 <= k <= n`"""<br><br>    n = len(nums1)<br>    res = 0<br>    sum_n1 = 0<br>    min_n2 = nums2[0]<br>    i, j = 0, 0<br>    while j < n:<br>        sum_n1 += nums1[j]<br>        min_n2 = min(min_n2, nums2[j])<br>        if j - i + 1  k:<br>            sum_n1 -= nums1[i]<br>            i += 1<br>        if j + 1 = k:<br>            res = max(res, sum_n1  min_n2)<br>        j += 1<br>    return res
